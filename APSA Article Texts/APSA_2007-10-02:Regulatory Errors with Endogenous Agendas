"","x"
"1","Regulatory error merits theoretical analysis for at least two reasons. First, the errors can have drastic economic and policy consequences. Perhaps the most prominent example of a Type I error in pharmaceutical regulation became public in September 2004, when Merck announced the voluntary worldwide withdrawal of Vioxx (rofecoxib), a COX‐2 inhibitor for the treatment of osteoarthritis and other indications. The stated reason for the withdrawal was that a Phase III trial of rofecoxib for patients with colon polyps had evinced a doubling of the rate of adverse cardiovascular events (stroke and heart attack) relative to the control group. The FDA soon published a controversial memorandum from one of its epidemiologists, Dr. David Graham of the Office of Drug Safety, suggesting that use of rofecoxib as opposed to other COX‐2 inhibitors was responsible for 27,000 to 55,000 deaths from 1999 to 2003.1 As an example of a Type II error, critics of the FDA have long pointed to the delayed availability of so‐called beta‐blockers as a telling example of the costs of U.S. pharmaceutical regulation (Hilts 2003; Wardell and Lasagna 1975). By blocking beta‐adrenergic receptors to which adrenaline binds in heart cells, beta‐blockers can reduce effective cardiovascular “workload.” The late introduction of alprenolol as a treatment for essential hypertension and angina was estimated by Wardell to have resulted in over 10,000 deaths per year in the United States.2"
"2","Second, there are typically players outside of the firm‐regulator “game” who are also affected by the trade‐offs between Type I and Type II errors (e.g., Quirk 1980). Their payoffs are central to understanding the interest group politics surrounding regulatory policy. In the context of drug approvals, for example, disease advocacy groups may favor quicker approvals and care directly about Type II errors, while consumer safety groups may favor slower approvals and care directly about Type I errors.         "
"3","Previous studies of administrative and regulatory error have generally fallen under three categories. An extensive literature on optimal stopping problems considers the trade‐off faced by a single decision maker between accepting an experimental product immediately and waiting for additional (and costly) information (e.g., Dixit and Pindyck 1994; Kamien and Schwartz 1972; Moscarini and Smith 2001; Reinganum 1982). In the regulatory approval context, Carpenter (2004) considers a single decision maker facing an exogenous stream of experimental results. In this environment, the model predicts no asymptotic Type II errors: good products may be delayed, but in the long run will always be accepted eventually.         "
"4","A second line of argument has focused upon administrative structure and the benefits and limits of redundancy (Bendor 1985; Heimann 1997; Landau 1969; Ting 2003). These efforts focused on the effects of parallel processing (multiple procedurally independent agents) and serial processing (multiple predecision steps). The former structure reduces Type II errors, but increases Type I errors, while the latter does the reverse. Overall, however, redundant and seemingly inefficient administrative arrangements could be useful ex ante in minimizing organizational error, particularly for agencies governing risky technologies.         "
"5","A final set of theories that can be usefully applied to organizational error invokes bounded rationality. Because decision making imposes computational costs, boundedly rational bureaucrats may satisfice rather than optimize (Simon 1968). They may employ shortcuts in learning, fail to collect all relevant information, or rely heavily upon frames in rendering decisions. These behaviors may collectively give rise to organization‐wide maladies (e.g., Allison 1971; Bendor and Kumar 2005). Unfortunately, few formal models have used these approaches both to study agency failure as well as to distinguish between outcomes under fully and boundedly rational decision making.         "
"6","These literatures mark the limits of our analysis. While they suggest natural extensions to our work, our model will produce systematic variations in errors even without their features. Instead, we focus on a common shortcoming of all of these explanations. To our knowledge, all formal analyses of Type I and Type II error have taken the agenda of the policymaker as exogenous. Thus the set of products (drugs, foods, energy technologies) or ideas (military or law enforcement doctrines) considered by the policymaker is unaffected by the anticipation of administrative decisions. Yet some of the most important influences on errors may come from the manner in which the policymaker induces “applicants” to advance or abandon their proposals. A fuller account of agency errors, then, demands an analysis of how bureaucratic agendas are shaped. The ensuing model comprises a first formalized attempt to do this."
"7"," Informational Environment and Players. There are two players: a firm (F) and a regulator (R). Both players are imperfectly informed about a parameter x of a product, which may be thought of as its “quality” or profitability if brought to market. We assume that x follows a Beta distribution, or x∼β(θ, n), where θ and n are integers with 1 < θ < n. The first parameter of the distribution, θ, is F's type, that is, the estimated product quality prior to any research. Since F has only one product, we use “firm” and “product” interchangeably. The type takes one of two possible values,  where  and the probability that  is p∈ (0, 1). We will refer to  and  as the “low” and “high” types, respectively, where the former corresponds to a lower expected quality. The type is known only to F, though R knows p. The second parameter, n, is common knowledge.            "
"8","A crucial feature of our model, then, is that no player is certain of the true product quality. Moreover, information is asymmetric because the firm's initial estimate of x is more precise than the regulator's. In this respect, the Beta distribution of quality is attractive because it offers a natural interpretation of a set of n Bernoulli trials, of which θ resulted in success and n−θ in failure.3 Additionally, it is flexible enough to accommodate a wide variety of “shapes” of the density function, as determined by θ and n. Given θ and n, the Beta distribution implies that x has a prior mean θ/n and prior variance θ(n−θ)/(n2(n+ 1)). The uncertainty over x can be reduced partially through observable experiments.            "
"9"," Sequence of Play: Development, Then Regulation. The game has up to four periods (t= 1, 2, 3, 4), denoted by subscripts. It begins in a development phase with up to three periods and possibly ends with a regulatory phase of one period.            "
"10","In the development phase, F chooses an action ft∈{S, W, E} at t= 1, 2. S denotes a submission for approval, which ends the development phase and commences the regulatory phase the next period. W denotes a withdrawal from consideration, ending the game. Finally, E denotes an experiment to gather more data. An experiment is a single Bernoulli trial, which produces a publicly observable result et∈{0, 1} corresponding to failure or success, respectively. For convenience we let e0= 0. Each experiment continues the development phase. F cannot experiment past the second period and thus must either submit or withdraw: f3∈{S, W}.            "
"11","At the beginning of the regulatory phase, R knows F's actions and experimental results and uses these to form expectations about θ and in turn x. Based on this, she makes a review decision r∈{A, R}, where A and R denote acceptance and rejection of the firm's submission, respectively.            "
"12"," Information and Belief Updating. A key component of the model is the way in which experimental results affect the players' expectations of product quality. (Without experiments, the model reduces to a simple signaling game with F as the sender.) The Beta distribution provides simple ways to calculate both the probability of an experimental success as well as the updated estimate of x. Beginning with a prior distribution of β(θ, n), the probability that the next experiment succeeds (i.e., e1= 1) is simply θ/n. An experimental record of n′ experiments producing θ′ successes generates a posterior distribution of β (θ+θ′, n+n′), with mean  and variance .            "
"13","Since the parameters affecting the estimate of x change with experimentation, it will be useful to distinguish notationally between F's “initial” type θ and the numerical value of the first parameter of the Beta distribution by letting .            "
"14"," Utilities. F receives x if the product is approved, and zero for rejection or withdrawal. Each experiment costs ce, and a submission costs cs, where:               "
"15","R receives x−k for an approved product, and zero otherwise. The parameter k is therefore the divergence between the preferences of R and F. It can represent a “certainty equivalent” that a risk‐averse citizen or legislature, fearing a product safety disaster, would demand from the firm in order for its product to be marketed. To rule out a number of trivial cases, let k satisfy:               "
"16","We characterize mixed strategy Perfect Bayesian Equilibria (PBE) that satisfy a minor refinement. Let Ht represent the set of all possible experimental histories (or experimental results) prior to time t; thus, H1≡∅, H2≡{0, 1}, and H3≡{0, 1}×{0, 1}. We use ht to denote generic elements of Ht. The equilibrium consists of the following three elements.            "
"17","                              "
"18","F's strategy is the triple (φ1, φ2, φ3), where  for t= 1, 2, and  map types and experimental histories to probability distributions over submitting, withdrawing, and experimenting (where feasible).                     "
"19","R's strategy ρ maps experimental histories, conditional upon a submission, into a probability of rejection. (Formally, ρ : ∪tHt→[0, 1]).                     "
"20","R has beliefs μ mapping the experimental and submission history into a probability that . Formally, μ: ∪tHt×{∅, S}→[0, 1]. These beliefs must be consistent with Bayes' Rule along the equilibrium path of play. Out of equilibrium, μ(ht, ·) =p.4"
"21","Our analysis will utilize two other pieces of notation. First, we decompose φt(θ, ht) into probabilities of submitting, σ(θ, ht), withdrawing, ω(θ, ht), and experimenting, η(θ, ht). Second, we denote the expected quality of a period t submission (given beliefs μ(·)) by:               "
"22","To simplify our presentation, we ignore “knife‐edge” equilibria, which are not robust to small perturbations in parameter values. While we do not prove the result due to space considerations, the remaining equilibria are the only ones in which the counterintuitive strategy of withdrawing good products in the last period is not used.5 As the subsequent development shows, the predicted equilibria are unique for most, but not all, parameter values. This nonuniqueness does not affect the tested hypotheses of the model. Therefore, rather than imposing another refinement, we remain neutral on which equilibrium obtains in these cases.            "
"23","We now turn to the main model. There are two types of equilibria—the Early Submission Equilibrium (ESE) and the Late Submission Equilibrium (LSE)—depending on how players react to a successful period 1 experiment.         "
"24","We begin by deriving strategies that are shared by both the ESE and LSE. Throughout the game, each player's strategy can be stated in the following general terms. R's decision problem occurs when F submits. Clearly, R accepts a submission if its expected quality is greater than k, rejects if it is less, and is indifferent (and so may mix) otherwise. Thus:               "
"25","F's choice will depend on his assessment of the value of experimentation. Let v(θ, ht) denote type‐θ's continuation value from experimentation, conditional upon experimental history ht. Clearly, v(θ, h3) = 0 ∀θ. He prefers submission over experimentation in period t if:               "
"26","The derivation of equilibrium strategies is simplified by a number of cases in which R does not need to distinguish between F's types. First, there are histories for which both types are either acceptable or unacceptable to R. By (3), R would reject any submission with no successes in the experimental history, and therefore neither type of F submits under any such history. If no successes have been achieved by t= 3, then the inability to experiment further forces a withdrawal. Likewise, R would approve any submission with two successes, and thus both types of F submit for h2= (1, 1). Thus we have:               "
"27","Second, when p is sufficiently high, there are histories for which the “average” quality of a submission when both firm types experiment and submit exceeds k, causing R to accept all submissions. This may occur for h2= 1 and h3= (0, 1) or (1,0). It will therefore be useful to define the expected period 2 quality conditional upon h2= 1, given that both types experiment in period 1:               "
"28","The parameters  and  are important to R because they determine whether her acceptance strategy needs to distinguish between types. For example, if  and both types experiment and submit upon a success, then R accepts a submission after a single experimental success with probability one.            "
"29"," Early Submission Equilibrium. An ESE is an equilibrium in which F submits “early” (i.e., in period 2) with positive probability. Clearly, an early submission requires that F's first experiment be successful. There are two possibilities induced by the history h2= 1. These depend on F's quality threshold, k, relative to , the expected period 2 quality conditional upon both types experimenting successfully in period 1.            "
"30","If , then the prior quality distribution is high. R is thus willing in expectation to accept the set of all successful first‐period experimenters, which includes low types. Since additional experiments are costly and cannot improve the odds of acceptance, both types submit and are accepted with certainty.            "
"31","If , then R would not accept the set of all first‐period successes. However, by (3), she wishes to accept the high type and can choose a rejection strategy to deter the low type from always submitting. Early submission requires that an initially successful high type prefer submission to continued experimentation. This occurs under the following condition, which we label Early Submission (ES):               "
"32","If instead F's first experiment ends in failure, then by (8) she cannot submit. The low type may then either continue experimenting or withdraw. The incentive to continue experimentation is maximized when ; i.e., the ex ante expected quality is high enough that a history of (0, 1) is sufficient for R to accept, given that both types experiment.            "
"33","A withdrawal will occur when cs and ce are high enough to outweigh any possible gain from approval. This occurs under the following condition, which we label Early Withdrawal (EW):               "
"34","The first result uses these observations to characterize the ESE strategies. Its derivation may be found in the appendix. Figure 1 graphically depicts F's equilibrium strategy.            "
"35","                 Firm Strategy in Early Submission Equilibrium.                         "
"36","                               Notes: With a type                           product, F submits after one success. With a type                           product, F may be forced to withdraw after one failure.                         "
"37","Proposition 1  (Early Submission Equilibrium) If or ES holds, then there exists an equilibrium given by (7)–(9) and:"
"38"," Late Submission Equilibrium. An LSE is an equilibrium in which there are no submissions at t= 2. This may occur either because the expected quality of successful experimenters at t= 2 does not warrant acceptance (i.e., ), or because an initially successful experimenter would prefer to gather more information (i.e., ES is violated). The existence of the LSE is assured when the ESE does not exist and furthermore does not depend on the values of cs or ce. The LSE requires only that , while the ESE requires that either  or ES hold. Thus when , the model uniquely predicts the ESE. And when  and ES does not hold, the model uniquely predicts the LSE. If  and ES holds, then both equilibria exist.               "
"39","Analogously to Proposition 1, the next result characterizes the LSE strategies. Much of the proof is identical to that of Proposition 1, as the two equilibria are identical for histories starting with h2= 0. The derivation can be found in the supplementary appendix.6"
"40","Proposition 2  (Late Submission Equilibrium) If                  , then there exists an equilibrium given by (7)–(9) and:"
"41","These two equilibria share several noteworthy features. First, costly activity by F can serve multiple purposes, thus distinguishing our model from standard costly signaling models. Here experimentation and submission are both signals of type, but experimentation also endogenously generates additional public information. This information, in addition to type, determines submission strategies."
"42","Second, at histories h2= 1 and h3= (0, 1), R may face the problem of distinguishing between types. The expected quality of submitted and accepted products is often R's reservation value of k. This makes R indifferent between rejection and acceptance. R therefore chooses rejection probabilities that screen out the low type by making him indifferent between experimentation or submission and withdrawal. R thereby benefits from its gatekeeping power, as  by assumption. However, the expected quality may exceed k if the proportion p of high types is large or low types withdraw early from consideration. Thus the approval regulation process allows R to “skim” the best products from a population that is ex ante unacceptable. A corollary is that, because of asymmetric information, R makes both Type I and Type II errors. Errors occur because submissions are pooled and R mixes between acceptance and rejection. This result contrasts with a decision‐theoretic world in which R knows θ and can experiment on her own. In this world, no errors would ensue.               "
"43","We now examine the model's predictions about the likelihood of ex ante regulatory errors.7 Recall that R commits a Type I error by approving a product with expected quality below k. Let ψ(ht) denote the equilibrium probability of a Type I error at each history ht in which F submits with positive probability. For all such histories except (1, 1), ψ(ht) is the same as the probability of accepting a low type, and so ψ(ht) = (1 −ρ*(ht))(1 −μ(ht, S)). This expression makes clear the fact that Type I errors are correlated with acceptance rates.            "
"44","We are primarily interested in comparative statics on ce and cs. There are three histories (1, (1, 0), and (0, 1)) for which ψ(ht) varies with these parameters. Calculating μ(ht, S) is facilitated by the fact that  whenever the rejection probability is interior, as it is for these histories. (When the rejection probability is not interior, μ(ht, S) must be calculated directly from F's equilibrium strategies.) Thus we may use (4) to obtain:               "
"45","Substituting ρ*(ht) from Propositions 1 and 2, we can then easily calculate the error probabilities that vary with costs at each history conditional upon submission:               "
"46","                "
"47","Type I errors are increasing in cs because of its role as a one‐time signal of quality (implying higher acceptance rates). Likewise, ce has a similar effect late in the development phase. In these cases, F's choice to bear costs conveys product quality and thus increases acceptance probabilities. This in turn raises the probability of Type I errors.            "
"48","In some histories, however, the dynamics of experimentation can confound this logic. For example, if h3= (0, 1) and EW holds, then only the high type submits and all submissions are accepted. The error rate is then zero and obviously does not depend on costs. Even more interestingly, for early submissions the effect of ce on ψ(ht) can be the opposite of that for late submissions. Since ce reduces the incentive of low types to continue experimentation, high values reduce R's acceptance probability, thus reducing the likelihood of an unwanted acceptance.            "
"49","Because the specific history of clinical trials can be difficult to obtain, the tested comparative statics predictions are based on the length of the presubmission experimental history. Let  denote the probability of a Type I error conditional upon submission after τ periods of experimentation. Aggregating over possible values of p, it is straightforward to obtain the following comparative statics:            "
"50","                              "
"51"," Short experimentation                        (τ= 1):                        "
"52"," Long experimentation                        (τ= 2):8"
"53","Note finally that for all histories, Type I errors are decreasing in k and weakly increasing in cs, and so at the margin raising the quality standard and reducing submission hurdles will decrease Type I errors. While it is not a direct prediction of the model, our analysis suggests that consumer protection groups such as Public Citizen, who place more weight on Type I errors, would support increasing regulatory standards and reducing submission costs.            "
"54","While our empirical results do not focus on them, a similar analysis can be performed on Type II errors. R commits a Type II error by rejecting a product with expected quality above k. Given ht and a submission, the equilibrium probability of a Type II error is simply ρ*(ht) μ(ht, S), which for h3≠ (1, 1) is the probability of rejecting the high type. Again, there are four histories—1, (1, 0), (0, 1), and (1, 1)—for which submissions occur with positive probability. Clearly, no Type II errors occur whenever all submissions are accepted. For the other histories, the error probabilities conditional upon submission are easily calculated using (14) and Propositions 1 and 2.            "
"55","Carrying out the exercise analogous to that for Type I errors, it is straightforward to show that the comparative statics on Type II errors mirror exactly those for Type I errors. Perhaps unsurprisingly, the same factors that increase acceptance rates (and Type I errors) reduce Type II errors. All values of ce or cs are therefore Pareto optimal. For all histories, Type II errors are increasing in k and weakly decreasing in cs. Accordingly, our analysis suggests that disease advocates, by placing more weight on Type II errors, would likely support lower quality standards and higher submission costs.            "
"56","Another kind of Type II error can occur if the submission of a product that R would accept is delayed. Due in part to the demand for treatments for HIV and other life‐threatening illnesses in the 1990s, patient advocates have pushed extensively for procedures that would reduce approval times, and hence delay‐induced errors.9 Our model provides a simple way of predicting submission delays. After one experiment, only the high type's product is acceptable to R. In the ESE, these products are submitted, while in the LSE they are not. An “unwanted” submission delay therefore occurs in the LSE with probability pm/n. The LSE requires that , which implies a low value of p. Thus, low ex ante confidence in the product inhibits early acceptances.10"
"57","A final class of Type II errors occurs if F stops development of a potentially acceptable product. This does not occur in our model because abandonments only occur when the low type withdraws after one failure. By assumption, such products could never be acceptable to R. However, if the model were extended to allow more experimentation, or cases where k < m/(n+ 2), then a low firm type might be induced to abandon a potentially good product after an initial experimental failure.            "
"58","Our statistical data for testing the model's predictions are drawn from a larger database of 32,216 molecular entities that have been developed to some stage of product maturity over the past 30 years under the system of pharmaceutical regulation in the United States. In part because U.S. regulatory authorities track drugs from preclinical stages to postapproval experience on the market, U.S. pharmaceutical development and regulation are some of the most heavily documented political‐economic activities in the world, offering large amounts of fine‐grained data for analysis.11 We have collected these data from a variety of sources, including Freedom of Information Act requests to the FDA, FDA archives, FDA Annual Reports from its drug reviewing divisions, F‐D‐C (Food, Drug, and Cosmetic) Reports, and the trade database Pharmaprojects™. Where our data concern recent products, we rely more heavily upon trade reporters and databases. Where our data concern older drugs, we rely much more heavily upon FDA sources, checked against the data of Dranove and Meltzer (1994) and Pharmaprojects™.         "
"59","Of the 32,216 molecular entities in total, 15,282 are “preclinical” drugs that have not yet been tested on humans. Of the remaining 16,934, we have reliable clinical development data on 16,723, of which 16.7%, or 2,789, were eventually submitted to the FDA. The others have been abandoned or are currently in a limbo R & D status. Our analyses here sample only the approved drugs, though it is worth remarking that a richer estimator would consider the quasi‐selection effect by which some drugs get submitted and others get abandoned."
"60"," Measuring Type I Errors. We confine our statistical analysis to Type I errors or “bad” approvals. The reason for doing so concerns the complexity of Type II errors. Coding Type II errors as drugs that are rejected in the United States but accepted and prescribed with relative success elsewhere would yield a very small set that precludes statistical analysis with regressors. The reason is that most drugs that are approved in overseas markets are eventually approved in the United States. It would also be almost impossible to know what drugs were wrongly rejected everywhere, as this would require information that was presumably unavailable to regulators and medical authorities. Hence Type II errors are better quantitatively measured in terms of delay, not absolute rejection, though this task presents considerable difficulties as well (Carpenter 2002, 2004; Dranove and Meltzer 1994).         "
"61","Even restricting our analysis in this way, measurement of Type I error presents some difficult questions. How would we know, after the fact, that the FDA had made a bad approval decision? There is no way of knowing this with certainty, but several observables would seem to be correlated with such events."
"62","First, a Type I error is more likely to have occurred when, after the approval of a new molecule, the FDA significantly revises the drug's labeling. That is, the FDA must attach evidence of new contraindications, or new side effects, that are serious in some way. This was precisely the intuition of the U.S. General Accounting Office (GAO) when it produced a report on “postapproval risks” of FDA‐approved drugs (U.S. GAO 1990). This report determined whether there had been a “significant” labeling revision for the drug, and if so, what those labeling revisions were. In part to ensure that the subjectivity of coding Type I errors is placed elsewhere, we adopt straightforwardly the GAO assessments for the dependent variable GAO Lines. GAO Lines is the number of lines of text describing significant labeling revisions in the report and is scored 0 for all unrevised drugs. This variable amounts to an implicit assumption that the Type I error was worse where more ex post revisions were added to the labeling, as detected and reported by the GAO. One drawback of these data is that we can only focus on NMEs approved from 1976 to 1985, and then only on the 198 drugs that the GAO selected for study. We are missing firm data on two of the drugs in the report, making our effective sample size 196 molecular entities.         "
"63","Second, if international regulatory decisions are at least somewhat independent of the FDA's, then we could infer Type I errors from approval and recall decisions for overseas markets for the same molecule, after approval by the FDA. Our second set of measures uses international comparisons in precisely this way. We define two indicators of a Type I error (many others are possible). We first code whether a drug was removed from at least one foreign market in a “highly developed” (HD) country (Withdrawn). One can adjust the cutoff here by specifying the number of withdrawals in HD countries (2, 5, 10, or more) necessary for the FDA's approval to qualify as a Type I error. Next, we also examine those HD markets where the drug never entered. If the drug failed to enter five or more markets in highly developed countries (No Entry), then it is likely that overseas regulators were less enthusiastic about the drug, and that they wished to avoid a Type I error that the FDA may have committed in approving the molecule.         "
"64"," Measuring Experimentation Cost            (ce). There are two dimensions along which ce can be thought to vary. The first, which we use here, is by firm (sponsor). One can think of experimentation cost as inversely related to firm size and experience. Larger firms should face lower experimentation costs, in part because they have (probably) already developed products with similar R & D processes in the past, in part because R & D obeys economies of scale. We measure size and experience jointly, by calculating the number of previous investigational new drugs (INDs) developed at the date of submission of the new drug application (NDA) for the molecular entity in question to the FDA. We label this variable Firm INDs and the log of this variable Log Firm INDs. This variable has the advantage of changing over time, by firm, allowing for firm‐specific effects in our estimations. This measure is also less sensitive to shocks than size measures based upon sales or revenues, which are highly dependent upon patent expiry and other “denominator” effects.12"
"65"," Measuring Submission Cost            (cs). It is more difficult to produce a finely grained measure of submission cost that would differ from experimentation cost. One tempting possibility is to consider onerous aspects of the regulatory process as a sort of submission cost. For instance, the FDA imposes hundreds of stylistic and procedural requirements upon new drug applications, and if the agency judges that a firm has not met these requirements, it can issue a “refusal‐to‐file” (RTF) judgment. One testable implication is that, where these requirements become more stringent, empirically observed acceptance probabilities will rise, ceteris paribus. However, changes in these regulatory submission requirements are difficult to measure other than by indicator (dummy) variables indexing when the requirements changed, and changes in these requirements are so common as to render this infeasible.         "
"66","The larger problem here is that any firm‐level variable measuring submission cost is likely to be highly correlated with firm‐level covariates of experimentation cost. Since the estimated costs of clinical development are much greater than the estimated costs of regulatory submission (Dranove and Meltzer 1994; Wardell and Lasagna 1975), we think it is preferable to use firm size to measure experimentation cost and not submission cost.         "
"67","We focus on a more blunt measure, namely the influence of the Prescription Drug User Fee Act (PDUFA) of 1992 and its expected effects upon drug approval times (Olson 2000). The law requires firms to submit a user fee with each new drug application to the FDA (in FY 2004, $573,500 for a new drug application requiring analysis of clinical data; Federal Register 68(148), August 1, 2003, pp. 45, 249–45, 252). The PDUFA dummy variable is scored 1 for products submitted after 1992.         "
"68","While PDUFA may have raised nomimal submission costs by requiring all sponsors to pay for each new NDA, in fact the law was widely predicted to reduce firm costs by accelerating FDA drug review. As part of the deal that secured the PDUFA legislation, it was agreed that the proceeds from user fee payments would go to hire more reviewers at the FDA's Center for Drug Evaluation and Research. It was understood by all that the explicit goal (and probable effect) of this legislation would be to reduce FDA review times for new drug applications. As one report described the legislation in 1992:"
"69","                        "
"70","In return for collecting what is expected to be about $300 million over the next five years from companies that want their drugs reviewed, Dr. David Kessler, the Commissioner of Food and Drugs, agreed to hire 600 new examiners to speed drug approval . … The plan is considered politically feasible because … F.D.A. estimates that companies may earn an average of $10 million for each additional month they have a drug on the market.13"
"71","As part of the 1992 agreement, the FDA agreed to review 90% of all “standard” new molecular entities within 12 months or less by 1997, and agreed to review 90% of “priority” drugs within six months or less. In part because of the review time goals and in part because of the enhanced resources, approval times did fall appreciably in the years following PDUFA's enactment. Importantly, while FDA review times fell following PDUFA's enactment, clinical development (experimentation) times did not.14 Hence the user‐fee regime seems to have affected submission costs much more than pre‐NDA experimentation costs.         "
"72","The remaining question is whether variation in approval times properly measures variations in submission cost. Here it is important to remember that, when a drug is submitted to the FDA, three phases of clinical trials have been completed.15 Hence the direct experimentation costs have already been expended. A reduction in FDA approval time then clearly reduces a drug's development cost but does not by definition influence the cost of experimenting with the drug. Hence a reasonable inference is that the user‐fee regime has reduced the submission‐cost component of drug development cost, and not the experimentation‐cost component.         "
"73","Since the user fees are quite small (ranging from $240,000 to about $1,000,000) relative to the capitalized benefits of earlier approval (Carpenter 2004; Carpenter et al. 2003), the implicit benefits of acceleration outweigh the explicit increase in user‐fee cost, and so the onset of PDUFA should be conceived as having reduced cs.         "
"74","While a firm‐level measure of submission cost would be superior, and while our measure is admittedly blunt, we believe that any firm‐level measure of submission cost would pick up variation in experimentation cost. We acknowledge the weaknesses of our measure, as other time‐varying factors could be influencing the FDA's error rate while not necessarily measuring submission cost."
"75"," Measuring Early and Late Submission Regimes. Because many of the model's comparative statics depend on the length of experimentation, we often restrict the sample to only those products with sufficiently long or short development times, and to only those products submitted by sufficiently large or small companies. Beginning with development time restrictions, for each drug, the variable IND Time measures the number of months it held IND status. To get at drugs that have been developed for a greater or lesser time, we compute quantile information for the IND Time distribution and use these quantiles as cutoffs. For example, if the mean of the IND Time is 72 months, one way of differentiating “long experimentation” from “short experimentation” is to consider all drugs for which IND Time< 72 as “short” and all drugs for which IND Time> 72 as “long.” As a robustness check and as a way of revealing theoretically relevant patterns in the data, we replicated our analyses across numerous cutoff points at different quantiles.16 We conduct an analogous set of “regime” restrictions for the Log Firm INDs variable.         "
"76"," Controls. For all analyses, we control for firm fixed effects.17 Where relevant, we also control for a brute time trend by including the year in which the NDA was submitted (Submission Year). The final control we use throughout is a blunt but effective measure of the severity or costliness of the disease targeted by the drug in question. For the primary indication of the drug, we calculate from the U.S. federal government's Health Care Utilization Project data, the average number of days of hospitalization per hospitalization. We call this variable Hospitalization Length and assign a value of zero whenever there are no recorded hospitalizations for a disease. This variable covaries positively with the mortality of a disease and its expected cost of treatment. It is also negatively and significantly associated with FDA approval times for NMEs (Carpenter 2002).         "
"77"," Summary. Descriptive statistics for the most important variables appear in Table 1. The first half of the table details the GAO data. Exactly half of in‐sample NMEs approved from 1976 to 1985 were subject to “serious” postapproval labeling revisions and therefore had nonzero GAO Lines values. The maximally revised labeling had 48 lines.         "
"78","The second half of the table covers the international comparison data. Here the frequency of Type I error as coded by international comparison is much smaller. Only 6.1% of NMEs approved by the FDA from 1984 to the present were approved but then withdrawn in one or more highly developed markets (Withdrawn). The fraction of entities approved by the FDA but that failed to enter five or more highly developed foreign markets is 2.9% for this period (No Entry). Notice that because our international comparisons data is of more recent vintage, the firm size variable has a sample distribution with higher mean and higher variance than the sample distribution for drugs studied by the GAO.         "
"79","We test the comparative statics of the model in a multivariate setting, using a mix of robust general linear models and maximum likelihood estimators. Some of the hypotheses are counterintuitive and depend on subsamples. To differentiate subsamples from one another, we use cutoffs given by the sample means of (logged) previous firm submissions and development time (in months; see Table 1). We do this in order to construct an artificial sample similarity with empirical regimes with low experimental cost and longer experimentation. Additionally, our measure of firm size, Firm INDs, is inversely related to ce. Table 2 summarizes the predicted coefficient signs by subsample.         "
"80","We begin with analysis of the GAO data. In Table 3, we regress GAO Lines on the relevant regressors using negative binomial regression.18 By Table 2, we have four predictions. First, for drugs that are submitted relatively early and by low‐ce firms, we predict that Type I error is increasing in Firm INDs. In specification (1) of Table 3, we present estimates derived from two truncated samples restricted to larger firms and drugs with shorter development times. There is some support for this hypothesis. The coefficient on Firm INDs is generally positive but attains statistical significance at the p < 0.05 level only in the smaller subsample where investigational development time is 72 months or less.19 The coefficient estimates in the sample with IND time less than 96 months is not statistically differentiable from zero. If the larger of these two estimates is used, a one standard deviation hike in previous INDs (39.4 INDs previously submitted) is associated with a 2.8‐line increase in the length of GAO listing.            "
"81","Our second prediction is that, for products characterized by longer experimentation times and submitted by larger firms, Type I errors should be decreasing in ce. By the results of the third section, long experimentation and low ce should be associated with a reversal of the comparative static on Firm INDs. In specification (2) of Table 3, we present estimates derived from truncated samples in which analysis is restricted to larger firms and drugs with longer development times. As predicted, the coefficient estimate on Firm INDs is negative, and this estimate is statistically significant. Here the marginal effects are smaller, about one‐half‐line increase in the GAO listing for the larger subsample (with development time greater than 72 months), and only a one‐eighth‐line increase in GAO listings for the smaller subsample.20"
"82","The final two predictions correspond to the high ce empirical regimes, where the model predicts that firm size should have no effect. Specifications (3) and (4) of Table 3 show that we cannot reject the null of zero, which modestly supports the hypotheses of zero association.            "
"83","Although we do not report them here, these results are generally replicated if we use generalized linear regressions with heteroskedasticity corrections for these variables. In particular, regressing Log GAO Lines (where ) upon Log Firm INDs yields results that are substantively identical to those of the maximum likelihood models.            "
"84","We now turn to the sample of drugs approved over the past 25 years (N= 1071) and employ our measures of Type I error as coded via international comparison. Again we use the relevant sample mean Log Firm INDs (approximately 3) as a cutoff in splitting subsamples from one another. Aside from the larger sample, the primary benefit of this dataset is that it allows us to test for the effects of the PDUFA dummy, PDUFA. We predict that PDUFA should decrease Type I errors under both short and long experimentation. We again include firm fixed effects. This results in a material reduction in sample size for these models, mainly because the fraction of ones in the sample dependent variable is so small.21 Hence any time that a firm has never had a foreign withdrawal in our dataset, or anytime that a firm has never been denied entry into five or more foreign markets, the inclusion of an indicator variable for this firm produces a perfect prediction, and the variable is dropped from the probit analysis.            "
"85","We first test our hypotheses by estimating GLS and probit regression models. Table 4 presents linear probability and probit regression results for the dependent variable Withdrawn. Here the results are quite interesting for the Log Firm INDs variable. In specification (1), where the sample is restricted to drugs produced by large firms and excludes drugs with long IND times, the coefficient estimates are positive but not statistically significant. However, in specification (2), where the sample is restricted to large firms and long IND times, we observe negative coefficient estimates, as predicted, and these estimates are statistically differentiable from zero.22 Marginal effects computations from the statistical model suggest that, in the “late‐submission equilibria” empirical regimes where we observe statistically significant associations, a one standard deviation increase in Log Firm INDs is associated with a 9.0 percentage point decline in Type I errors (this corresponds to a 97% decline in the sample rate of error). Finally, in the third and fourth empirical regimes (high ce, or low INDs), where the model predicts no variate relationship of experimentation cost and error, we cannot reject the null of zero.            "
"86","The estimates in Table 4 also allow for a partial test of our hypotheses on submission costs. If PDUFA lowered submission costs, it may have also induced a lower Type I error rate at the margin. Across the various subsamples defined by the submission and withdrawal conditions, there is inconsistent support for this hypothesis from Table 4. Specification (1) yields the predicted statistically significant negative coefficient estimate. Specifications (2) and (4) have the correct sign, but we cannot reject the null hypothesis of no relationship. Finally, the model predicts no relationship in specification (3), and we cannot reject that hypothesis.            "
"87","Better traction on this hypothesis is had from examining the “full sample” for effects of secular changes in the cost of submission. Because cs has a nonnegative comparative static in all cases, we can combine the cases and test for a positive relationship between submission cost and Type I error (equivalently, a negative relationship between PDUFA and Type I error). Table 5 shows the results of four regressions. In the first two specifications, the dependent variable is the same as in Table 4, Withdrawn, while the last two use No Entry. All models include a measure of submission cost as well as a time trend, which is the year in which the molecule in question commenced development. Some plausible evidence for the validity of these indicators comes in the fact that the measured covariates are of the same sign across specifications, significantly so for the hospitalization and time trend covariates. Notice that once the full sample is examined without theoretically relevant conditions and restrictions, simple linear inclusion of Log Firm INDs does not add to the model's predictive power and does not yield coefficient estimates statistically differentiable from zero.            "
"88","In the probit regressions (first and third columns), submission cost is measured as the (differenced) series in median approval time, calculated for the submission year of the molecule in question as . In toto, this represents firms' generally unconditioned expectations about the time‐ or delay‐cost of their submission. In the linear probability models, we instrument for  with PDUFA.23 The idea here is that PDUFA represents a shift in expected approval times that was anticipatable by submitting firms. Here we find more robust associations between the instrumented change in median approval time and Type I error rate.            "
"89","Rather strikingly, the estimates suggest that reductions in approval time may be associated with less Type I error, not more. All four estimates are positive, but they are statistically significant only in the No Entry regressions. This is counterintuitive in light of recent debates and arguments which suggest that PDUFA may have raised the occurrence of Type I errors (see Avorn 2004). Since our data offer no measure of severity and our model assumes homogeneity of Type I error, we cannot make statements here about PDUFA's welfare effects. Additionally, these relationships are hypothesized to hold at the margin, and so PDUFA may have had effects on other institutional parameters that influence error but are not captured by our model.            "
"90","We find appreciable support for our primary hypotheses, though the pattern of support differs based upon the hypothesis examined. For our more intuitive hypothesis—that where experimentation cost is low and development times are shorter, Type I regulatory error is increasing in firm size or experience—we find mixed support, with positive coefficient estimates that are statistically significant only in some subsamples. For our more counterintuitive hypothesis—that where experimentation cost is low and development is longer, increases in firm experience are accompanied by reductions in Type I error—we find consistent support across datasets and across specifications. This inversion of statistical associations occurs across two datasets and in each of two measures of Type I error in each dataset.24 The results of Tables 3 and 4 lend solid support to the predicted reversal of the comparative static on ce across the low‐ce subsamples.            "
"91","We can test for effects of submission cost only brutely, and only in our second set of analyses on more recent data. There, the coefficient estimates for  in Table 5 broadly support the hypothesis that by reducing the cost of submission, PDUFA lowered the probability of Type I errors. The coefficient for expected approval time is positive in all estimations and significant in two, as well as in a variety of specifications not reported here. If it can be safely interpreted that PDUFA implicitly reduced submission costs, then we have evidence that Type I error rates are decreasing in cs. However, all of our causal leverage here is nonlinear over‐time variation, so anything else that might have occurred in the past 10 years, that covaries with PDUFA but not with a linear trend, and that also covaries with Type I error rates, would confound our inferences. Among these, better technology for detecting drug hazards ex ante is one plausible candidate.            "
"92","We conclude with two observations. First, we do not attempt to attach any theoretical meaning to the estimated effect of Hospitalization Length. However, it should be noted that Hospitalization Length is only weakly associated with error as coded via international comparisons, but sometimes negatively and robustly associated with GAO‐coded errors. Second, one weakness with No Entry is that it may be determined as much by firms' decisions to avoid market entry as by regulators' decisions to deny entry. If this is true, and if a significant component of No Entry is affected by firm‐level variables that are not responsive to anticipation of regulatory strategy (e.g., portfolio factors in the company's pipeline), then it is possible that our inferences are subject to error from confounding variables.            "
"93","Regulatory decision making and error are frequently shaped by an endogenous agenda, which is determined by a firm's anticipation of the regulator's behavior. In this context, an important determinant of regulatory approval is the credibility of the firm's submission. The theory produces several counterintuitive predictions about the interaction between costs and the dynamics of research and development choices. To date, these predictions have not been generated by existing approaches to agency failure, either structural or cognitive. Empirical tests of some predictions about Type I error are supported by data on pharmaceutical relabeling, safety‐based withdrawals, and product introductions."
"94","These results may be useful for guiding both theoretical and empirical research on a large class of regulation and political economy problems. Two of these merit remark here. First, government grant making and procurement represent an appealing policy arena in which to apply the model. In the United States alone, the National Science Foundation and National Institutes of Health award over $23 billion in grants and contracts annually, while the Department of Defense spends over $84 billion on procurement annually (figures taken from the Budget Request of the President of the United States, FY 2007). Over $100 billion per year, then, is spent according to mechanisms that resemble the game laid out here. Our model would predict similar results based on the length and costs of development, as well as the characteristics of the research team or company. Second, while our empirical analysis concerns the regulation of pharmaceuticals in the United States, there are dozens of national and federation‐based regulators of drugs, medical devices, vaccines, and other health‐related goods throughout the world that play roles similar to that of the FDA. In principle, at least, the same sort of data could be gathered for these regulators as for the FDA.         "
"95","There remain features of approval regulation that are poorly captured by our model. First, we could incorporate a richer regulatory phase by allowing the regulator to audit, request additional testing, monitor the firm post‐approval, or decide according to more realistic decision rules (such as the voting rules used by FDA review teams). Second, the development phase may better reflect empirical reality by incorporating mandated minimum pre‐submission experiments (e.g., Phase I and Phase II clinical trials) or “fast track” procedures. Finally, the theory could incorporate actors such as a legislature, competing firms, or interest groups, who may differ in both their objectives and their abilities to detect and prevent errors. We underscore the difficulty of adding these to the present model while retaining analytic clarity, however. As it stands, the approval regulation framework provides a tractable foundation for studying bureaucratic decision making and error in an environment with endogenous information acquisition and submissions."
