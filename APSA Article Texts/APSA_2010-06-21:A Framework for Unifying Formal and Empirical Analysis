"","x"
"1","Methodological unification provides analytical transparency to support cumulative scientific practice. In more concrete terms, the EITM framework provides the necessary steps in attaining valid inference and prediction. For example, it is well known that when we specify that variable Y is a function of variable X, the statistical “tests” estimating a correlation between X and Y cannot determine causation between the two even when their correlation is statistically significant. Without unifying formal and empirical analysis we lack a basic analytical attribute suitable for identifying the following possibilities defining the relation between X and Y. A significant statistical result between X and Y can be due to (a) X causing Y directly; (b) X causing an unknown variable, Z, which causes Y; and (c) X and Y being caused by an unknown common factor, W, but there is no causality between X and Y. Too often researchers, using applied statistical methods, end up inferring the significant correlation to be result “a.”            "
"2","There have been numerous applied statistical attempts addressing these challenges to inference and prediction. Most econometric textbooks provide “solutions” such as instrumental variables (IV) estimation and different robustness checks, including EBA (Leamer 1983). However, these applied statistical tools, when used in isolation, lack power since they are not linked to a formal model.10 Of course, formal models are simplifications of what is studied. Nevertheless, they systematically sort rival arguments and confounding factors in relating X to Y.11 If formalized predictions are inconsistent with empirical tests, theory—as represented in the formal model—needs adjustment.12"
"3","Past academic research provides a scientific foundation for unifying formal and empirical analysis. The Cowles Commission, for example, established conditions in which structural parameters are identified within a model. It explored the differences between structural and reduced‐form parameters. Along with their work on structural parameters, Cowles Commission members also gave formal and empirical specificity to issues such as exogeneity and policy invariance (Aldrich 1989; Christ 1994; Heckman 2000; Morgan 1990).            "
"4","These contributions rested, in part, on a scientific vision merging formal and applied statistical analysis. The basis for this linkage was the idea that random samples were governed by some latent and probabilistic law of motion (Haavelmo 1944; Morgan 1990). This viewpoint meant formal models, when related to an applied statistical model, could be interpreted as creating a sample draw from the underlying law of motion. A test of a theory was accomplished by relating a formal model to an applied statistical model and testing the applied statistical model.            "
"5","While sharing some similarities with the Cowles Commission approach, the EITM framework possesses modifications. First, if one were to adhere to the Cowles Commission approach, we forego the chance of modeling new uncertainty created by shifts in behavioral traits. The consequence of this omission directly affects the issues of identification and invariance because these unaccounted behavioral shifts of variables would not be linked with the other variables and specified parameters. Ex ante predictions are adversely affected (Lucas 1976). To address this issue, and to give greater support for ex ante predictions, we emphasize modeling human behavior so new uncertainties due to shifts in behavioral traits such as public tastes, attitudes, expectations, and learning are properly studied.            "
"6","A second issue concerns the modeling process itself. A discipline such as political science studies the interactions between agent behavior and public policies all the time, but current research practices do not typically create formal models to predict or analyze these interactions. The Cowles Commission is associated with building a system of equations and then following rules (rank and order conditions) for identification that count equations and unknowns. In contrast, our EITM framework is agnostic on the choice to build and relate a system or to partition the system (via assumption) into a smaller set of equations, even a single equation. Priority is given to leveraging the mutually reinforcing properties of formal and empirical analysis."
"7","A final point on model specification addresses the critiques of the structural approach leveled by Sims (1980). It is well known that structural parameters are not identified from reduced form estimates. The practice of finding ways to identify models leads to “incredible” theoretical specifications (Freeman, Lin, and Williams 1989; Sims 1980). The proposed EITM framework, through the use of behavioral concepts and analogues, addresses Sims's criticisms in a theoretically meaningful way. Analogues, in particular, have important scientific importance since they hold the promise of operationalizing mechanisms.13"
"8","This EITM framework is summarized as follows."
"9","Concepts of particular concern in this framework reflect many overarching social and behavioral processes. Examples include (but are not limited to) decision making, expectations, and learning."
"10","It is also important to find an appropriate statistical concept to match with the theoretical concept. Examples of applied statistical concepts include (but are not limited to) persistence, measurement error, nominal choice, and simultaneity."
"11","To link concepts with tests, we need analogues. An analogue is a device representing a concept via a continuous and measurable variable or set of variables. Examples of analogues for the behavioral (formal) concepts such as decision making, expectations, and learning include (but are not limited to) decision theory (e.g., utility maximization), conditional expectations (forecasting) procedures, and adaptive and Bayesian learning (information updating) procedures."
"12","Examples of applied statistical analogues for the applied statistical concepts of persistence, measurement error, nominal choice, and simultaneity include (respectively) autoregressive estimation, error‐in‐variables regression, discrete choice modeling, and multistage estimation (e.g., two‐stage least squares)."
"13","The third step unifies the mutually reinforcing properties of the formal and empirical analogues. There are various ways to establish the linkage. For example, when researchers assume citizens (voters) or economic agents are rational actors who make decisions to maximize their own payoffs, a common analogue is utility (or profit) maximization. With this theoretical analogue in place, the other step is to determine the appropriate statistical concept and analogue to test the theoretical relation. Consider a basic Downsian model of voting. Voters decide to vote for one of the parties to maximize their utilities. This theoretical concept and analogue can be unified with the applied statistical concept, nominal choice, and its analogue, discrete choice modeling."
"14","In this section, we discuss four EITM examples. These examples contain, to varying degrees, the basic steps of the EITM framework. We also leverage the authors' EITM approach to show how their respective models and tests extend to new formalizations or tests or a combination of both."
"15","The act of voting provides a useful window into methodological unification. In Hotelling (1929) and Downs (1957), voters choose one party over the others based on the relative political positions of parties—proximity voting theory. Voters are more likely to vote for a political party if the position of the party is closer to voters' ideal position. As the party's position further deviates from a voter's ideal position, the voter receives less utility and is less likely to vote for it.14 While the voting literature finds some empirical support for the proximity model (see Blais et al. 2001), Kedar (2005) believes this effect diminishes if the institutional environment involves more power sharing.            "
"16","The Relation between Decision Theory and Discrete Choice Models In this example, decision theory and discrete choice serve as the EITM relation. Kedar (2005) asserts that, along with the proximity of parties' positions, voters are also concerned about each party's contribution to the aggregate policy outcome. She begins with the proximity model:                  "
"17","Assuming party positions affect policy outcomes, Kedar (2005) specifies the policy outcome as a weighted average of policy positions of the respective parties:                  "
"18","If voters are policy‐outcome oriented and concerned the policy outcome may deviate from their ideal point if party j is not elected, then the utility of voter i pertaining to party j becomes:                  "
"19"," Equation (3) provides an important insight on how voters view the contribution of party j to the policy outcome affecting their utility. If party j takes part in policy formulation and makes the policy closer to voter i's ideal point , that is, , then voter i gains positive utility when party j is involved (i.e., ). However, if the inclusion of party j makes the policy outcome increase in distance from voter i's ideal point such that  then the utility of voter  for party j is negative.               "
"20","Now assume voter i has expectations concerning party j based on the weighted average of both the party's relative share position and its contribution to policy outcomes. With this analogue for expectations, voter i's utility for party j is:                  "
"21","From equation (5) voter i's optimal or “desired” position for party j is obtained by solving the first‐order condition of  with respect to :                  "
"22","Unifying and Evaluating the Analogues The empirical tests follow directly from the theoretical model (i.e., equation 5). In equations (7)–(9), voters make an optimal voting decision based on representational (proximity) and compensational voting considerations. The theoretical prediction, γ is between zero and one, reflects the degree of political bargaining in different institutional systems. In majoritarian systems, where the winning party implements its ideal policy with less need for compromise, voters are expected to place greater value on γ and vote for the party positioned closest to their ideal position. However, in the case where institutional power sharing exists (i.e., γ is small), voters select a party whose position is further from their ideal positions to draw the collective outcome closer to their, the voters', ideal point.               "
"23","Methodological unification occurs when Kedar derives an empirical analogue for discrete choice. The log‐likelihood multinomial model is based on equation (5), and it estimates issue voting in four political systems using three measures: (1) seat shares in the parliament, (2) vote shares, and (3) portfolio allocation in government. The following hypotheses are tested:               "
"24","                                    "
"25","Voters' behavior in the countries with a majoritarian system follows the proximity model more closely (larger γ) than those in the countries with a consensual system (smaller γ).15"
"26","The pure proximity model  does not sufficiently represent voting behavior.                        "
"27","Kedar tests these empirical implications (based on the value of γ) using survey data from Britain, Canada, the Netherlands, and Norway. The empirical results support the first theoretical hypothesis: voting behavior in the majoritarian systems (i.e., Britain and Canada) is more consistent with the proximity model relative to consensual systems (i.e., the Netherlands and Norway). Hypothesis 2 is tested using a likelihood ratio test. Kedar shows that γ is significantly different from 1 in all four political systems. This result suggests compensational voting behavior exists in the particular sample. The pure proximity model is an insufficient explanation."
"28","Leveraging EITM and Extending the Model In forming the behavioral mechanism of decision making, Kedar chooses utility maximization as an analogue: voters select their ideal party position or policy outcome or both by maximizing their utility. The author links the theoretical findings of the optimal choice model to multinomial estimation. One way to build on her formal model is to relax the behavioral assumption that voters' expectations are error free since it is well known that equilibrium predictions change when expectations are based on imperfect or limited information. The extension amends the formal model of voter expectations to incorporate modern refinements on how voters adjust and learn from their expectation errors. Leveraging Kedar's EITM design allows us to draw (empirical) implications on how voter expectations and learning affect ex ante model predictions.               "
"29","A substantial economic voting literature exists. One specific area starts with Kramer (1983) and extends to Alesina and Rosenthal (1995). The features of these studies are the refinements in voter sophistication and applied statistical tests. In the former regard, voters possess the capability to deal with the uncertainty in assigning blame or credit toward incumbents for good or bad economic conditions. For the latter, applied statistical tests include some of the more advanced tools in time‐series analysis. In this example, we focus on Alesina and Rosenthal (1995).            "
"30","The Relation between Expectations, Uncertainty, and Measurement Error The formal model contains the behavioral concepts of expectations and uncertainty (Alesina and Rosenthal 1995, 191–95). Their model of economic growth is based on an expectations‐augmented aggregate supply curve:                  "
"31","With voter inflation expectations established, we turn to the concept of uncertainty. Assume voters try to determine whether to attribute credit or blame for economic growth  outcomes to the incumbent administration. However, they face uncertainty in determining what part of economic performance comes from incumbent competence (i.e., policy acumen) or simply good or bad luck. Uncertainty is based on the stochastic shock in equation (10). The analogue in equation (11) is commonly referred to as a “signal extraction” or measurement error problem:                  "
"32","Competence can persist and support reelection. This feature can be characterized as an MA(1) process:                   "
"33"," Alesina and Rosenthal (1995) tie economic growth performance to voter uncertainty. If “rational” voters predict inflation with no systematic error (i.e., ), then economic growth rate deviations from the average are attributed to administration competence or chance events:                  "
"34","To demonstrate this behavioral effect, the authors make use of conditional expectations as an analogue for the optimal forecast of competence . In particular, let rational voters form conditional expectations of  in equation (14) by observing the composite error  given all available information , and  at time t:                  "
"35","Unifying and Evaluating the Analogues Using the method of recursive projection and equation (15), we link the behavioral analogue for expectations to the empirical analogue for measurement error (an error‐in‐variables “equation”):                  "
"36","In equation (16), the expected value of competence is positively correlated with economic growth rate deviations. Voter assessment is filtered by the coefficient, , which represents the proportion of competence voters observe and interpret. The behavioral implications are straightforward. If voters interpret that variability of economic shocks comes solely from the incumbent's competence (i.e., ), then . On the other hand, the increase in the variability of uncontrolled shocks, , confounds the observability of incumbent competence since the signal coefficient  decreases. Voters assign less weight to economic performance in assessing incumbent competence.               "
"37"," Equations (10)–(12) serve as the formalization for tests using U.S. data on economic outcomes and political parties for the period 1915 to 1988. The theoretical model is based, in part, on equation (12). It specifies that competence, , follows an MA(1) process. The MA process is important since it has properties to capture short‐term changes. When the incumbent party is in office for the second period, the authors argue the covariance of  is larger for the incumbent party than if a new party had taken office in that same period.               "
"38","Alesina and Rosenthal estimate and compare the size of covariances within party and between parties to test this argument. They first estimate equation (10) to collect the estimated exogenous MA shocks, , and restrict the variance‐covariance structure of . The authors report null findings (i.e., equal covariances) and conclude there is little evidence suggesting voters are retrospective and use incumbent competence as a basis for support.               "
"39","Leveraging EITM and Extending the Model Alesina and Rosenthal provide an EITM connection between equations (10), (12), and their empirical tests. They link the behavioral concepts—expectations and uncertainty—with their respective analogues (conditional expectations and measurement error) and estimate the variance‐covariance structure of the residuals to test this relation. Yet, Alesina and Rosenthal's model is tested in other ways. For example, the empirical model resembles an error‐in‐variables specification, which is testable using dynamic methods such as rolling regression (Lin 1999). Alternatively, one could evaluate the competence equation (16) and use different measures for uncertainy such as the empirical tests and measures Suzuki and Chappell (1996) use for permanent and temporary changes in economic growth.17"
"40","In the post–World War II era there have been several regime shifts in U.S. macroeconomic policy (e.g., Bernanke et al. 1999; Taylor 1999). One line of research has focused on the use of interest rate rules in new Keynesian models (see Clarida, Gali, and Gertler 2000). These models are not generally used in political science since classic work in macro political economy does not typically rely on structural models.18 While structural models are few in number, Granato and Wong (2006) derive a model with new Keynesian properties to determine the relation between inflation‐stabilizing policy, inflation persistence and volatility, and business cycle fluctuations. Extending this model to various political phenomena, particularly as it pertains to policy (e.g., policy rules), is feasible.19"
"41","The Relation between Expectations, Learning, and Persistence This example demonstrates an EITM relation between expectations, learning, and persistence.20 Based on Fuhrer (1995) and Fuhrer and Moore (1995), this small structural model of macroeconomic outcomes and policy follows the Cowles Commission tradition, but it also contains behavioral analogues for expectations and learning.21 The details in the model are as follows. First, a two‐period contract is assumed where prices reflect a unitary markup over wages. The price at time , is expressed as the average of the current  and the lagged  contract wage:22"
"42"," Equation (19) captures the behavioral characteristics contributing to inflation persistence. Since agents make plans about their real wages using both past and future periods, the lagged price level  is taken into consideration as they adjust (negotiate) their real wage at time t. This model feature allows the inflation rate to depend on both the expected inflation rate as well as past inflation.               "
"43"," Equation (20) represents a standard IS curve where the quantity demanded (signified by ) is negatively associated with the changes in real interest rates:                  "
"44","Policymakers are assumed to follow an interest rate rule, the Taylor rule, when conducting monetary policy (Taylor 1993):                  "
"45","To determine the equilibrium inflation rate, first substitute equation (21) into equation (20). Next, solve for  and then put that result into equation (19). The expression for  is:                  "
"46","Unifying and Evaluating the Analogues Methodological unification occurs when we solve for the REE since this step involves merging the behavioral analogue of expectations with the applied statistical analogue for persistence. Take the conditional expectations at time  of equation (22) and substitute this result into equation (23):                  "
"47","A final step in the modeling process is to use the adaptive learning analogue and determine if the REE is unique and stable.25 Based on the magnitude of the model's parameters, we determine the properties of the quadratic solutions and solve for the relation between aggressive inflation‐stabilizing policy  and inflation persistence . The two values are defined as:                  "
"48","Quarterly U.S. data (for the period 1960:I to 2000:III) are used to test the relation between the policy parameter(s) and inflation persistence. According to the model, inflation persistence should fall under an aggressive inflation‐stabilizing policy.26 From equation (23) Granato and Wong estimate a first‐order autoregressive process (i.e., AR(1)) of the U.S. inflation rate. The formal model predicts a positive inflation stabilization policy parameter  reduces inflation persistence,  Granato and Wong also estimate equation (21) in order to contrast the parameter movements in  and .27"
"49"," Figure 1 provides point estimates of inflation persistence  and policy rule parameters,  and , for a 15‐year rolling sample starting in the first quarter of 1960 (1960:I). Figure 1 shows both  and  de‐emphasize inflation and output stability in approximately 1968. Prior to 1968, countercyclical policy emphasized output stability . Aggressive inflation‐stabilizing policy occurs only after 1980, when . Consistent with the model's predictions, inflation persistence falls after this policy change in 1980.               "
"50","                 Inflation Persistence and Policy Parameters                            "
"51","Leveraging EITM and Extending the Model The model presented here, in some respects, is a mirror of Kedar's (2005) model. Kedar emphasizes microfoundations but makes assumptions about voter expectations. This example, on the other hand, has an explicit analogue for expectations, but little in the way of microfoundations or in the strategic interaction between policymakers and the public. In addition, the policy rule (21) is devoid of any political and social factors. Both the inflation target variable  and the response parameters  could be made endogenous to political and social factors, including (but not limited to) partisanship, elections, and social interaction where information levels are heterogeneous (Granato, Guse, and Wong 2008).               "
"52","In previous studies of turnout, researchers have used discrete choice models to estimate the probability of voting. The explanatory variables in these empirical models include ad hoc transformations and lack a formal theoretical foundation. For example, age, the square of age, education level, and the square of education level are used. The variables are included typically for the sake of a better statistical fit within the sample, but they lack power for policy and intervention analysis where behavioral responses are important modeling considerations. Achen (2006) uses an EITM framework to link an ex ante theoretical prediction, based on Bayesian analysis, with an applied statistical analogue—“double‐probit.” He finds the fit of his model is superior to a traditional and rival applied statistical specification.            "
"53","The Relation between Decision Theory, Learning, and Discrete Choice Models Achen assumes a voter receives positive expressive utility of voting if he expects the true value of the difference between two parties in the next period, , to be different from zero (where n is the number of prior elections that the voter experiences). Voters also possess imperfect foresight about the true value of the party differences. Instead, the voter “learns” the expected value based on his information set (updated using a Bayesian mechanism).               "
"54","The subjective (expected) distribution of  is written as:                  "
"55","For theoretical convenience, Achen (2006) assumes  is nonnegative: the voter only votes for the party valued higher than another. Following Downs (1957), Achen (2006) suggests the utility of voting in period  is the difference between the expected benefit of voting,  and the cost of voting:                  "
"56","The learning process is characterized as follows. The voter's observed difference in party benefits is:                   "
"57","The same Bayesian procedure is used to update the posterior mean of the PID difference . It is based on the posterior mean of PID at time n (i.e., , in equation (28)), campaign information,  in equation (29), and the trusted information source,  at time :                  "
"58","Unifying and Evaluating the Analogues Achen approximates the population mean probability of voting given the information as follows. Let there be a critical level of utility, call it , such that if , the voter will vote, otherwise the voter will not. Given the normality assumption for the utility distribution, we construct the probability that  is less than or equal to U based on the normal cdf:                  "
"59","To estimate the determinants of voting turnout, Achen presents the probit model which follows from equation (32). Using maximum likelihood estimation, Achen (2006) estimates simultaneously two normally distributed cdf's in equation (32): a double‐probit. To interpret the coefficients, we first focus on the inner normal cdf. If the voter does not have any accurate information about the future, that is,  then . In this case, equation (32) is equivalent to:                  "
"60","On the other hand, if the voter is fully informed and the posterior precision of information is quite large, that is,  then  Therefore, we have:                  "
"61","To estimate equation (32), Achen uses the variables systemtime and education as the proxies for PID, , and campaign information, , respectively. Systemtime is defined as the voter's age minus 18 years. Education is classified as six categories: (1) No High‐School, (2) Some High‐School, (3) High‐School Degree, (4) Some College, (5) College Degree, and (6) Postgraduate Level.               "
"62","Achen argues the age of voters (systemtime) shows the strength of PID while education level is an attribute in understanding campaign information.29 Based on the availability of data, the theoretical model (32) is used to estimate the following double‐probit model:                  "
"63"," Achen (2006) uses voter turnout data from the 1998 and 2000 Current Population Surveys (CPS) and the Annenberg 2000 presidential election study. When he contrasts his EITM‐based model with traditional applied statistical models in the existing literature, he finds his model has a better fit. Equally important, when the focus turns to the parameters in Achen's model, the empirical estimates are consistent with the theoretical predictions of his model (see equation (32)). For example, the estimated values of c and α range between 1.212 and 2.424 and between 3.112 and 4.865, respectively. These values are statistically indistinguishable from the values predicted in his model.               "
"64","Leveraging EITM and Extending the Model Achen uses the behavioral concepts of rational decision making and learning. His behavioral analogues are utility maximization and Bayesian learning, respectively. He links these behavioral analogues with the applied statistical analogue for discrete choice: probit. To accomplish this EITM linkage he assumes the voting decision and Bayesian learning are normally distributed events. With that assumption in place, his formal model is tested using two probit regressions simultaneously.               "
"65","There are many ways to leverage Achen's EITM model. One of the more important extensions is to take advantage of the dynamic properties in his theory and model. Retrospective evaluations are assumed in the model, but there is no specification or test on how long these evaluations persist or how long a voter's memory lasts. We know, for example, that in matters of policy, retrospective judgments by the public have a profound influence on policy effectiveness. Equally important, persistence analogues exist to complete the unification process."
"66","In this article, we demonstrate a framework that provides formalized behavioral explanations suitable for statistical inference and prediction. EITM not only builds on the Cowles Commission's work to recover a model's parameters, but it also addresses both Lucas's (1976) and Sims's (1980) critiques of conventional applied statistical estimation practice. A way to address these criticisms is to ensure analogues are tied to concepts. We work through the properties of the analogues and focus on the relation between the formal‐theoretical parameter(s) and the applied statistical parameter(s). Explicit emphasis on the parameters allows for greater likelihood of knowing what is being tested. Ultimately, EITM means a clean break from the methodological status quo. No half‐measures will suffice if the goal is to build a cumulative science based on the transparency between theory and test.         "
"67","The EITM framework, because it runs counter to current practice, raises new challenges. One challenge is practical. The overall process of methodological unification, applying the EITM framework, and the diverse examples used means a reorientation in methodological training. There are well‐known differences between formal and empirical approaches, and current training reflects the siloed thinking. This framework serves as a way to create courses and teaching modules where research questions dictate the analogues used and make the “how” we examine the problem equal in emphasis to “what” we study."
"68","Two technical challenges emerge. One technical challenge is the development of analogues. Unlike the natural sciences, the social sciences study human subjects who possess expectations affecting their current behavior. This “dynamic” creates moving targets for many social science questions. How to improve upon current analogues for distinctly human behavioral traits (e.g., expectations and learning) is a key future hurdle to scientific cumulation."
"69","A second technical challenge relates to the framework's emphasis on parameters as a building block for ex post and ex ante prediction (see Bamber and van Santen 1985, 2000). It is almost impossible to capture all parameters in complex political, social, and economic systems. However, the EITM framework is useful since it helps researchers open the “black box” relating different theoretical parameters to the estimated coefficients in an empirical model.30 A more general point is the EITM framework's focus on parameters separates variables that aid in fundamental prediction from other variables considered “causal” but are of minor predictive importance."
"70","The EITM framework is part of an ongoing process geared toward methodological unification (see Morton 1999). What we describe can be extended in many ways. Unified methodological frameworks now exist across different levels of analysis (Freeman and Houser 1998; Kydland and Prescott 1982), make explicit use of game theory (Mebane and Sekhon 2002; Signorino 1999), or are part of a mutiple‐method research design (Poteete, Janssen, and Ostrom 2010). Experiments also provide a rich alternative or complement to the secondary data analysis we use in this article and are a natural outlet for methodological unification. A good deal of EITM research will no doubt take place using experimental methods because of the relative ease in connecting a formal model and prediction to tests.31"
