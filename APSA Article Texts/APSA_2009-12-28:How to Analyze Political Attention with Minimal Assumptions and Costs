"","x"
"1","Each method for analyzing textual content imposes its own particular set of assumptions and, as a result, has particular advantages and weaknesses for any given question or set of texts. We focus our attention here on the basic problem of categorizing texts—placing texts into discrete target categories or bins.2 Methods of text categorization vary along at least five dimensions: (1) whether they take the target categories as known or unknown, (2) whether the target categories have any known or unknown relationships with one another, (3) whether the relevant textual features (e.g., words, nouns, phrases, etc.) are known or unknown, (4) whether the mapping from features to categories is known or unknown, and (5) whether the categorization process can be performed algorithmically by a machine. We are at pains, in particular, to describe how five ways of categorizing texts—reading, human coding, automated dictionaries, supervised learning, and the topic model we describe here—fill distinctive niches as tools for political science.         "
"2","Each of these five methods comes with unique costs and benefits. We find it useful to think of these costs along two main dimensions: (1) the extent to which the method requires detailed substantive knowledge and (2) the length of time it would take a single person to complete the analysis for a fixed body of text. Each of these two types of costs can be incurred at three stages of the analysis: the preanalysis phase where issues of conceptualization and operationalization are dealt with (perhaps in one or more pilot studies), the analysis phase where the texts of interest are categorized, and the postanalysis phase where the results from the analysis phase are interpreted and assessed for reliability and validity. Tables 1A and 1B depict how five major methods of text categorization compare in terms of their underlying assumptions and costs, respectively. The cell entries in Table 1A represent the minimal assumptions required by each method.         "
"3","In the most general sense, the fundamental “method” for inferring meaning from text is reading. For example, one reader of a specific journal article might attempt to place that article into one of a set of substantive categories (e.g., legislative studies / agenda setting / methodology / text analysis), while another reader might categorize the text in terms of its relevance (cite / request more information / ignore). Not only might the relevant categories change by reader, but a given reader will create new categories as more information about the text becomes apparent.         "
"4","For some target sets of categories, we could delineate specific features of the text that make particular categories more likely. We can imagine that words like Congress or legislature make it more likely that we place an article under “legislative studies,” that typesetting in  or multiple equations makes it more likely that we place it under “methodology,” and so on. For other target concepts, the relevant features are more abstract. To place it in the “cite” bin, we might require that the text display features like importance and relevance. Different readers may disagree on the salient features and their presence or absence in any particular text. This is important for the promise of automation via algorithm. We all use search engines that are useful at helping us find articles that are topically relevant (Google Scholar, JSTOR) or influential (Social Science Citation Index), but we would be more skeptical of an algorithm that attempted to tell us whether a given article should be cited in our own work or not.         "
"5","As one might expect—since all automated methods require at least some human reading—the act of reading a text rests on fewer assumptions than other methods of text categorization. The number of topics is not necessarily fixed in advance, the relationships between categories are not assumed a priori, texts can be viewed holistically and placed in categories on a case‐by‐case basis, and there is no attempt to algorithmically specify the categorization process. This allows maximum flexibility. However, the flexibility comes with nontrivial costs, especially when one attempts to read large, politically relevant texts such as the British Hansard or the U.S. Congressional Record. More specifically, human reading of text requires moderate‐to‐high levels of substantive knowledge (the language of the text and some contextual knowledge are minimal but nontrivial requirements) and a great deal of time in person‐hours per text.3 Finally, condensing the information in a large text requires a great deal of thought, expertise, and good sense. Even in the best of situations, purely qualitative summaries of a text are often open to debate and highly contested.         "
"6"," Human coding (see, for instance, Ansolabehere, Snowberg, and Snyder 2003; Budge et al. 2001; Ho and Quinn 2008; Jones, Wilkerson, and Baumgartner n.d.; and Klingemann et al. 2006) is the standard methodology for content analysis, and for coding in general, in social science. For such manual coding, the target categories of interest are assumed to be known and fixed. Coders read units of text and attempt to assign one of a finite set of codes to each unit. If the target categories have any relationship to each other (e.g., nesting), it is assumed to be known. There is typically no requirement that the readers use any particular feature in identifying the target category and the exact mapping from texts to categories is assumed unknown and never made explicit. One can tell, through reliability checking, whether two independent coders reach the same conclusion, but one cannot tell how they reached it. Manual coding is most useful when there are abundant human resources available, the target concepts are clearly defined a priori, but the mapping from texts to categories is highly complex and unknown (“I know it when I see it”).         "
"7","By using clearly defined, mutually exclusive, and exhaustive categories to structure the coding phase, human coding methods require less substantive knowledge than would be necessary in a deep reading of the texts. Nevertheless, the texts do still need to be read by a human (typically a research assistant) who is a competent reader of the language used in the texts. Further, some moderate contextual knowledge is required during this phase so that texts are interpreted in the proper context. While human coding is less costly than deep reading during the analysis phase, it has higher initial costs. In particular, arriving at a workable categorization scheme typically requires expert subject‐matter knowledge and substantial human time."
"8","The first steps toward automation can be found in dictionary‐based coding, which easily carries the most assumptions of all methods here. Examples include Gerner et al. (1994), Cary (1977), and Holsti, Brody, and North (1964). In dictionary‐based coding, the analyst develops a list (a dictionary) of words and phrases that are likely to indicate membership in a particular category. A computer is used to tally up use of these dictionary entries in texts and determine the most likely category.4 So, as with manual coding, target categories are known and fixed. Moreover, the relevant features—generally the words or phrases that comprise the dictionary lists—are known and fixed, as is the mapping from those features into the target categories. When these assumption are met, dictionary‐based coding can be fast and efficient.         "
"9","As with human coding, dictionary methods have very high startup costs. Building an appropriate dictionary is typically an application‐specific task that requires a great deal of deep application‐specific knowledge and (oftentimes) a fair amount of trial and error. That said, once a good dictionary is built, the analysis costs are as low or lower than any competing method. A large number of texts can be processed quickly and descriptive numerical summaries can be easily generated that make interpretation and validity assessment relatively straightforward."
"10","A more recent approach to automation in this type of problem is supervised learning (Hillard, Purpura, and Wilkerson 2007, 2008; Kwon, Hovy, and Shulman 2007; Purpura and Hillard 2006). Hand coding is done to a subset of texts that will serve as training data and to another subset of texts that serve as evaluation data (sometimes called “test data”). Machine‐learning algorithms are then used to attempt to infer the mapping from text features to hand‐coded categories in the training set. Success is evaluated by applying the inferred mapping to the test data and calculating summaries of out‐of‐sample predictive accuracy. Gains of automation are then realized by application to the remaining texts that have not been hand coded. There are a wide variety of possible algorithms and the field is growing. Again, note that target categories are assumed to be known and fixed. Some set of possibly relevant features must be identified, but the algorithm determines which of those are relevant and how they map into the target categories. Some algorithms restrict the mapping from text features to categories to take a parametric form while others are nonparametric.5"
"11","Since supervised learning methods require some human coding of documents to construct training and test sets, these methods have high startup costs that are roughly the same as human‐coding methods. Where they fare much better than human‐coding methods is in the processing of the bulk of the texts. Here, because the process is completely automated, a very large number of texts can be assigned to categories quite quickly."
"12","In the same way that supervised learning attempts to use statistical techniques to automate the process of hand coding, our topic model attempts to automate the topic‐categorization process of reading. The key assumption shared with reading, and not shared with hand coding, dictionary‐based coding, or supervised learning, is that the target categories and their relationships with each other are unknown. The target categories—here, the topics that might be the subject of a particular legislative speech—are an object of inference. We assume that words are a relevant feature for revealing the topical content of a speech, and we assume that the mapping from words to topics takes a particular parametric form, described below. The topic model seeks to identify, rather than assume, the topical categories, the parameters that describe the mapping from words to topic, and the topical category for any given speech."
"13","The topic‐modeling approach used in this article has a very different cost structure than all methods mentioned so far. Whereas other methods typically require a large investment in the initial preanalysis stage (human coding, dictionary methods, supervised learning) and/or analysis stage (reading, human coding), our topic model requires very little time or substantive knowledge in these stages of the analysis. Where things are reversed is in the postanalysis phase where methods other than deep reading are relatively costless but where our topic model requires more time and effort (but no more substantive knowledge) than other methods. The nature of the costs incurred by the topic model become more apparent below.         "
"14","The data‐generating process that motivates our model is the following. On each day that Congress is in session a legislator can make speeches. These speeches will be on one of a finite number K of topics. The probability that a randomly chosen speech from a particular day will be on a particular topic is assumed to vary smoothly over time. At a very coarse level, a speech can be thought of as a vector containing the frequencies of words in some vocabulary. These vectors of word frequencies can be stacked together in a matrix whose number of rows is equal to the number of words in the vocabulary and whose number of columns is equal to the number of speeches. This matrix is our outcome variable. Our goal is to use the information in this matrix to make inferences about the topic membership of individual speeches.6"
"15","We begin by laying out the necessary notation. Let t = 1, …, T index time (in days); d = 1, …, D index speech documents; k = 1, …, K index possible topics that a document can be on; and w = 1, …, W index words in the vocabulary. For reasons that will be clearer later, we also introduce the function . s(d) tells us the time period in which document d was put into the Congressional Record. In addition, let ΔN denote the N‐dimensional simplex.         "
"16","The dth document yd is a W‐vector of nonnegative integers. The wth element of yd, denoted ydw, gives the number of times word w was used in document d. We condition on the total number nd of words in document d and assume that if yd is from topic k"
"17","If we let πtk denote the marginal probabilities that a randomly chosen document is generated from topic k in time period t, we can write the sampling density for all of the observed documents as               "
"18","As will become apparent later, it will be useful to write this sampling density in terms of latent data z1, …, zD. Here zd is a K‐vector with element zdk equal to 1 if document d was generated from topic k and 0 otherwise. If we could observe z1, …, zD we could write the sampling density above as               "
"19","To complete a Bayesian specification of this model we need to determine prior distributions for θ and π. We assume a semiconjugate Dirichlet prior for θ. More specifically, we assume               "
"20","The prior for π is more complicated. Let πt∈ΔK−1 denote the vector of topic probabilities at time t. The model assumes that a priori               "
"21","In what follows we specify Ft and Gt as a local linear trend for ωt:               "
"22","Viewed as a clustering/classification procedure, the model above is designed for “unsupervised” clustering. At no point does the user pretag documents as belonging to certain topics. As we will demonstrate below in the context of Senate speech data, our model, despite not using user‐supplied information about the nature of the topics, produces topic labelings that adhere closely to generally recognized issue areas. While perhaps the greatest strength of our method is the fact that it can be used without any manual coding of documents, it can also be easily adapted for use in semisupervised fashion by constraining some elements of Z to be 0 and 1. It is also possible to use the model to classify documents that were not in the original dataset used to fit the model.            "
"23","We present here an analysis of speech in the U.S. Senate, as recorded in the Congressional Record, from 1995 to 2004 (the 105th to the 108th Congresses). In this section, we briefly describe how we process the textual data to serve as input for the topic model and then discuss the specification of the model for this analysis.         "
"24","The textual data are drawn from the United States Congressional Speech Corpus7 (Monroe et al. 2006) developed under the Dynamics of Political Rhetoric and Political Representation Project (http://www.legislativespeech.org). The original source of the data is the html files that comprise the electronic version of the (public domain) United States Congressional Record, served by the Library of Congress on its THOMAS system (Library of Congress n.d.) and generated by the Government Printing Office (United States Government Printing Office n.d.).            "
"25","These html files correspond (nearly) to separately headed sections of the Record. We identify all utterances by an individual within any one of these sections, even if interrupted by other speakers, as a “speech” and it is these speeches that constitute the document set we model. For the eight‐year period under study, there are 118,065 speeches (D) so defined.            "
"26","The speeches are processed to remove (most) punctuation and capitalization and then all words are stemmed.8 There are over 150,000 unique stems in the vocabulary of the Senate over this eight‐year period, most of which are unique or infrequent enough to contain little information. For the analysis we present here, we filter out all stems that appear in less than one‐half of 1% of speeches, leaving a vocabulary of 3,807 (W) stems for this analysis.            "
"27","This produces a  input matrix of stem counts, which serves as the input to the topic model. This matrix contains observations of just under 73 million words.9"
"28","The model contains millions of parameters and latent variables. We can focus on two subsets of these as defining the quantities of substantive interest, the β's and the z's.            "
"29","The β matrix contains K × W (≈ 160, 000) parameters. Each element βkw of this matrix describes the log‐odds of word w being used to speak about topic k. If  it is the case that word w is used more often on topic k than word w′. This is the source of the semantic content, the meaning, in our model. That is, we use this to learn what each topic is about and how topics are related to one another. β describes the intratopic data‐generating process, so it can be used to generate new “speeches” (with words in random order) on any topic. It can also be used, in conjunction with the other model parameters, to classify other documents. This is useful either for sensitivity analysis, as noted below, or for connecting the documents from some other setting (newspaper articles, open‐ended survey responses) to the topical frame defined by this model.            "
"30","                   Z                   is a D × K matrix with typical element zdk. Each of the approximately 5,000,000 zdk values is a 0/1 indicator of whether document d was generated from topic k. The model‐fitting algorithm used in this article returns the expected value of Z which we label . Because of the 0/1 nature of each zdk, we can interpret  (the expected value of zdk) as the probability that document d was generated from topic k.            "
"31","We find that approximately 94% of documents are more than 95% likely to be from a single topic. Thus, we lose very little information by treating the maximum zdk in each row as an indicator of “the topic” into which speech d should be classified, reducing this to D (118,000) parameters of direct interest. Since we know when and by whom each speech was delivered, we can generate from this measures of attention (word count, speech count) to each topic at time scales as small as by day, and for aggregations of the speakers (parties, state delegations, etc.). It is also possible to treat  as a vector of topic probabilities for document d and to then probabilistically assign documents to topics.            "
"32","We fit numerous specifications of the model outlined in the third section to the 105th–108th Senate data. In particular, we allowed the number of topics K to vary from 3 to 60. For each specification of K we fit several models using different starting values. Mixture models, such as that used here, typically exhibit a likelihood surface that is multimodal. Since the ECM algorithm used to fit the model is only guaranteed to converge to a local mode, it is typically a good idea to use several starting values in order to increase one's chances of finding the global optimum.            "
"33","We applied several criteria to the selection of K, which must be large enough to generate interpretable categories that have not been overaggregated and small enough to be usable at all. Our primary criteria were substantive and conceptual. We set a goal of identifying topical categories that correspond roughly to the areas of governmental competence typically used to define distinct government departments/ministries or legislative committees, such as “Education,”“Health,” and “Defense.” This is roughly comparable to the level of abstraction in the 19 major topic codes of the Policy Agendas Project, while being a bit more fine‐grained than the 10 major categories in Rohde's roll‐call dataset (Rohde 2004) and more coarse than the 56 categories in the Comparative Manifestos Project. Conceptually, for us, a genuine topic sustains discussion over time (otherwise it is something else, like a proposal, an issue, or an event) and across parties (otherwise it is something else, like a frame). With K very small, we find amorphous categories along the lines of “Domestic Politics,” rather than “Education”; as K increases, we tend to get divisions into overly fine subcategories (“Elementary Education”), particular features (“Education Spending”), or specific time‐bound debates (“No Child Left Behind”). Results matching our criteria, and similar to each other, occur at K in the neighborhood of 40–45. We present here results for the K = 42 model with the highest maximized log posterior. A series of sensitivity analyses are available in the web appendix.            "
"34","This is a measurement model. The evaluation of any measurement is generally based on its reliability (can it be repeated?) and validity (is it right?). Embedded within the complex notion of validity are interpretation (what does it mean?) and application (does it “work”?)."
"35","Complicating matters, we are here developing multiple measures simultaneously: the assignment of speeches to topics, the topic categories themselves, and derived measures of substantive concepts, like attention. Our model has one immediate reliability advantage relative to human and human‐assisted supervised learning methods. The primary feature of such methods that can be assessed is the human‐human or computer‐human intercoder reliability in the assignment of documents to the given topic frame, and generally 70–90% (depending on index and application) is taken as a standard. Our approach is 100% reliable, completely replicable, in this regard."
"36","More important are notions of validity. There are several concepts of measurement validity that can be considered in any content analysis.10 We focus here on the five basic types of external or criterion‐based concepts of validity. First, the measures of the topics themselves and their relationships can be evaluated for semantic validity (the extent to which each category or document has a coherent meaning and the extent to which the categories are related to one another in a meaningful way). This speaks directly to how the β matrix can be interpreted. Then, the derived measures of attention can be evaluated for convergent construct validity (the extent to which the measure matches existing measures that it should match), discriminant construct validity (the extent to which the measure departs from existing measures where it should depart), predictive validity (the extent to which the measure corresponds correctly to external events), and hypothesis validity (the extent to which the measure can be used effectively to test substantive hypotheses). The last of these speaks directly to the issue of how the z matrix can be applied.         "
"37"," Table 2 provides our substantive labels for each of the 42 clusters, as well as descriptive statistics on relative frequency in the entire dataset. We decided on these labels after examining  and also reading a modest number of randomly chosen documents that were assigned a high probability of being on topic k for k = 1, …, K. This process also informs the semantic validity of each cluster. Krippendorff (2004) considers this the most relevant form of validity for evaluating a content analysis measure. We discuss these procedures in turn.            "
"38","In order to get a sense of what words tended to distinguish documents on a given topic k from documents on other topics we examined both the magnitude of  for each word w as well as the weighted distance of  from the center of the  vectors other than  (denoted ). The former provides a measure of how often word w was used in topic k documents relative to other words in topic k documents. A large positive value of  means that word w appeared quite often in topic k documents. The weighted distance of  from the center of the , which we operationalize as               "
"39","Inspection of these tables produced rough descriptive labels for all of the clusters. After arriving at these rough labels we went on to read a number of randomly chosen speech documents that were assigned to each cluster. In general we found that, with the exception of the procedural categories, the information in the keywords (Table 3, extended) did an excellent job describing the documents assigned to each (substantive) topic. However, by reading the documents we were able to discover some nuances that may not have been apparent in the tables of  values, and those are reflected in the topic labels and clarifying notes of Table 2.            "
"40","In general, the clusters appear to be homogeneous and well defined. Our approach is particularly good at extracting the primary meaning of a speech, without being overwhelmed by secondary mentions of extraneous topics. For example, since 9/11, the term terrorism can appear in speeches on virtually any topic from education to environmental protection, a fact that undermines information retrieval through keyword search.12 It is worth noting that this technique will extract information about the centroid of a cluster's meaning and lexical use. There will be speeches that do not fall comfortably into any category, but which are rare enough not to demand their own cluster.13"
"41","Reading some of the raw documents also revealed some additional meaning behind the clusters. For instance, two of the clusters with superficially uninformative keywords turn out to be composed exclusively of pro forma “hobby horse” statements by Senator Jesse Helms about the current level of national debt and by Senator Gordon Smith about the need for hate crime legislation."
"42","The β parameters identify words that, if present, most distingush a document of this topic from all others, for the time period under study and for the Senate as a whole. Our approach does not demand that all legislators talk about all topics in the same way. To the contrary, there is typically both a common set of terms that identifies a topic at hand (as shown in Table 3) and a set of terms that identifies particular political (perhaps partisan) positions, points of view, frames, and so on, within that topic.            "
"43","For example, Table 3 lists the top 10 keys for Judicial Nominations (nomine, confirm, nomin, circuit, hear, court, judg, judici, case, vacanc), all of which are politically neutral references to the topic that would be used by speakers of both parties. Within these topically defined speeches, we can define keys that are at any given time (here the 108th) the most Democratic (which include republican, white, hous, presid, bush, administr, lifetim, appoint, pack, controversi, divis) or the most Republican (which include filibust, unfair, up‐or‐down, demand, vote, qualifi, experi, distinguish), clearly reflecting the partisan split over Bush appointees and Democratic use of the filibuster to block them.14"
"44","An important feature of the topic model, another sharp contrast with other approaches, is that the β matrix is an estimate of the relationship between each word in the vocabulary and each topical cluster. As a result, we can examine the semantic relationships within and across groups of topics. Given the more than 150,000 parameters in the β matrix, there are many such relationships one might investigate. Here we focus on how the topics relate to each other as subtopics of larger metatopics, how they aggregate. The coherent meaning of the metatopics we find is further evidence of the semantic validity of the topic model as applied to the Congressional Record. This type of validation has not been possible with other approaches to issue coding.            "
"45","One approach to discovering relationships among the 42 topics is agglomerative clustering of the β vectors, , by topic. Agglomerative clustering begins by assigning each of the 42 vectors to its own unique cluster. The two vectors that are closest to each other (by Euclidean distance) are then merged to form a new cluster. This process is repeated until all vectors are merged into a single cluster. The results of this process are displayed in the dendrogram of Figure 1.15 Roughly speaking, the lower the height at which any two topics, or groupings of topics, are connected, the more similar are their word use patterns in Senate debate.16"
"46","                 Agglomerative Clustering of 42‐Topic Model                         "
"47"," Notes: Hierarchical agglomerative clustering of . Clustering based on minimizing the maximum euclidean distance between cluster members. Each cluster is labeled with a topic name, followed by the percentage of documents and words, respectively, in that cluster.                        "
"48","Reading Figure 1 from the bottom up provides information about which clusters were merged first (those merged at the lowest height). We see that topics that share a penultimate node share a substantive or stylistic link. Some of these are obvious topical connections, such as between the two health economics clusters or between energy and environmental regulation. Some are more subtle. For example, the “Environment 1 [Public Lands]” category, which is dominated by issues related to management and conservation of public lands and water, and the “Commercial Infrastructure” category are related through the common reference to distributive public works spending. Both contain the words project and area in their top 25 keys, for example. The “Banking / Finance” category and the “Labor 1 [Workers]” category discuss different aspects of economic regulation and intervention, the former with corporations and consumers, the latter with labor markets. Other connections are stylistic, rather than necessarily substantive. The symbolic categories, for example, all have great, proud, and his as keywords.            "
"49","We can also read Figure 1 from the top down to get a sense of whether there are recognizable rhetorical metaclusters of topics. Reading from the top down, we see clear clusters separating the housekeeping procedural, hobby horse, and symbolic speech from the substantive policy areas. The more substantive branch then divides a cluster of conceptual and Constitutional issues from the more concrete policy areas that require Congress to appropriate funds, enact regulations, and so on. Within the concrete policy areas, we see further clear breakdowns into domestic and international policy. Domestic policy is further divided into clusters we can identify with social policy, public goods and infrastructure, economics, and “regional.” Note that what binds metaclusters is language. The language of the Constitutional grouping is abstract, ideological, and partisan. The social policy grouping is tied together by reference to societal problems, suffering, and need. The public goods / infrastructure grouping is tied together both by the language of projects and budgets, as well as that of state versus state particularism. The most interesting metacluster is the substantively odd “regional” grouping of energy, environment, agriculture, and trade. Exploration of the language used here shows that these are topics that divide rural and/or western senators from the rest—distributive politics at a different level of aggregation.            "
"50","This approach has the potential to inform ongoing debates about how to characterize the underlying political structure of public policy. Whether such characterization efforts are of interest in and of themselves—we would argue they are—is not of as much relevance as the fact that they are necessary for understanding dimensions of political conflict (Clausen 1973; Poole and Rosenthal 1997), the dynamics of the political agenda (Baumgartner and Jones 2002; Lee 2006), the nature of political representation (Jones and Baumgartner 2005), or policy outcomes (Heitschusen and Young 2006; Katznelson and Lapinski 2006; Lowi 1964). Katznelson and Lapinski (2006) provide an eloquent defense of the exercise and a review of alternative approaches.            "
"51","The construct validity of a measure is established via its relationships with other measures. A measure shows evidence of convergent construct validity if it correlates with other measures of the same construct. A measure shows discriminant construct validity when it is uncorrelated with measures of dissimilar constructs (Weber 1990).            "
"52","Construct validity has a double edge to it. If a new measure differs from an established one, it is generally viewed with skepticism. If a new measure captures what the old one did, it is probably unnecessary. In our case, the model produces measures we expect to converge with others in particular ways and to diverge in others. Consider a specific policy‐oriented topic, like abortion. We expect that, typically, a roll call on abortion policy should be surrounded by a debate on the topic of abortion. This convergent relationship should appear in our measure of attention to abortion in speech and in indicators of roll calls on abortion policy."
"53"," Figure 2 displays the number of words given in speeches categorized by our model as “Abortion” over time. We also display the roll‐call votes in which the official description contains the word abortion. We see the basic convergence expected, with number of roll calls and number of words correlated at +0.70. But note also that we expect divergence in the indicators as well. Attention is often given to abortion outside the context of an abortion policy vote, the abortion policy nature of a vote might be unclear from its description, and a particular roll call might receive very little debate attention.            "
"54","                 The Number of Words Spoken on the ‘Abortion’ Topic Per Day                         "
"55","Consider first, the spikes of debate attention that do not have accompanying roll‐call votes. The first such spike is in February of 1998, when no vote was nominally on abortion. The occasion was the Senate confirmation of Clinton's nominee for Surgeon General, David Satcher, and debate centered around Satcher's positions on abortion. “Abortion” appears nowhere in the description of the vote. Hand‐coding exercises would also not code the vote as abortion. For example, Rohde's roll‐call data (Rohde 2004) cover the House, but if extended to the Senate would clearly characterize the accompanying vote on February 10 as a confirmation vote, within a larger procedural category. None of Clausen (1973), Peltzman (1985), or Poole and Rosenthal (1997) extends forward to 1998, but all code previous Surgeon General confirmations at similar high levels of aggregation. For example, the C. Everett Koop confirmation vote, in 1981, is coded under the Clausen system as “Government Management,” under Peltzman as “Government Organization” (primarily) and “Domestic Social Policy” (secondarily), and under Poole and Rosenthal as “Public Health”.17 Satcher would have been coded identically in each case. But it is clear from reading the transcript that the debate was about, and that attention was being paid to, abortion.            "
"56","Another such spike is in March of 2004, when the Unborn Victims of Violence Act establishing penalties for violence against pregnant women was debated. The House vote on this identical bill is coded in the Rohde data under “Crime / Criminal Procedure” (Rohde 2004). Much of the debate attention, however, centered around the implications of the bill and possible amendments for abortion rights. In both cases, the spike in attention to abortion is real—captured by the speech measure and uncaptured by roll‐call measures.            "
"57","Similarly, the speech measure captures subtleties that the roll‐call count does not. For example, on or around July 1 in every year from 1997 to 2003, Senator Murray offered an amendment to the Department of Defense Appropriations bill, attempting to restore access to abortions for overseas military personnel. The roll‐call measure captures these through 2000, but misses them later. This is because the word abortion was removed from the description, replaced by a more opaque phrase: “to restore a previous policy regarding restrictions on use of Department of Defense medical facilities.” But with speech, these minor spikes in attention can be seen. Moreover, the speech measure captures when the amendment receives only cursory attention (a few hundred words in 1998) and when it is central to the discussion (2000, 2002).            "
"58","Note also the relationship between speech and hearing data. The hearings data are sparse and generally examined at an annual level. At this level of aggregation, the two measures converge as expected—both show more attention to abortion by the Senate during the Clinton presidency (1997–2000) than during the Bush presidency (2001–4). But at a daily level, the measures are clearly capturing different conceptual aspects of political attention. Higher cost hearings are more likely to capture attention that is well along toward being formulated as policy‐relevant legislation. Speech is lower cost, so more dynamic and responsive at the daily level, more reflective of minority interests that may not work into policy, and potentially more ephemeral."
"59"," Predictive validity refers to an expected correspondence between a measure and exogenous events uninvolved in the measurement process. The term is perhaps a confusing misnomer, as the direction of the relationship is not relevant. This means that the correspondence need not be a pure forecast of events from measures, but can be concurrent or postdictive, and causality can run from events to measures (Weber 1990). Of the limitless possibilities, it suffices to examine two of the most impactful political events in this time period: 9/11 and the Iraq War.            "
"60"," Figure 3a plots the number of words on the topic that corresponds to symbolic speech in support of the military and other public servants. Here we see a large increase in such symbolic speech immediately after 9/11 (the largest spike on the plot is exactly on September 12). There is another large spike on the first anniversary of 9/11 and then a number of consecutive days in March 2003 that feature moderate‐to‐large amounts of this type of symbolic speech. This corresponds to the beginning of the Iraq War.            "
"61","                 The Attention to ‘Symbolic [Remembrance—Military]’ and ‘Defense [Use of Force]’ Topics over Time                         "
"62","The number of words on the topic dealing with the use of military force is displayed in Figure 3b. The small intermittent upswings in 1998 track with discussions of Iraqi disarmament in the Senate. The bombing of Kosovo is represented as large spikes in spring 1999. Discussion within this topic increased again in May 2000 surrounding a vote to withdraw U.S. troops from the Kosovo peacekeeping operation. Post 9/11, the Afghanistan invasion brings a small wave of military discussion, while the largest spike in the graph (in October 2002) occurred during the debate to authorize military action in Iraq. This was followed, as one would expect, by other rounds of discussion in fall 2003 concerning the emergency supplemental appropriations bill for Iraq and Afghanistan, and in the spring of 2004 surrounding events related to the increasing violence in Iraq, the Abu Ghraib scandal, and the John Negroponte confirmation.            "
"63","Hypothesis validity—the usefulness of a measure for the evaluation of theoretical and substantive hypotheses of interest—is ultimately the most important sort of validity. In this section we offer one example of the sort of analysis to which attention measures can be applied directly. We return to a discussion of further applications in the concluding discussion."
"64","One direct use of these data is to investigate floor participation itself to answer questions about Congressional institutions, the electoral connection, and policy representation. Prior empirical work has had severe data limitations, depending on low frequency events (e.g., floor amendments; Sinclair 1989; Smith 1989), very small samples (e.g., six bills; Hall 1996), or moderately sized, but expensive, samples (e.g., 2,204 speeches manually coded to three categories; Hill and Hurley 2002). Our data increase this leverage dramatically and cheaply.            "
"65"," Figure 4 summarizes the results from 50‐count models (negative binomial) of the speech counts on all nonprocedural topics and selected metatopical aggregations, for the 106th Senate, for all 98 senators who served the full session. Selected hypotheses, discussed below, are represented by shaded backgrounds.18"
"66","                 Speech Count Models, 106th Senate                         "
"67"," Notes: Each column represents a negative binomial model of speeches delivered on a given topic, or group of topics, in the 106th Senate, with one observation per senator who served the entire two years (98 in total). Each row of the table represents a covariate: “Committee” (binary indicating whether the senator is on a topic‐relevant committee); “Chair/Ranking” (binary indicating the senator is the chair or ranking member of a topic‐relevant committee); “Freshman” (binary); “Republican” (binary); “Extremity” (absolute value of Dimension 1 Poole‐Rosenthal DW‐NOMINATE scores); “Agriculture” (log of state agricultural income per capita, 1997); “Election” (dummy, up for election in next cycle); “Retiring” (dummy, retired before next election); “Unemployment” (state unemployment rate, 1999). Plotted are standardized betas and 95% confidence intervals, darker where this interval excludes zero. Shaded areas represent hypotheses discussed in the text.                        "
"68","Congressional behavior is of core relevance to questions about the existence and possible decline of “norms” of committee deference, specialization, and apprenticeship (Hall 1996; Matthews 1960; Rohde, Ornstein, and Peabody 1985; Shepsle and Weingast 1987; Sinclair 1989; Smith 1989). As noted by Hall, this is a difficult empirical question as the primary leverage has come from floor amendment behavior, a relatively rare occurrence (1996, 180–81). Figure 4 shows that committee membership, but not necessarily service as chair or ranking member, continues to have a substantial impact on the tendency to participate in debate across policy topics. The apprenticeship norm, as indicated by a negative impact of freshman status, also seems to be present in more technical policy areas, but notably not in common electoral issues like abortion or the size of government. Examination of the data over time could further inform the question of decline (Rohde, Ornstein, and Peabody 1985; Sinclair 1989; Smith 1989) and, with the cross‐topic variation provided here, the role of expertise costs (Hall 1996) versus norms (Matthews 1960) in both deference and apprenticeship.            "
"69","Since at least Mayhew, Congressional scholars have also been interested in how career considerations affect the electoral connection (Fenno 1978, 1996; Hill and Hurley 2002; Maltzman and Sigelman 1996; Mayhew 1974). The sixth and seventh rows of Figure 4 identify two career cycle effects in the electoral connection and symbolic/empathy speech. A senator approaching election is more likely to give speeches in the symbolic (“I am proud to be one of you”) and social (“I care about you”) categories than is one whose next election is further in the future. Conversely, senators who subsequently retired gave many fewer such speeches, adding further evidence to the literature on participatory shirking (Poole and Rosenthal 1997; Rothenberg and Sanders 2000).            "
"70","The last two rows of Figure 4 provide evidence of two (arbitrary) examples of policy representation, unemployment and agriculture. This reflects the notion of representation as congruence between constituency and representation, a subject of considerable scholarly attention (Ansolabehere, Snyder, and Stewart 2001 is a prominent example, in a literature that traces at least to Miller and Stokes 1963). Previous studies of congruence have generally been limited, on the legislator side, to measures of position based on elite surveys or roll calls. Jones and Baumgartner (2005) examine the year‐by‐year congruence of relative attention to topic (via hearings) with aggregate (not constituency‐level) demand measured by Gallup “most important problem” data.            "
"71","The party and ideology results (rows four and five) also contain interesting insights for our broader interests in how speech can inform our understanding of the lanscape of political competition. Democrats are more likely to speak on social issues and more likely to speak in general. Given that Democrats were in the minority in the 106th Senate, this does lend some support to the assertion that speech is better than more constrained legislative behaviors at revealing thwarted minority party preferences and strategies."
"72","Extremity (absolute DW‐NOMINATE score) is associated with more speeches on constitutional, international, and economics topics, but not generally on social issues or geographically driven topics. This could be taken as evidence that the former set of topics is of greater interest to ideological extremists. Or—our view—it could be taken as evidence that these are the topics that define the current content of the primary dimension captured by roll‐call‐based ideal point estimation procedures. The lack of association between “extremism” and attention to other topics is suggestive that those other topics define higher dimensions of the political space.            "
"73","In this article we have presented a method for inferring the relative amount of legislative attention paid to various topics at a daily level of aggregation. Unlike other commonly used methods, our method has minimal startup costs, allows the user to infer category labels (as well as the mapping from text features to categories), and can be applied to very large corpora in reasonable time. While other methods have one or more of these features, no other general method possesses all of these desirable properties."
"74","While our method has several advantages over other common approaches to content analysis, it is not without its own unique costs. In particular, the topic model discussed in this article requires more user input after the initial quantitative analysis is completed. Since no substantive information is built directly into the model, the user must spend more time interpreting and validating the results ex post.         "
"75","This article presents several ways that such interpretation and validation can be performed. Specifically, we demonstrate how (a) keywords can be constructed and their substantive content assessed, (b) agglomerative clustering can be used to investigate the semantic relationships across topics, (c) construct validity of our daily measures of topic attention can be evaluated by looking at their covariation with roll calls and hearings on the topic of interest, and (d) predictive validity of our measures can be assessed by examining their relationships with exogenous events (such as 9/11 or the Iraq War) that are widely perceived to have shifted the focus of attention in particular ways. In each case, we find strong support for the validity of our measures.         "
"76","While our method is useful, it will not (and should not) replace other methods. Instead, our data and method supplement and extend prior understandings of the political agenda in ways that have been to date prohibitively expensive or near impossible. Our method is particularly attractive when used as an exploratory tool applied to very large corpora. Here it quickly allows new insights to emerge about topic attention measured at very fine temporal intervals (in our example days). In some applications this will be enough; in others more detailed (and expensive) confirmatory analysis will be in order."
"77","There are many potential applications beyond those we have given here for such measures of attention as this. The dynamic richness of our data allows topic‐specific examination of policy‐agenda dynamics, and questions of incrementalism or punctuated equilibrium (Baumgartner and Jones 1993). The dynamic richness also allows us to move beyond static notions of congruence into dynamic notions of responsiveness, illuminating the topics and conditions under which legislators lead or follow public opinion (Jacobs and Shapiro 2000; Stimson, MacKuen, and Erikson 1995).         "
"78","Moving another step, there are many possible indirect applications of the topic model. Once speeches are separated by topic, we can examine the substantive content—the values and frames—that underlie partisan and ideological competition. We can, for example, track in detail the dynamics by which issues and frames are adopted by parties, absorbed into existing ideologies, or disrupt the nature of party competition (Carmines and Stimson 1989; Monroe, Colaresi, and Quinn 2008; Poole and Rosenthal 1997; Riker 1986).         "
"79","Further, once we know the content of party competition, we can evaluate the positioning of individual legislators. That is, as hinted above, the topic model is a valuable first step toward using speech to estimate ideal points from legislative speech. This allows dynamically rich, topic‐by‐topic ideal point estimation, and insights into the content and dimensionality of the underlying political landscape (Lowe 2007; Monroe and Maeda 2004; Monroe et al. 2007).         "
"80","Perhaps most exciting, our method travels beyond English and beyond the Congressional setting, where conventional methods and measures can be prohibitively expensive or difficult to apply. We hope this might provide an important new window into the nature of democratic politics."
