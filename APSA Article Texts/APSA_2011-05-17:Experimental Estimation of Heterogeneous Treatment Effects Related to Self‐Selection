"","x"
"1","Does campaign negativity affect turnout? The heated and still unresolved debate on this question has extended far beyond the boundaries of academic political science. Advocates of campaign finance reform cited research showing that negative campaign advertising reduced turnout; later, scholarly criticisms of that work entered the realm of public policy debates and even court proceedings, in McConnell v. FEC."
"2","Political scientists have reported three mutually exclusive and exhaustive findings about the effects of negative television advertisements (“ads” hereafter) on voting turnout: they depress it, enhance it, and have no effect at all. And they have used three distinct approaches to generate these disparate results: conducting experiments in which exposure to negative ads is the key treatment; analyzing aggregate data, including variables characterizing the degree of negativity of various contests; and analyzing survey data, including self‐reports of ad exposure.3"
"3","The thesis that negative ads reduce turnout is rooted in a prominent experimental study (Ansolabehere et al. 1994; also Ansolabehere and Iyengar 1995). The researchers randomly assigned a convenience sample of potential Los Angeles area voters to one of three treatments. Subjects watched a newscast, embedded in which were a negative ad, a positive ad, or, as the control, a (positive) nonpolitical product ad. The ads came from the actual campaigns of candidates running for governor and the U.S. Senate in California in 1992. In brief, the authors found effects of roughly 5 percentage points; those who saw a positive ad were about 2.5 points more likely to say they would vote than those in the control condition, while those who saw a negative ad were around 2.5 points less likely to say they would vote (1994, 833). Ansolabehere, Iyengar, and Simon (1999) then analyzed aggregate data on 1992 U.S. Senate elections to buttress their experimentally based inferences.         "
"4","Political scientists have offered a variety of explanations for the wide‐ranging results and conclusions across studies.4 Critics of the experimental results point to unrealistically strong laboratory treatments; critics of survey‐based research focus predominantly on measurement error, especially the unreliability of respondents’ self‐reported recalls of exposure; and critics of studies using aggregate data emphasize poor measures of advertising tone and volume, and, especially, possible spurious correlations.         "
"5","Strikingly, for all of the work done on the subject, we know of no study that assesses the logic of using a random‐assignment experiment to make valid inferences about the effects of campaign ads outside the experimental context. Current thinking applauds the use of such experiments, putting aside the possibility of overly strong treatments, for the very reason that randomization equalizes the effects of possible confounds across conditions and, more directly relevant here, completely eliminates selection effects. The random‐assignment experiment, in short, produces clean, readily interpretable results about cause and effect."
"6","Why, then, even consider introducing a self‐selection condition into the classical random‐assignment experiment? Consider how random assignment eliminates self‐selection. Some people in the population who would expose themselves to the treatment (a negative ad) do not get it, while others who would never expose themselves do. This deliberate, coercive elimination of self‐selection can become a problem for the experimenter aiming to infer how the treatment affects the response variable among people outside the experimental context. The problem arises when some people select into and others out of the treatment, and the treatment's effects, whether realized or not, differ between those who select in and those who select out. Then, the estimated treatment effect, the overall average effect of a negative ad on voting turnout, exists only within the random‐assignment experiment itself.5"
"7","We can restate the problem in terms of a discrepancy between questions researchers intend to ask and those they actually answer. Garfinkel (1981) argues convincingly that social scientists all too often formulate research questions that lack clarity and precision. This failure obscures the meaning of any single study's results and hampers substantive communication across studies. Research on the topic of negative campaign ads and voting turnout includes observational and experimental studies. All contributors seek to make inferences about the causal effects of ads in real campaigns. A single, sometimes explicitly stated, question—How do negative campaign ads affect voting turnout?—would seem to guide them.         "
"8","On reflection, that question is too vague. The typical observational study implicitly addresses this question: what are the effects of negative campaign ads given that some people are exposed to them and others are not? The random‐assignment experiment addresses a different question: what would be the effects of negative campaign ads if everyone watched them? Each question is important, though the two are distinct. Thus, observational and experimental studies should not generate the same results unless: outside the experiment, self‐selection does not occur or ads identically affect individuals who are and are not exposed; or, inside the experiment, selection to watch or not watch ads is incorporated in the design.6"
"9","We continue to use campaign negativity as a running example below, although many other topics and phenomena studied with experiments are also prone to self‐selection. The investigation of source effects, for instance, is a staple of political psychology and political communication. Researchers routinely ask whether and how hearing different framings of issues affects people's beliefs and attitudes, including their willingness to tolerate people who differ from them (Chong and Druckman 2007; Kinder and Sanders 1990; Nelson, Clawson, and Oxley 1997; Sniderman and Theriault 2004); or whether watching different newscasts—CNN versus FOX News, for example—causes people to react differently to unfolding events; or how incivility among politicians affects viewers’ attitudes about politics (Mutz 2007). Political scientists are increasingly using experiments to study the effects of various deliberation structures on participants’ acceptance of the collective outcomes, and whether participating in a deliberative process that requires people openly to justify their beliefs and opinions changes those beliefs and opinions (see, for example, Sulkin and Simon 2002).         "
"10","In these instances and others, researchers seek to make causal inferences that apply beyond the specific experimental context; thus, all of the cautions we raised with respect to the study of negative campaign ads and voting turnout apply here as well. Generally, the average effect detected in the classic random‐assignment experiment mixes together a real‐world phenomenon (the treatment's effect on those who are, away from the experimental simulation, treated) and a hypothetical one that exists only within the experiment itself (its effect on those who are not generally treated away from the experimental simulation). In turn, the estimated experimental effect will be of limited value to the researcher."
"11","For decades, most data analysis in political science and elsewhere was motivated by a rather casual notion of causality. Observational data were the modal variety, and data analysts usually took for granted that strong associations therein could be interpreted as causal findings, notwithstanding the ubiquity of the slogan “correlation is not causation.”"
"12","Eventually, however, skepticism about the value of observational data for making causal claims began to make its way from statistics to the natural and social sciences. As Holland (1986) emphasized, conventional statistical analysis of observational data can answer only the question, “Which of a set of observable, potential causes are associated with an effect?” To answer the distinct question, “What are the effects of a cause (treatment)?” requires random assignment of the treatment.         "
"13","The logic of the random‐assignment experiment, meanwhile, is not quite so straightforward as to require no exposition at all. A treatment's effect, in the case of a dichotomous treatment, is the simple difference, (Yi | t= 1) – (Yi | t= 0), where the variable Y describes some phenomenon of interest. In most contexts, this difference is purely hypothetical, since the researcher cannot observe multiple outcomes (realizations of the variable of interest) for any given unit. Hence, experimentalists traditionally estimate the unobservable treatment effect by comparing averages of groups. That is, they use  to estimate the quantity E((Yi | t= 1) – (Yi | t= 0)), usually labeling the former “the treatment effect,” although “average treatment effect” more aptly describes the difference‐of‐means estimate, since the averages of the treatment and control groups mask within‐group heterogeneity.7"
"14","We have proposed that one particular form of heterogeneity, between those likely to be treated outside of experiments and those not, holds special importance. We are not the first to broach this point. Gerber and Green write: “The concept of the average treatment effect implicitly acknowledges the fact that the treatment effect may vary across individuals in systematic ways. One of the most important patterns of variation in τi[their parameter for treatment effect] occurs when the treatment effect is especially large (or small) among those who seek out a given treatment. In such cases, the average treatment effect in the population may be quite different from the average treatment effect among those who actually receive the treatment” (2008, 362). The very possibility of such heterogeneity signals the importance of incorporating self‐selection into the experimental design.         "
"15","To see this point more clearly, and more generally, assume the following: Y is a dichotomous variable that measures a behavior of interest; α is the proportion of the population that self‐selects into a treatment when given the choice; in the absence of treatment, self‐selectors have probability ys and nonself‐selectors have probability yn of exhibiting the behavior of interest; the treatment effects, which alter the probabilities and are not assumed to be equal, are ts for self‐selectors and tn for nonself‐selectors. Finally, assume for now that the process of selecting treatment can be perfectly simulated within the experiment.8"
"16","The classic random‐assignment experiment, depicted on the bottom branch of Figure 1, generates two means (proportions) whose difference is the average treatment effect, α (ts) + (1 −α)(tn). Note that α is unknown, and thus separating ts and tn is not possible. Randomization balances, in expectation, heterogeneity in baseline probabilities across the treatment and control groups, so that the ys and yn terms cancel in subtraction. The estimated treatment effect is a weighted average, but the weights and the distinct treatment effects remain unknown to the experimenter.         "
"17","                 The Self‐Selection Experiment and Heterogeneity                      "
"18","A simple variation on the traditional experimental design allows estimation of all three treatment effects: the average effect and the effects among selectors and non‐selectors. Figure 1 shows that it consists of randomly assigning subjects to one of the three conditions just noted, the usual treatment and control conditions, plus a self‐selection condition in which subjects themselves choose whether or not to receive the treatment.9 Random assignment ensures that the mixtures of self‐selectors and nonself‐selectors will be identical, in expectation, in all three randomly formed groups.         "
"19","The figure shows, for each of the three conditions, the expected proportions exhibiting the behavior of interest (the expected value of the mean of outcome variable Y). Selection within the self‐selection group is not random, and simple algebra reveals the analytic usefulness of this fact. The expected value for the whole, randomly constituted group assigned to the selection condition is a weighted average of two probabilities, the probability given treatment for the selectors and the probability given no treatment for the nonselectors. Allowing some subjects to choose their treatments directly generates an estimate of α, the proportion of the population that self‐selects into treatment. The researcher can then use that estimate to recover estimates of the two potentially distinct treatment effects, unpacking any heterogeneity between those inclined and those disinclined to be treated:            "
"20","Obtaining standard errors for  and  is not trivial. Because  is itself an estimate, these estimators are nonlinear functions of estimates, and ratio bias is thus a concern. The two estimators include some common terms and will normally be correlated. One common approach to dealing with nonlinear combinations of parameters of this type is the delta method for approximating the distributions and variances (Cameron and Trivedi 2005, 231–32).10 Bootstrapping is another possibility.         "
"21","An astute observer might have observed that the formulae presented immediately above resemble a popular calculation to correct for the nontreatment of subjects assigned to treatment in field experiments, one form of the problem commonly known as noncompliance (Angrist, Imbens, and Rubin 1996; Gerber and Green 1999). The similarity is not accidental, and comparing the purposes and similarities of that calculation with ours reveals some unique logical challenges associated with incorporating self‐selection into field experiments.         "
"22","When field experimenters observe unintended nontreatment, they commonly divide the difference between the dependent variable for the treatment and control groups by the proportion of the group assigned to be treated that was, in fact, successfully treated. The logic is that subjects can be regarded as falling into two types, the easy‐to‐reach and the hard‐to‐reach. The types might differ on the dependent variable when untreated and also in the effect of the treatment on their behavior. If the contacting process is assumed to be a perfect classifier of type, then the point of the adjustment can be seen in the expected value of this estimator (mimicking the notation above, but with “e” designating easy‐to‐reach and “h” hard‐to‐reach, and γ designating the proportion of the population that is type “e”):             "
"23","Two key assumptions underlie this calculation: first, randomization guarantees that, in expectation, the groups assigned to treatment and control are identical in composition; and, second, noncompliance is limited to observable nontreatment of those assigned to treatment, so that gamma is estimated without bias."
"24","This calculation differs from our proposed hybrid design in that it does not recover any estimate of either th or of the average of the two treatment effects for the two types. Although authors often describe the resulting quantity as an estimate of “the” treatment effect, the entire process of adjustment is premised on there being two potentially different treatment effects, only one of which is estimated. In the event that easy and hard types can be regarded as identical to selectors and nonselectors, a field experimenter who follows standard practice produces an estimate of what we have termed the treatment effect for selectors only. Assuming that nonselectors and the hard‐to‐reach are not identical complicates an already difficult problem.         "
"25","All this said, we refrain from stating absolutely that self‐selection can never be incorporated into field studies. Scholars are creative, and they have made notable progress in overcoming or compensating for noncompliance. It would be imprudent to assume that today's limitations will apply tomorrow."
"26","In summary, from a purely logical perspective, incorporating self‐selection into the random‐assignment experiment adds considerable analytical leverage. Random assignment of the treatment isolates the treatment effect, but at the cost of weakening the capacity to make inferences about an external world in which individuals might vary in their exposures to a treatment or in their responses to it in the event of exposure. Including self‐selection garners empirical validity, but it also conflates treatment and self‐selection effects, thus introducing the very problem that besets observational studies. Combining random assignment and self‐selection should generate data that, when correctly analyzed, reveal more about the phenomenon under study than the classic random‐assignment experiment."
"27","In support of this assertion, suppose that some unobserved factor affects whether or not people see negative ads in their day‐to‐day lives.11 To be concrete and realistic, yet simple, consider the two scenarios in Table 1. Both assume two types of people, conflict‐loving and conflict‐averse, who differ in how they react to watching negative campaign ads. In scenario 1, all of the conflict lovers watch ads and none of the conflict‐averse do. The second scenario relaxes the pure‐type assumption and assumes, instead, 0.6 and 0.4 probabilities of watching the ads among the conflict‐loving and the conflict‐averse, respectively, since treatment‐effect heterogeneity (variation in t parameters) need not coincide precisely with treatment selection heterogeneity (variation in probability of exposure to treatment). For each scenario, we review the estimated treatment effect from (1) a naive analysis of observational data (assuming no measurement error), (2) a random‐assignment experiment, and (3) an experiment of the sort we advocate, combining random assignment and self‐selection.         "
"28","In scenario 1, all conflict lovers, and only conflict lovers, watch the ads. A naive observational study would compare the turnout rates of the watchers (about 63%) and nonwatchers (about 50%) and thus estimate, on average, a +13% point effect. This estimate conflates the true treatment effect among the watchers (+5%) and the difference in baseline voting rates (+8%) between the watchers and nonwatchers."
"29","If V is the proportion reporting, truthfully, that they intend to vote, then a random‐assignment experiment would generate an expected‐treatment‐effect estimate of E(V | T)‐E(V | C): (0.40) (0.58 + 0.05) + (0.60) (0.50–0.10) – (0.40) (0.58) + (0.60) (0.50) = 0.52–0.48 =−0.04. This is a correct estimate of the average treatment effect were everyone to watch the ads. Randomization corrects for the different baseline rates, and the estimated treatment effect is a weighted average of the two possible treatment effects, where the weights are population shares. One should not conclude, however, that negative ads reduce turnout in the external world. The random‐assignment experiment demonstrates, rather, that universal exposure to the ads, if it existed, would reduce turnout."
"30","A self‐selection experiment generates this same overall average estimate, but also two additional estimates, as described above. Assume, for now, that implementing selection is unproblematic (we return to this important practical question below). For watchers, the expected value for the estimated treatment effect is E(V | S)‐E(V | C)/E(α) = (0.552–0.532)/0.4 = 0.05; for nonwatchers, it is E(V | T)‐E(V | S)/(1‐E(α)) = (0.492–0.552)/(0.60) =−0.10. Both treatment‐effect parameters are estimated without bias. Allowing some respondents to select treatment thus generates information on a real‐world counterfactual, an estimate that the ads would reduce the probability of voting by about 10 percentage points among those who do not watch them."
"31","There are, in these extra estimates, multiple answers to the question, “What is the effect of negative ads in the real world?” The estimated treatment effect for watchers provides one answer: “On average, viewing negative ads increases the probability of voting by about 5 points among those who see them.” Because the experiment also provides an estimate of the proportion of people who watch ads, it can provide a slightly fuller answer to this question, continuing with “And since about 40% of the population chooses to watch ads, the net effect is a 2 percentage point rise in turnout.”12 Finally, as with the random‐assignment experiment, the data provide an estimate of what watching negative ads would do in a world with universal treatment. Sometimes this counterfactual will be of interest, sometimes not. By combining randomization and self‐selection, therefore, the self‐selection experiment avoids the mistake of the simple observational study, of the self‐selection module in isolation, and of attributing all difference between watchers and nonwatchers to exposure; and by attending to an important variety of heterogeneity in the treatment effect, it escapes the limitations of a classic random‐assignment design.         "
"32"," Figure 2 shows values from 500 simulations of the estimates for this scenario, each with N = 1,000. It provides a sense for how much uncertainty arises from sampling for the ratio estimators, and how the two estimators can be correlated. In this case, the mode is slightly off the true values, but more striking is the dispersion, suggesting that sample size is more important than in a more conventional estimation problem.         "
"33","                 Simulated Treatment‐Effect Estimates                      "
"34","The second scenario in Table 1 slightly complicates the data‐generating parameters. Types are no longer pure, so both watchers and nonwatchers are, in any given instance, mixtures of the two fundamental types. Hence, treatment‐effect heterogeneity now exists within the observable, behavioral types, those who watch ads and those who do not.         "
"35","The naive estimate would now be about a 1% demobilization effect. This figure conflates two differences between watchers and nonwatchers, their tendencies to watch (or not watch) the ads, and their reactions to the ads. The random‐assignment portion of the experiment ignores real‐world exposure, the only aspect of the first scenario that was altered, so it produces precisely the same estimate as above, still best understood as the potential effect of negative ads, given universal exposure. The self‐selection experiment generates estimates of ts and tn that shrink to −0.025 and −0.054, respectively. These values are no longer exact matches of the treatment‐effect parameters specified by the data‐generating process, but they are accurate estimates of the actual effects of negative ads on watchers and nonwatchers in this world where both groups mix conflict lovers, mobilized by negativity, and conflict avoiders, demobilized by it.         "
"36","Treatment heterogeneity could easily take more complicated forms. A third type, conflict‐neutral individuals, might not alter voting intentions when confronted by negative messages. More generally, there need not be a discrete number of homogenous types. Individuals could be characterized by unique combinations of baseline probabilities, effects of exposure to negative ads, and probabilities of exposure to these ads, with a multivariate distribution over these three parameters taking some unknown form. The promise of incorporating self‐selection is not to recover the full set of parameters characterizing any conceivable data‐generating process. Rather, it augments the random‐assignment experiment by grappling with one important variety of heterogeneity.13"
"37","In the end, it is reasonable to assume that the experimental simulation of self‐selection will be imperfect. Then the question becomes, how much error does it take before the estimated treatment effects become problematic? Returning to scenario 1, wherein we assumed two types of people, conflict‐loving (selectors), who always watch campaign ads, and conflict‐averse (nonselectors), who never do, Figure 3 shows how errors in experimental classification affect the estimated treatment effects of seeing a negative ad among the two types. The two horizontal axes show probabilities of selection of treatment in the experiment (se) given selector (s) or nonselector (n). The probability for selectors varies from 1 down to 0.5 and, for nonselectors, from 0 to 0.5. For the case of selectors, the true data‐generating parameter is, again, 0.05, reflecting a mobilization effect. It is designated by a large marker at the point where experimental selection precisely mirrors real life, i.e., p(se | s) = 1 and p(se | n) = 0. As the selectors become less likely and/or the nonselectors more likely to select the experimental treatment, the experimental simulation of selection is less and less accurate, misclassifying more and more subjects. As subjects’ experimental behaviors increasingly misrepresent their true behavior, the estimates of proportions of selectors and nonselectors (∝ and 1‐∝) go awry. Because these estimated proportions are the key to decomposing the average treatment effect into the two distinct effects, the estimated treatment effects for selectors and nonselectors increasingly become biased.         "
"38","                 Misclassification and Estimated Ad Effects                      "
"39","The figure shows that the bias can be positive or negative, and, unsurprisingly, will be severe when the experimental selections very poorly represent external behavior. More crucially, although there is no obvious baseline for “good enough” when interpreting these estimates, it appears that the modest amount of error included in the selection simulation is not fatal."
"40","So far, we have assumed accurate simulation of selection as it occurs outside the experimental context. That an experiment can precisely mimic external‐world behavior is a strong assumption, and how to simulate external selection behavior in an experiment often will not be self‐evident. Although perfect simulation is not necessary to retain internal validity while increasing external validity, how best to simulate selection behavior is not at all self‐evident."
"41","For the most part, our proposed design avoids one set of potential problems. The self‐selection condition does not ask subjects to predict future behavior, recall past behavior, or describe hypothetical, counterfactual behavior. Nor does it ask them to justify or explain their choices. Our proposed design avoids errors that might arise from these reflective self‐evaluations and simply invites subjects to make a choice that should resemble its nonexperimental equivalent."
"42","One question that any user of a self‐selection experiment will want to consider is how blind the subject should be to selection into the treatment. As a general matter, economists and psychologists could not disagree more, with the former answering “not blind at all” and the latter answering “completely blind.” Staying with the ads example, to simulate self‐selection, an experimenter might simply give subjects the option of watching a negative campaign ad, a positive campaign ad, or an ad that has no political content. Giving subjects that choice would, however, break with past precedent of disguising the experiment's focus. Insofar as one worries that subjects react to ads differently if they know that the experiment is focused on ads, an option is to alter the self‐selection item.14 Because candidates frequently air their campaign advertisements during the evening news hour, researchers might invite subjects to choose whether or not to watch a newscast. That implementation might mimic accidental self‐selection, but it would not account for those who systematically miss real‐world advertisements because they leave the room, change the channel, or shut off their brains when the ads air. To that end, the study might also give subjects the ability to fast‐forward through video material, change channels, turn the TV off, or walk about. Generally, there will be trade‐offs between alternative forms of simulating external selection, and we cannot embrace any one general approach as optimal for all conceivable studies. Multiple studies and diverse practices would be welcome.         "
"43","A distinct reason to embrace multiple studies stems from the chance of pretreatment, outside of the experimenter's control. Above, we were a little cavalier in treating the viewing of negative ads as a simple dichotomy. Gaines, Kuklinski, and Quirk (2007) have argued that experimenters should be sensitive to a priori real‐world treatments. In this case, insofar as people vary in their exposures to nonexperimental negative advertising, the treatment effects uncovered by an experiment are averages of marginal additional‐viewing effects. Two experiments including selection undertaken in environments that differ in the prevalence of negative ads should generate different estimates of treatment effects. If, for instance, there are diminishing effects for seeing negative ads, a researcher would want to expose virgin viewers to a mudslinging ad to estimate the ad's initial impact. Screening out the already treated might be done at the individual level, but since there are good reasons to worry about recall of exposure, it might be preferable to undertake parallel studies in distinct districts, chosen for their (high) variance in level of negativity.         "
"44","Finally, might the selection decision itself induce some effect that shapes responses to treatment? In other words, do selectors and nonselectors differ systematically? We assume they normally do differ; indeed, this is precisely the reason for including selection in the classic experiment. And while the immediate goal might not be to identify those factors that distinguish selectors from nonselectors, our proposed experimental design affords an opportunity to do precisely that. In the long run, then, incorporating self‐selection in the classic design could potentially represent a first step to an improved understanding of selection processes and their effects in a variety of social situations."
"45","The discussion thus far has been theoretical and hypothetical. To put theory into practice, we undertook a simple self‐selection pilot experiment within a survey of adult residents of Illinois, as part of the Cooperative Campaign Analysis Project.15 We considered the effects of negative campaign material, but on candidate evaluations rather than on turnout.         "
"46","Specifically, we asked respondents to rate John McCain and Barack Obama on feeling thermometers, having first randomly assigned them to one of three conditions. Some respondents viewed negative campaign flyers about the two candidates (the treatment condition), some did not (the control condition), and some were told (the self‐selection condition):"
"47","                        "
"48","We can show you a couple of very brief campaign e‐flyers, one from Barack Obama and one from John McCain. You can see flyers in which each candidate is criticizing the other candidate…or not look at any flyers. Would you like to see the flyers?16"
"49","Our self‐selection instrument was, plainly, direct and simple. As we noted in the preceding section, other options, including some more complicated approaches that are conceivably preferable to this one, warrant serious consideration in future research."
"50","In addition to using the design to generate separate treatment effects for selectors and nonselectors (those who choose to view flyers and those who decline), we also separated respondents into partisan groups, whom we expected to vary dramatically in their affective reactions to partisan candidates. We combined the “leaners” and weak and strong partisans, and omitted the small number of pure independents, to simplify presentation. Table 2 shows the results.         "
"51","The average effects of the negative flyers are reported at the bottoms of the columns, along with 95% confidence intervals. There are no great surprises, though there are some slight asymmetries. Our negative flyers appear to have reduced Republicans’ ratings of Obama by about five‐and‐a‐half points and Democrats’ ratings of McCain by a little less, about three‐and‐a‐half points. Republicans who were given the negative materials also reported comparatively lower McCain assessments, while Democrats given the negative materials assessed their candidate slightly more highly, as against the control group."
"52","More interesting, for present purposes, are the two numbers below the estimated average effects, our estimates of distinct treatment effects for those who prefer not to view ads (nonselectors) and for those who prefer to see them (selectors). In all four cases, the effects of the flyers seem to be stronger for those who opt to view them, given the choice. Those partisans, Democrats and Republicans alike, who choose to see negative ads assign the opposition candidate a score about 9–10 points lower than those who see no ads. By contrast, for those who prefer not to see ads, the estimated effects of the ads, when they are imposed, are minor. The asymmetry noted above, between Democrats who rally around Obama given attack and Republicans who downgrade McCain given attack, is more pronounced among those who select flyers: McCain's rating by Republican selectors falls five points (versus two for all Republicans assigned to the treatment condition) and Obama's score by Democratic selectors rises nearly nine points (versus four across all Democrats assigned to the treatment condition). Finally, Democrats are slightly more likely to opt to see the negative material than Republicans, with selection percentages of about 40 versus 35, respectively."
"53"," Table 2 also shows that the 95% confidence intervals for the average effects all span 0, in no small part because we had quite small samples in the various conditions. The confidence intervals on the selectors’ and nonselectors’ treatment effects, inflated to account for the sampling uncertainty in our estimate of alpha, are not shown, but are wider still. For instance, a bootstrap approximation for the confidence interval on the point estimate of 8.8 for Democrats who select to view negative materials about Obama is (−9.0, 28.4). An interval for nonselectors is (−15.7, 10.5).         "
"54","These very wide intervals demonstrate a serious cost to our approach, which we do not minimize. Splitting samples into three randomly formed conditions, rather than two, creates smaller sample sizes. Our estimator is a ratio of estimates and is therefore subject to additional uncertainty. To detect small differences with confidence will often require large samples."
"55","The power that randomization brings to causal inference is indisputable, as Fisher convincingly argued long ago. In his words, “Apart…from the avoidable error of the experimenter himself introducing with his test treatments, or subsequently, other differences in treatment, the effects of which the experiment is not intended to study, it may be said that the simple precaution of randomization will suffice to guarantee the validity of the test of significance, by which the result of the experiment is to be judged” (1935, 21). Indeed, this trait is the random‐assignment experiment's claim to fame, the envy of observational studies.         "
"56","We have not proposed to discard this power, but to harness it in a novel way for topics where it seems probable that self‐selection is an important part of the phenomenon. This is a simple idea, whose origins can be traced back decades (Hovland 1959). In essence, we simply recommend that analysts look past the overall mean treatment effect to address one especially important form of treatment effect heterogeneity. In the case of negative ads, many have advanced intriguing conjectures about forms of heterogeneity, including gender (King and McConnell 2003), sophistication (Stevens 2005), and personality type (Mondak et al. 2010). For that subject and for others, we are not proposing any particular substantive theory about heterogeneity, but stressing an even more fundamental point. If self‐selection of treatment (e.g., watching ads or not) is probable, then there is good reason to estimate the treatment effect separately for those who do, as a general rule, experience the treatment and those who do not.         "
"57","The experimental design proposed above is one general template to produce such estimates. The logistics of implementing the idea will vary with the problem under study, and we do not mean to minimize challenges. Implementing self‐selection in a satisfactory manner will often prove tricky, and best designs might not emerge except through multiple studies using alternative approaches. But the payoff will be a much improved understanding of the phenomena we study."
