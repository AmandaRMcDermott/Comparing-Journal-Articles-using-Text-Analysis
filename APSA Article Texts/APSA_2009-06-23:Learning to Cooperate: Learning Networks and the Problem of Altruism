"","x"
"1","The simulation model explores the situation in which two populations come in contact that are each well integrated internally but have been isolated from each other until recently, although our results turn out to be equally relevant for the problem of sustaining cooperation whenever learning only takes place within populations. The populations can represent a new ethnic group moving into an established neighborhood or village (Putnam 2007), stakeholders in two independent, well‐established policy arenas whose decisions increasingly impinge on each other (Schneider et al. 2003), or representatives of two parties, of independent Congressional committees or of specialized local, state, and federal authorities forced by a recent crisis or by economic development and increased globalization to deal with the same issues (Scholz and Stiftel 2005).         "
"2","In each situation, new opportunities for exchange between members of opposite populations hold out potential benefits involving some risk if the new partner does not cooperate. Exchanges could include straightforward exchanges of goods and services, but also of critical policy information, social or political favors, or joint policy projects or political agreements. Exchange is modeled as iterated or finitely repeated two‐person prisoner's dilemmas (IPD); we focus on the initial phase of cooperation in which two‐person interactions develop a foundation on which n‐person coalition and institution‐building activities can later be built (Lubell et al. 2002; Sabatier and Jenkins‐Smith 1993).         "
"3","Exchange opportunities involve random encounters of relatively short duration with an unknown population and are relatively unimportant compared with ongoing exchanges within each population. Individuals therefore economize on cognitive effort by utilizing trial‐and‐error imitation and unsophisticated innovation processes to learn simple strategies that perform well in the new exchanges (e.g., Bala and Goyal 1998; Bikhchandani, Hirshleifer, and Welch 1998; Fudenberg and Levine 1998).         "
"4","The model assumes that learning takes place through established relationships within each population—through friends, relatives, neighbors among ethnic groups, or through political allies and authority relationships in political or policy arenas. Individuals can tell who among their contacts does best in exchanges with the opposing population and can generally learn what strategy that person is using. Network analysis provides a means of representing different patterns of relationships as learning networks that provide the primary factor manipulated in the simulation study.         "
"5","Two different learning processes are analyzed: an imitation process that maximally exploits the existing repertoire of strategies and a more innovative process that explores a broader range of strategies. The myopic best response process always chooses the best‐performing strategy known to the player through learning network contacts. Imitation is particularly powerful in our study setting since all players confront the same set of players in the other population, so the best‐scoring strategy represents the best response for any player. This process accelerates the spread of the highest‐scoring strategy, but limits choice to the range of existing strategies and does not anticipate responses in the opposing population that may change the best response strategy in the future.1"
"6","The adaptive learning process provides for more innovation at the cost of slower dissemination of successful strategies. This process is based on proportional fitness selection models developed originally in evolutionary biology (e.g., Holland 1992) but used extensively to represent learning processes in evolutionary game theory as well (Fudenberg and Levine 1998). Individuals still seek the best response among learning network contacts, but their willingness to change their current strategy is now proportional to the difference in payoffs. Furthermore, individuals consider each component of the best response strategy and can adopt specific components rather than just copying the entire strategy. Finally, individuals have the capacity to adopt new strategic components not present in their own or the best response strategies, which corresponds to mutations in proportionate fitness models. In short, individuals can develop new strategies primarily by borrowing components from the most successful strategies, but also by occasionally developing a new component.2"
"7","A genetic algorithm is commonly used to model the adaptive learning process in studies of the evolution of cooperation (Gotts, Polhill, and Law 2003). The genetic algorithm is particularly valued as a problem‐solving search strategy because of its ability to find global maxima in large problem spaces (Mitchell 1996). The continuing ability to innovate reduces the risk of premature selection of an inferior local maximum, although innovation also imposes costs in terms of prolonging the search. Thus the trade‐off between these two learning processes reflects the observed conflict in complex systems between characteristics that enhance the exploration of new ideas and those that maximize the exploitation of existing ideas (Axelrod and Cohen 2001; March 1991). The best response process most rapidly exploits existing ideas in the population, while the adaptive process more broadly explores the full range of strategies potentially available to the population.         "
"8","Given an initial assignment of strategies to each player, the learning process and learning network fully specify an evolutionary process; as individuals learn from established contacts within their own population, the range and frequency of strategies coevolve in opposing populations. The coevolution of strategies thus determines the level of mutual cooperation achievable in the exchanges across populations."
"9","Our inquiry builds on the extensive social dilemma (reviewed in Gotts, Polhill, and Law 2003) and evolutionary dynamics (Szabó and Fáth 2007) literatures that focus on the emergence of mutual cooperation among individuals playing IPD. A political scientist, Axelrod (1984), helped establish the evolutionary analysis of cooperation in his inquiry into international conflict, although our discipline has tended to ignore extensive new developments in biology, physics, and sociology. Axelrod noted that nice (never defect first), retaliatory (defect whenever a defection is encountered) strategies like tit‐for‐tat (TFT) provide a basis for the evolution of cooperation if the shadow of the future and cooperative payoffs are sufficient. We will call these strategies retaliators, will call all remaining nice strategies that do not retaliate altruists, and will call all nonnice strategies that defect first exploiters."
"10","Axelrod transformed the study of cooperation from the choice of whether or not to cooperate in the prisoner's dilemma stage game to the choice of strategies to use in the IPD, as illustrated in Table 1. The upper panel presents a simplified representation of the underlying prisoner's dilemma (PD) stage game payoffs to row player. Any PD payoff matrix can be linearly transformed by first subtracting the sucker's (S) payoff from all payoffs to set S= 0 and dividing by the resultant temptation (T) payoff to set T= 1. By setting the penalty payoff P=x and the cooperation reward R= 1 −x and limiting x to the [0,0.5] interval, all PD games with symmetric differences between temptation and sucker payoffs (T − R = P − S) can be portrayed with just one parameter (Schmidt et al. 2001). The advantage of cooperation increases as x approaches zero.         "
"11","The middle panel illustrates how payoffs in the IPD for any pair of opposing strategies can be calculated for any given values of x and r, the number of repetitions that determines the length of the learning period after which strategies are evaluated. The example uses r= 5 to represent rapid learning cycles expected of the short, intermittent interactions in new exchange situations. Payoffs are given for row player when confronting the column player for three well‐known strategies: an altruist who always cooperates (AllC), a retaliator who cooperates initially and only defects whenever the opponent defected in the previous round (TFT), and an exploiter who always defects (AllD). Total payoffs for adopting a given strategy for a given population in the transformed strategy selection game can be calculated by multiplying each payoff in Table 1, respectively, with the frequency of encounter with each respective strategy.         "
"12","The strategy selection game transforms the original PD game into a coordination game with multiple equilibria. Note that TFT is both the payoff‐ and risk‐dominant equilibrium in Table 1 when paired only with AllD, since the total payoff for TFT will exceed that of AllD as long as TFT represents over 50% of opponents. Axelrod (1984) introduced a spatial dimension by noting that clusters of TFT players can invade an AllD population as long as clustering increases the frequency of mutual TFT contacts enough to ensure that the invading TFT players have higher payoffs than the dominant AllD population, thus giving TFT a selective advantage; for payoffs in Table 1, TFT will outscore AllD whenever TFT is at least 34% of the population. Indeed, Bendor and Swistak (1997) demonstrate that nice, retaliatory strategies like TFT gain the greatest advantage of clustering compared with all possible strategies whenever selection processes reflect the standard proportional fitness models of evolution. They find that retaliators can ensure their survival under all potential games and payoffs if they account for at least half the encounters.         "
"13","Since the three strategy types will be important for our analysis, the lower panel in Table 1 normalizes and generalizes the payoffs from the middle panel to illustrate our definition of the critical differences between retaliators, altruists, and exploiters.3 First, altruists and retaliators both gain the full benefit of continued mutual cooperation (= 1) whenever encountering their own or the other strategy. Exploiters encountering other exploiters gain a lesser amount (= 0) reflecting their continued mutual defection, the Nash equilibrium of the stage game. Second, exploiters are their own best response because they do better against themselves than either altruists or retaliators do against them (xa < xr < 0). However, retaliators are also their own best response as long as yr < 1: in the central panel payoffs, for example, the retaliatory TFT will get a higher payoff than AllD when playing against TFT (3.375 vs. 2.3). Altruists, on the other hand, are dominated by exploiters (ya > 1) to reflect their exploitability; retaliators are not as exploitable as altruists (ya > yr), although exploiters still do better against retaliators than against other exploiters (yr > 0).         "
"14","Only exploiters are evolutionary stable (ESS) in standard evolutionary models (e.g., Nowak 2006), but in the evolution of cooperation, being discrete really matters (Szabó and Fáth 2007). That is, “traditional ESS and Nash conditions are neither necessary nor sufficient to imply protection by selection in finite populations” (Nowak 2006, 122). Finite‐population models featuring stochastic replicator processes demonstrate that a single TFT can invade a full population of AllD (Nowak et al. 2004), a finding replicated for several finite‐population translations of the classic proportional replicator dynamic (Szabó and Fáth 2007). The proof is based on Markov chain analysis of replicator dynamics in which the probability ρ can be calculated that a single invading strategy can replace the full finite population.4 Intuitively, a lone TFT in an AllD population fares very poorly, but still has some nonzero probability ρTFT of reproducing and hence adding another TFT in the next period, and another in the following period, and so on until they take over the full population. For payoffs of the strategy selection game in the center panel of Table 1, the probability of taking over the whole population will be considerably higher for TFT than the probability ρAllD that a single AllD will take over a full population of TFT; TFT will outscore AllD as soon as TFT makes up at least 33% of the population, whereas AllD will outscore TFT only after it reaches 66% of the population. For population size N, selection favors cooperation as long as ρTFT > 1/N > ρAllD.            "
"15"," Nowak et al. (2004) calculate the boundary condition for this selection advantage in terms of payoffs and population size for any two strategies, as illustrated in Figure 1 for TFT and AllD.5 The horizontal axis represents payoffs in the stage game, x, and the vertical axis represents the size of the population, N, up to 200, the range of interest for our study. Each solid line portrays the boundary condition for r repetitions of the stage game, with r labeled at the top of the figure above the line. Selection favors TFT invading an AllD population for all values to the left and above the lines, labeled as “Conditions favoring TFT” for r= 2 in Figure 1.            "
"16","                 The Impact of Payoffs, Repetitions, and Population Size on Boundary Condition for TFT to Invade AllD                         "
"17","For a given population size, more repetitions (higher r, or greater “shadow of the future”) can compensate for less favorable payoffs; at r= 50, almost any payoff will favor the evolution of cooperation. For a given number of repetitions, the range of payoffs that favor cooperation increases rapidly with N for very small populations, but the marginal increase diminishes considerable after about N= 25. As population size increases beyond the range of Figure 1, however, the likelihood that a single invader could invade becomes increasingly smaller. Selection no longer favors cooperation as populations approach 1,000 or more, as is consistent with the infinite population results (Nowak et al. 2004).            "
"18","Markov chain selection processes with no mutations will eventually be absorbed into states with pure populations of a single strategy. In this case, when selection favors cooperation the state of full cooperation will be reached more frequently than when noncooperation is favored. With mutation pure populations are no longer absorbing, since mutations introduce new strategies that have some small potential of taking over the full population. In this case, the ratios ρTFT/(ρTFT+ρAllD) and ρAllD/(ρTFT+ρAllD) approximate the respective proportion of time spent in pure populations of TFT and AllD after all effects from initial conditions have been eliminated: that is, they provide the invariant frequency distribution of the Markov chain as mutation rates approach zero (Fudenberg and Imhof 2006). Using the payoffs in the middle panel of Table 1 for TFT and AllD, for example, a Markov chain with N= 10 would average about 67% of its time in full TFT populations with full mutual cooperation and the remaining time in full AllD populations with no cooperation at all, so we would on average observe mutual cooperation 67% of the time. If the population grows to N= 20, the probabilities are even more favorable to cooperation, with mutual cooperation expected on average 96% of the time.            "
"19","The presence of altruists compromises the selective advantage of retaliators because these neutral mutants do as well as TFT in mutual encounters, but provide support for exploiters when they meet. This support in turn reduces the selection advantage of retaliators (Bowles 2004; Imhof, Fudenberg, and Nowak 2005). The invariant distribution in a three‐strategy process that includes the altruistic AllC strategy for the same payoffs and population N= 10 would now include 55% AllD, 38% TFT, and 7% AllC. By including altruists, the average expected level of mutual cooperation by nice strategies drops from 67% to 45%. Since it is unclear how well these Markov chain processes reflect the learning processes and learning networks we model, we will return to this problem of altruists in the results section.            "
"20","To understand the impact of learning networks on the evolution of cooperation, we turn from homogeneous population models to those that use graph theory to represent local network structures. In these models, vertices or nodes generally represent players, and links connecting the nodes (called edges or arcs) indicate interactions (Ohtsuki and Nowak 2008; Szabó and Fáth 2007). The game network specifies who plays with whom; most models assume that players learn about other strategies only from those they play, so they assume that the learning network (who learns from whom) and game network are the same. Based on these assumptions, simulation studies have shown that game networks can provide sufficient clustering to favor cooperation for two‐dimensional lattices (Axelrod (1997), for lattices with random (small world) connections (Macy and Skvoretz 1998), and even for stable random graphs (Cohen, Riolo, and Axelrod 1998)—the structures that we will analyze.         "
"21","The limited contacts determined by local structure in graphs provide cooperative advantages much as do limited populations in finite models. Ohtsuki et al. (2006), for example, show analytically that cooperation can evolve even for simple cooperate or defect strategies in one‐shot PD games on infinite lattices.6Ohtsuki, Pacheco, and Nowak (2007) extend this lattice‐based analysis to separate networks for games and learning and demonstrate that when both networks overlap, the network with the largest degree determines the required cooperative benefit necessary to attain cooperation. Thus greater connectivity or degree of the learning network does not affect cooperation if it is smaller than the game network, but works against cooperation if it is larger.7 Both analyses suggest that poorly connected learning networks support higher levels of cooperation, although simulation studies of IPD on two‐dimensional lattices have provided mixed support for this conclusion.8"
"22","These results cannot be applied directly to our learning problem because their Moran process and simple two‐strategy evolution are not particularly suited for a learning model, because the regular graphs they rely on provide a very limited representation of learning networks, and particularly because there is no overlap between game and learning networks in our two‐population learning problem; if at all applicable, the implication might be that cooperative benefits would have to approach infinity as the proportion of overlap approaches zero. To provide a more adequate foundation for hypotheses about the impact of learning network structure on cooperation, we next consider three of the most common characteristics of graphs—size, degree, and clustering (Wasserman and Faust 1993)—from the perspective of the exploitation‐exploration trade‐off discussed previously for the learning process (cf. Lazer and Friedman 2007).         "
"23","Graph characteristics that enhance the set of available strategies and accelerate the diffusion of highest‐scoring strategies would provide a network function that maximally exploits the best available strategy. Sampling theory suggests that a larger size (number of nodes in full network) would provide a higher probability of including high‐scoring strategies, assuming as the model does that the initial strategies in the learning process are independently and randomly distributed across nodes. Similarly, a higher degree (or number of learning links per node) increases the likelihood that the highest‐scoring strategy will be in each node's learning set. Put differently, a higher degree decreases the average number of steps (or path length) required for the highest‐scoring strategy to reach every node. Hence we would expect larger size and higher degree to be associated with the ability to rapidly exploit existing high‐scoring strategies.            "
"24","Clustering for a given node is defined as the proportion of all linked nodes that share links with each other, or the proportion of a player's learning contacts that learn from each other. Clustering provides redundancy at the cost of a broader reach: if all nodes have a degree of 4, for example, a node with fully clustered relationships can reach only eight nodes in two steps, compared with 16 nodes if none of the connected nodes were connected with each other. Redundancy reduces transmission efficiency, although it may enhance accuracy when transmission errors or congestion causes problems. Granovetter (1973) found that successful searches for new jobs relied on nonredundant “weak ties” that brought in a wider range of information. In short, more clustering leads to slower diffusion and hence less exploitation.            "
"25","The featured role of highly clustered, overlapping networks of reciprocation in the resolution of social dilemmas (Coleman 1988; Granovetter 1985; Putnam 1993) is perhaps most commonly understood in terms of the game networks, where clustering provides the previously discussed and well‐known advantages for cooperative strategies. Yet some of the common arguments in support of social capital refer also to learning effects relevant to the two‐population problem. Clusters of individuals interlinked by overlapping strong ties can develop shared values, mutual cooperation, and trust. Within clusters, like‐minded neighbors can more readily share experiences about encounters with the other population and explore alternative strategies for dealing with them. In conjunction with individual learning processes that emphasize innovation and exploration, clustered learning networks in which the local best response strategy is altered and recombined in multiple ways may provide creative crucibles required for developing robust cooperative strategies where none exist.            "
"26"," Watts and Strogatz (1998) and Watts (1999) argue that small‐world networks combine the clustering advantages of ring networks with the efficient transmission advantage of random graphs. They also provide a more realistic representation of observed social networks than either of these theoretically interesting graphs. The ring network links each node symmetrically to its k/2 nearest neighbors on each side in a ring or one‐dimensional lattice, where k is the number of links per node. Whenever nodes have more than two links, the overlapping links provide a clustered neighborhood in which neighbors of neighbors are linked. The random network links each node to every other node with an equal probability, which provides much shorter path lengths than rings with the same number of nodes and links. Randomly assigned links also ensure low clustering since one node's neighbors have no more than average probability of being linked together.            "
"27","The small‐world network begins with the ring structure, but then randomly assigns each existing tie with probability β to a randomly selected node to which the initial node is not already linked.9Watts (1999) shows that, as β increases, average path length initially decreases very rapidly while clustering decreases only slowly. Thus small‐world networks with low values of β may simultaneously offer the global exploitation advantages of short path lengths in random networks and the local exploration advantages of higher clustering in ring networks. If the exploration‐exploitation trade‐off affects the evolution of cooperation, small‐world learning structures may provide relatively high levels of both.            "
"28","We investigate the relative importance of exploration and exploitation by comparing these three types of networks for different population sizes and degrees. The exploitation hypothesis implies that cooperation will be more prevalent in larger, better connected, less clustered populations. On the other hand, the exploration hypothesis implies that cooperation will increase with clustering, while the exploration‐exploitation trade‐off hypothesis suggests that the small‐world network will outperform the other two."
"29","Agent‐based models represent the decision and learning processes for each agent in a computer program, using fixed parameters that reflect the model's assumptions and variable parameters that represent the experimental conditions to be manipulated (Axelrod 1997). In our model, we vary the learning process and learning network structure to evaluate their impact on levels of mutual cooperation in IPD games played across populations. Computer simulation has played a particularly useful role in the analysis of social dilemmas in network settings that are too complex for currently available analytic models (Gotts, Polhill, and Law 2003; Szabó and Fáth 2007). Our simulation experiment is based on a discrete‐time, autonomous, finite‐population, stochastic adjustment model (Fudenberg and Levine 1998, 137) involving games played between two populations where strategies are learned within each population. We first review how the model represents the basic assumptions discussed previously and then describe the experimental design.         "
"30","Strategy To represent an initial learning situation in which many potential strategies may exist, including a variety of retaliatory, altruistic, and exploitative strategies, we randomly assign to each player (the agent) with equal probability one strategy from the complete set of 32 pure strategies with a one‐period memory (Gotts, Polhill, and Law 2003). The learning process then potentially modifies each player's strategy in each learning cycle, which we will refer to as a learning period.            "
"31","The genetic coding for each strategy specifies whether to cooperate (C= 1) or defect (D= 0) in the first period and then whether to cooperate or defect after each of the four possible outcomes in the previous period {DD, CD, DC, CC}, with the player's choice indicated first and the opponent's choice second. We can therefore code a player's strategy with five binary variables or bits to represent the total strategy space of 25= 32 strategies. The first bit indicates the first‐round choice, and the remaining four bits encode instructions for the four possible outcomes of prior interaction, giving a five‐bit representation of the strategy (Nice, DD, CD, DC, CC).            "
"32","For example, (0, 0, 0, 0, 0) represents the AllD strategy that always defects, while (1, 1, 1, 1, 1) represents the AllC strategy that always cooperates. The eight nice strategies that will never be the first to defect are represented by (1, z, z, z, 1), where z can take the values of either 0 or 1. Of these nice strategies, the two retaliatory strategies that always retaliate after the other side defects include TFT (1, 0, 0, 1, 1), which cooperates in the first round and then cooperates whenever the opponent cooperated in the previous round, and GRIM (1, 0, 0, 0, 1), which cooperates in the first round, and then cooperates only if both players cooperated in the previous period.            "
"33","Learning The two learning processes outlined above are both based on each player's fitness score for the learning period, which is determined by the player's average payoffs for all r repetitions of the stage game across all games with the opposing population. The myopic best response process adopts the best‐scoring strategy among a given player's contacts, as determined by the learning network. The adaptive learning process, on the other hand, copies each gene in the best‐scoring strategy with a probability determined by               "
"34","Experimental Design There are three nested stages in the experimental design. Stage 1 consists of setting all fixed and variable parameters for a given experimental condition and then running stages 2 and 3 for each condition. We fix the payoff for mutual defection at x= .325, r= 5 after initial analysis discussed in the next section. To maintain tractability and focus on learning networks in the subject population, we also fix one population (representing authorities in our initial formulation) and vary parameters only in the other population (representing subjects). The number of authorities is fixed at N= 10 with a modestly connected (k= 2) ring learning network. We then systematically vary each network parameter for the subject population using four values for N∈{25, 50, 100, 200} and three for k∈{2, 4, 8}for the ring, small‐world, and random networks, providing 4 × 3 × 3 = 36 experimental conditions to be evaluated. The range of parameter values was set to reflect the likely size and communication structures of regulatory authorities and firms within local jurisdictions analyzed in Scholz and Wang (2006).            "
"35","Since ring networks have only limited clustering and therefore may not adequately test the impact of clustering, we also include two “caveman” or village networks for each population size, one with five and the other with 10 nodes per village; N= 25 is omitted for villages of 10 because it cannot be constructed.10 Within each village all nodes are interconnected, and two “gatekeeper” nodes are also linked to different neighboring villages, creating a ring of densely clustered villages, each linked to two adjoining villages.            "
"36","For each experimental condition in stage 1, stage 2 creates the appropriate network structure and assigns each player an initial randomly determined strategy. Since the model contains stochastic elements in network structures, strategy assignments, and the learning process, stage 2 is repeated multiple times for each experimental condition. The main reported results are averaged over 900 simulations per experimental condition and thus represent the expected level of cooperation for each experimental condition given the initial uniform distribution of strategies across nodes."
"37","Stage 3 performs the individual simulation for each assignment of networks and strategies from stage 2. All players initially play five repetitions of the stage game with every player in the opposite partition. They then observe the outcomes associated with strategies of players they are linked to and use this information to update their strategy. All players again play five repetitions in the second period, again observe linked players and update their strategy, and so on. The learning cycle is repeated for 3,000 periods to ensure that we reach a relatively steady state, and information about the level of mutual cooperation is recorded in each period. Note that the relatively small number of iterations in the IPD and the large number of learning cycles are consistent with our assumption that adaptive feedback learning rather than strategic calculation better characterizes the early contact situation we model: players use simple strategies but adapt rapidly over a long period."
"38","Our initial analysis probes whether stable levels of cooperation could emerge under any payoff conditions. We systematically vary payoffs in the range .20 ≤x≤ .40 and compare the mean level of mutual cooperation (the proportion of CC outcomes) in the subject population achieved after 3,000 learning periods, averaged in this preliminary test over 500 simulations for each value of x.11 The four values of x≤ 0.3 achieve a proportion of mutual cooperation between .8 and .9 within the first 200 periods and remain at these levels, revealing the typical Markov chain pattern of an initial burn‐in period influenced by the initial conditions followed by stable oscillations representing a typical Markov chain pattern.         "
"39","When x > 0.35, cooperation never rises substantially, confirming an invariant distribution in which selection does not favor cooperation. The values between 0.3 < x < 0.35 define the most interesting transition zone in which cooperation is only moderately favored. Strong selection effects outside this transition zone leave little room for learning networks to make a difference; why consult with friends if the gains or losses from cross‐population exchanges are readily apparent from personal experience? We select x= .325 for the remaining experimental manipulations to analyze the effect of networks within the transition zone. For this payoff, selection is favored for N > 9 in Figure 1, so even the fixed population of N= 10 is just sufficient to favor cooperation in the finite population model. Network manipulations produce levels of cooperation ranging from a low of 59.5% (N= 25, k= 2, random) to a high of 71.9% (N= 200, k= 8, ring), which indicates the relative differences in cooperation attributable to differences in learning networks within the transition zone. Although specific levels of cooperation would undoubtedly differ, the observed patterns of results reported below should be generalizable to the comparable transition zones determined by other values of x, r, and N.         "
"40","We next test the myopic best response learning process on all ring, random, and small‐world networks, and find that achievable levels of mutual cooperation never rise above 15%. This learning process rapidly eliminates altruists and retaliators alike; on average, exploiters comprise 86% of the final population, and the remaining 14% is evenly split between retaliators and altruists. The evolution of strategies stops well before the 25th period; with rapid dissemination and no mutation, the initial uniform distribution of strategies determines which absorbing state is reached. With only 25% of the strategy types being nice, initial distribution apparently favored exploiters 86% of the time. Since low levels of cooperation associated with myopic best response provide little room for learning networks to matter, we only use the adaptive learning model to analyze the impact of learning network structure.            "
"41","The two graphs in Figure 2 report the proportion of mutual cooperation in the final 500 periods, averaged over 900 simulations for each experimental condition or 450,000 observations; this reflects the time‐averaged invariant distribution of strategies associated with each condition's unique Markov chain. The symbols indicate the observed level of cooperation and lines connect observations of the same type. The upper graph reports the results as a function of population size and degree (averaged over all types of networks), while the lower graph reports the results for village, ring, small‐world, and random networks for the indicated population size (averaged over all degrees).            "
"42","                 Observed Levels of Mutual Cooperation                         "
"43"," Note: Mean values for last 500 periods, averaged over 900 runs per condition.                        "
"44","The upper graph shows that cooperation increases with both population and degree, but at a declining rate. Population size consistently accounts for the greatest variance in both graphs. Holding degree constant, the level of cooperation rises with each increase in size, although the rate of change decreases as the population becomes larger and appears to level off around N= 100.12 Cooperation also tends to increase with degree when holding population constant, although the differences between degrees of four and eight are small, suggesting a limiting advantage as degree increases beyond eight. Villages of degree five and 10 both outperform other networks with degree eight, and both of these village networks perform at very similar levels, suggesting that clustering also plays a role in promoting cooperation.            "
"45","The lower graph confirms the importance of clustering based on a comparison of different network structures. For all sizes, the clustered village and ring networks provide the strongest support for cooperation. The advantage of the ring is more evident for smaller sizes, with differences that tend to disappear as the size increases to N= 100, as in the upper graph. The small‐world results are not much lower than the ring, but clearly are not better; the expected advantage of the small world for the exploration‐exploitation hypothesis is not supported.            "
"46","Overall, the two graphs show that cooperation varies with all three dimensions, with the largest differences provided first by size, then by degree, and finally by the clustering of the network structure. Achieving higher levels of cooperation apparently requires some blend of exploration and exploitation, although not the blend associated with the small‐world network. The strong performance of the adaptive learning process and the clustered village network underscore the advantages associated with more global exploration. The gains associated with greater size and degree, on the other hand, support the role of rapid exploitation. To explain these results, we analyze the impact of each factor on the selection advantages of retaliators, altruists, and exploiters."
"47"," Figure 3 illustrates the critical role of retaliators and altruists in the evolution of mutual cooperation for one experimental condition. The solid line in Figure 3 reports mutual cooperation in each of the 3,000 periods averaged over 500 simulations, with greater detail shown in the left panel for the first 25 periods. As in all conditions, mutual cooperation starts at an expected 25% that reflects the randomized uniform distribution of strategies, 25% of which are nice. Cooperation quickly drops to a very low level as exploiters rapidly replace exploitable nice strategies in the first 25 periods, after which it recovers and reaches a relatively stable plateau after the first 1,000–1,500 periods. Note that each period reports averages—most initial conditions lead rapidly to a full extinction of nice strategies, while the few favorable ones lead rapidly to high levels of cooperation.            "
"48","                 Nice Strategies Explain the Evolution of Cooperation                         "
"49"," Note: Solid line represents the mean proportion of mutual cooperation averaged over 500 simulations for small world with N= 200 and k= 8 for each of the 3,000 generations. The dotted line presents the proportion of all strategies that are retaliators, and the dashed line presents the proportion that are nice strategies (including retaliators). The vertical line separates the expanded scale for the early periods from the compressed scale for the later periods.                        "
"50","The dashed line in Figure 3 plots the proportion of the population represented by the eight nice strategies that always cooperate with each other, which account for most of the observed mutual cooperation. Their proportion begins at the expected .25 (representing their proportion of strategy types), drops rapidly in the first few periods, and then expands following a path similar to that of cooperation. The dotted line charts the evolution of the retaliatory strategies, TFT and GRIM. Retaliators on average maintain their proportion of the population as the altruistic nice strategies are eliminated and account for almost the entire population of nice strategies in this initial hostile environment dominated by exploiters. Once favorable distributions emerge, the selection advantage of retaliators increases their proportion to just over half the population.            "
"51","Altruists expand almost as rapidly as retaliators in the more favorable environment created by retaliators. In the final conditions reflecting the invariant distribution, the observed population consists of 54% retaliators, 26% altruists, and 20% exploiters. To explain this expansion of altruists, Figure 4 shows the probability that TFT genes will be copied by AllC and AllD for different proportions of each strategy, calculated using the adaptive learning algorithm (equation 1). The horizontal axis represents the proportion of altruists (AllC), and the vertical axis indicates the corresponding probability of copying a gene from the retaliatory strategy (TFT). Retaliators outscore exploiters for all values in the figure, so selection always favors retaliators. The lines indicate the probability of copying by altruists (the three dotted lines) and exploiters (the three solid lines) for populations with proportions of retaliators p(t) = .7 (indicated by circles), .5 (triangles), and .3 (squares), respectively.            "
"52","                 Probability of Adopting Retaliatory Strategy Decreases with the Proportion of Altruists                         "
"53"," Note: Lines indicate the probability that Altruists and Exploiters will adopt genes from the retaliatory strategy for three fixed proportions of retaliatory strategies p(t), based on payoffs for Altruist = AllC, Retaliator = TFT, Exploiter = AllD for x= .325, r= 5.                        "
"54","The downward‐sloping lines indicate that selection pressure against altruists and exploiters decreases as altruists increase. More importantly, selection pressure increases against exploiters but decreases against altruists as retaliators increase from .3 (squares) to .5 (triangles) and .7 (circles). When retaliators compose half the population and retaliators and altruists are equally divided at 25% each in Figure 4, the probability of copying from TFT is .11 for altruists versus .02 for exploiters. If the proportion of retaliators increases to 70% and altruists remain at their final average of 25%, the probability of conversion reverses to .02 for altruists and .10 for exploiters.            "
"55","Any deterministic selection process would produce a full population of retaliators from any distribution in Figure 4, but the mutation and proportionate fitness features of the adaptive learning process at some point counterbalance the retaliators’ selection advantage over altruists.13 Selective learning allows at least some exploiters to copy nice genes but not retaliatory ones, resulting in an increase in altruists rather than retaliators at the expense of exploiters; the increase in altruists reduces selection pressures on both exploiters and altruists. In addition, as retaliators grow in number, mutation converts more retaliators to altruists than altruists to retaliators; at some point, mutation conversions outweigh selection conversions, thereby limiting the growth of retaliators. Mutation also produces exploiters, but selection pressures increase against exploiters as retaliators increase and are not counterbalanced by mutation. In sum, the combination of diminishing selection pressures, selective learning, and mutation allow the population of altruists to survive, which in turn allows evaders to survive as well. The stochastic learning process produces oscillations driven initially by the increase in retaliators, with occasional expansion of altruists to the point where exploiters outscore retaliators, followed by decreases in the altruist and retaliator populations until retaliators again outscore exploiters, and so on.            "
"56","Thus innovative learning processes provide both advantages and limitations to the evolution of cooperation. Although innovation in the form of mutation aids and abets altruism in limiting achievable levels of cooperation, it is critical in early periods to escape the dominance of exploiters. Most of the runs we observed crashed to all‐exploiter populations several times in the first thousand periods, and hence relied on mutation to reintroduce retaliators and eventually achieve higher levels of cooperation. Far from being a peculiar artifact of evolutionary models, mutation represents a critical component of the process of learning to cooperate that deserves further exploration."
"57","Learning processes that enhance exploration rather than exploitation appear to be more conducive to cooperation, but structures that enhance exploitation appear to be more advantageous for learning networks based on the results in Figure 2. Population size and degree increase cooperation, and basic probability theory suggests the reason that they can enhance the selection advantage for retaliatory strategies. The likelihood that mutation will produce a retaliator in an all‐exploiter population is proportional to the population size times the mutation rate, so larger populations provide the higher likelihood of retaliators that is critical for successful invasion (Nowak 2006). Furthermore, the likelihood that a retaliator will be in the learning set of the average player increases exponentially with the number of partners, accelerating the rate of expansion whenever retaliators have achieved a selection advantage. For example, once TFT exceeds the .25 threshold frequency and becomes the highest scorer, the probability that a randomly drawn learning set will NOT include TFT is .75 for only one partner, .75 * .75 = .56 for two partners, .1 for four partners, and miniscule for eight partners.            "
"58","Yet even in learning networks, exploration appears to play some role in enhancing cooperation, since we have seen that clustering increases cooperation particularly for lower values of size and degree. Clustering creates redundant links, which reduces the spread of successful strategies for a given degree. Although this retards the spread of retaliators when they are in the minority, it also stabilizes the population when they are in the majority. Small populations with small learning sets are more prone to exaggerate stochastic disturbances, so clustering can dampen the oscillations between expanding altruist and exploiter populations in these conditions once high levels of cooperation have been achieved.14 Of course, clustering also retards the initial growth of cooperation and recovery from occasional successful invasions of exploiters,15 but the overall dampening effect still favors cooperation since the evolutionary process spends more time at higher levels of cooperation once initial conditions become less important. This defensive role of clustering appears similar to the role of conformist norms in Bowles (2004), which helped to prevent the invasion of “grabbers” in more cooperative populations.            "
"59","We believe that these findings apply well beyond our simulation model. The simulation modeled the initial development of cooperation, but the stability of cooperation around an apparent Markov chain invariant distribution suggests that the results apply equally to the maintenance of cooperation. The simulation included two populations, but the ten encounters with agents in the “authority” population could just as well represent random encounters with other agents within a single population, particularly since observed average frequency distributions of strategies in both populations were almost identical. The simulation explored only one value of x and r, but the same factors should influence the evolutionary process at other transition zones as indicated in Figure 1. Similarly, an increase in the size of the authority population (or equivalently an increase in the number of agents encountered each period) should also influence the values of x and r that determine the transition zone, but not the evolutionary process within that zone. Finally, the limitation on population size capable of supporting cooperation suggested in finite‐population models applies only when every agent at least potentially exchanges with every other agent; the structuring of exchange interactions into game networks can provide an alternative means of partitioning even large populations in a manner capable of supporting cooperation. To further explore the interaction between such game network structures and the learning networks analyzed here, our findings suggest that models including retaliators, altruists, and exploiter payoffs as defined in Table 1 would be sufficient to explore the critical evolutionary process.            "
"60","We find that the way individuals get and process information about exchanges strongly influences the coevolution of cooperation as two populations warily seek to establish and maintain mutually advantageous exchanges. An adaptive learning process that is not too quick to exploit existing knowledge is particularly important for achieving high levels of cooperation, given the relatively hostile and diverse initial environments where learning commences. The best response learning process in which individuals immediately adopt the highest‐scoring strategy available to them produces little cooperation here; everyone quickly adopts the exploitative strategies favored in most initial distribution of strategies, and the learning process has no innovative feature to reintroduce the retaliatory strategies necessary to support cooperation. Best response learning can accelerate the evolution of cooperation, but only if retaliatory strategies are dominant; adaptive learning, on the other hand, can achieve high levels of cooperation even when exploitative strategies dominate initially because it is both innovative and more selective in learning from higher‐scoring strategies. Intelligent innovation and learning may be helpful, but even the simple‐minded stochastic process in our model of trying new strategies and retaining successful ones plays a critical role in learning to cooperate. In our earlier examples, membership turnover in the legislature, newly activated stakeholders in policy arenas, and increased rate of immigration may all accelerate the initial evolution of cooperation to the extent that they enhance exploration through the introduction of new strategies and the enhanced willingness of new arrivals to consider a range of strategies."
"61","Innovation is critical in hostile early environments, but eventually limits the average level of attainable cooperation by abetting the problem of altruism. Without altruists, retaliators could fully dominate the population and sustain higher levels of cooperation. But altruists are neutral mutants of retaliators who do as well against retaliators, but fail to retaliate against exploiters; neutral drift through mutation expands the altruist population, creating cycles in which exploiters subsequently expand and decline. The rate at which nice strategies lose their retaliatory edge thus determines the attainable level of cooperation. Institutions that encourage conformity and hence constrain innovation could enhance cooperation once retaliators are dominant (Bowles 2004), but conformity would also diminish the likelihood that sustainable cooperation would coevolve in the first place. Thus stability rather than turnover in the legislature, in policy arenas, and in existing immigrant communities becomes valuable to sustain cooperation once it has been achieved.         "
"62","Learning networks determine the spread of successful strategies; a larger, more connected population with more overlapping clusters of relationships averages 72% mutual cooperation in our simulation, a 12 percentage point gain over the 60% averaged by the smallest, least connected, least clustered population. Larger populations and more contacts both increase the chance of learning from successful retaliatory strategies and hence increase the growth of retaliatory strategies whenever selection favors cooperation. Thus larger populations with more developed learning relationships should on average exhibit higher levels of cooperation. This would suggest that cooperation across longstanding cleavages would develop more rapidly in larger interactive legislatures, larger more established policy arenas, and larger integrated immigrant communities—in the latter case, however, analytic results for finite populations suggest that at some point larger populations may impede the evolution of cooperation unless interactions are restricted into structured game networks."
"63","Clustering in learning networks does not hasten the development of superior strategies, but appears to decrease the ability of exploiters to invade once high levels of cooperation have been established, an advantage that is most apparent in smaller, less connected populations that are more prone to such invasions. Thus clustering, like conformity constraint, is more valuable in the later rather than earlier stages of learning to cooperate and is most important for smaller population size and degree. In our examples, the clustering of legislators into geographic or policy subgroups within each party, of stakeholders into multiple policy study groups and planning venues within each policy arena, of immigrants and existing citizens into segregated neighborhood organizations or church groups may potentially serve this function, at least to the extent that these clusters increase the redundancy of existing ties within the population."
"64","Of course, the evolutionary advantages of cooperation identified in our individual learning model can be accelerated or retarded by existing institutions and coalitional activities. For example, if the era of partisan confrontation were to end in the U.S. Congress, our model suggests that bipartisan cooperation would be more likely to emerge in the larger House than in the smaller Senate, ceteris paribus. Differences in formal authority and party strategies that enhanced partisan conflicts to begin with, however, are likely to overshadow individual learning processes in shaping any possible bipartisan cooperation. Similarly, in particularly volatile situations in which even a small number of individual defections from either group will trigger an escalation of ethnic conflict and a complete breakdown of intergroup relations, the development of effective “ingroup policing” mechanisms or a coordinated fear of triggering the “spiral equilibria” to prevent defections (Fearon and Laitin 1996) may be necessary as a parallel process in support of the individual learning emphasized in our model. Contextual features such as these are important to consider when applying our basic model; incorporating important classes of features into future models can provide a stronger foundation for understanding the evolution of cooperation in the broad range of settings encountered in political science.         "
