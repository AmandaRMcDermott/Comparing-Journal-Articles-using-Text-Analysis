"","x"
"1","            "
"2","The function of statistical tests is merely to answer: Is the variation great enough for us to place some confidence in the result; or, contrarily, may the latter be merely a happenstance of the specific sample on which the test was made? The question is interesting, but it is surely secondary, auxiliary, to the main question: Does the result show a relationship which is of substantive interest because of its nature and its magnitude? (Kish 1959, 336)               "
"3","Some time ago, I attended a talk by a scholar doing research on teacher training effectiveness. After explaining his research design, in which teacher success would be operationalized using students' scores on standardized exams, he presented a slide with a long list of coefficients estimated under several models. In keeping with ritual, he called our attention to one or two variables, which had the good fortune to be marked by asterisks. He then noted that, according to his results, parents might do well to ask whether their children's teachers received in‐state training; after controlling for a long list of other predictors, the estimated “effect” of in‐state training was found to be positive and statistically significant. Asked to interpret model coefficients, the presenter was unable to do so. As it turned out, the expected jump in test scores associated with a teacher being trained in‐state was around one‐fortieth of a standard deviation! Pressed on whether a parent should seriously be concerned by a (predicted) relationship so small, he conceded that the magnitude did not seem too large, but it was, after all, statistically significant. This extreme deference to statistical significance, wherein the very term statistical is lorded over the audience as if to imply that it is simply a more rigorous form of everyday significance, leads to opportunities for mischief and—even more perniciously—rewards laziness.         "
"4","Over the past half century, individual fields in the behavioral, health, and social sciences have grappled publicly with the role significance testing of hypotheses should take in the assessment of research results.2 The disciplines of psychology, sociology, and economics have devoted entire volumes to the topic (Altman 2004; Harlow, Mulaik, and Steiger 1997; Morrison and Henkel 1970). One of these (Harlow, Mulaik, and Steiger 1997) bears the provocative title What If There Were No Significance Tests? This was no empty rhetoric; around the time of the book's publication, the American Psychological Association in fact appointed a task force to consider the recommendation that journal editors ban the reporting of p‐values altogether (Wilkinson and Task Force on Statistical Inference 1999). While the proposal did not pass, the task force recommended that estimates of effect size accompany any published p‐values, and by 2002, no fewer than 19 journals required the reporting of “effect sizes,” a family of standardized measurements meant to identify nontrivial effects of experiments (Thompson 2004).3"
"5","Why all the fuss about p‐values? Despite concerns about their widespread misinterpretation—indeed, one's fixation on p‐values and asterisks seems to be proportional to one's misunderstanding of what they actually measure—these are merely the most recognizable trappings of an overall framework that overemphasizes minor details. It is not so much their inclusion in analyses that is objectionable as much as their outsized role. As two of the most outspoken critics of NHST assert, “statistical ‘significance,’ once a tiny part of statistics, has metastasized” (Ziliak and McCloskey 2008, 4), causing many of us to obsess over signal‐to‐noise ratio in our data—even to the point of forgetting to ask what exactly we are measuring. The more we dwell upon the simple detectability of a signal rather than attempting to characterize and interpret it in context, the more likely we are to be satisfied with the former and permit the absence of the latter.         "
"6","Numerous authors have presented a long list of criticisms of the NHST approach to social science, and these have been extensively reviewed elsewhere (see, for e.g., Cohen 1994; Gigerenzer 1998; Gill 1999; Meehl 1978; Ziliak and McCloskey 2008). A number of criticisms dwell on persistent misinterpretations of key concepts (p‐values, significance levels, null versus alternative hypotheses, the meaning of statistical significance); it is thus tempting to come to the conclusion, as many have, that we need simply to do a better job instructing our students. In fact, some of the most egregious mistakes (e.g., treating p‐values as the probability that the null hypothesis is true, or 1‐p as the probability of result replication) are banished from our scientific rhetoric during graduate training. The nature of scientific inquiry, though, leads us inexorably to seek within our data the means to assess the relative plausibility of competing hypotheses; however well we might master the acceptable frequentist rhetoric, we cannot help but be unsatisfied by estimating Pr (data|H0), tantalizingly close as it is to Pr (H0|data). As Falk and Greenbaum (1995, 94) write, “Significance tests fail to give us the information we need, but they induce the illusion that we have it.” Thus, we learn to apply a probabilistic analogue to Aristotle's so‐called modus tollens syllogism:            "
"7","If the null hypothesis were true, these data would be unlikely to have arisen."
"8","These data have in fact arisen."
"9","⇒ The null hypothesis is unlikely to be true."
"10","Although we teach some version of this to our students as the basis of NHST logic, such thinking is not as firmly grounded as it might seem; it has been named the “permanent illusion” and even more provocatively, the “Bayesian Id's wishful thinking” (Gigerenzer 1993, as cited in Cohen 1994). Critics have launched challenges to this extension of deductive logic since at least as long ago as Berkson (1942). It is not so much that the p‐value is difficult to define; rather, how we should use the p‐value, properly defined, is at issue. A correct interpretation of the p‐value simply raises the question of how we should use this information in drawing inferences, from a frequentist perspective. According to the oft‐repeated quip of Jeffreys (1961, 385), “What the use of P implies is that a hypothesis that may be true may be rejected because it has not predicted observable results that have not occurred.”         "
"11","That key elements of NHST reasoning are so easily and persistently misconstrued in fact reveals a large gap between what researchers expect from hypothesis testing and what NHST actually allows. The two most troubling aspects of the NHST approach in practice are (1) that it compels us to engage in a sort of Kabuki theater, going through the motions of what Rozeboom (1960) has called our “tribal ritual” of rejecting H0, when we know that with a large enough sample, a point null hypothesis will almost surely be rejected, and (2) the nagging sense that engaging in this empty charade distracts us from delving more deeply into matters of measurement, interpretation, and inference. By repeatedly pretending to be making an up‐down decision about a null hypothesis that we know a priori to be false, we risk mindless engagement in this ritualization (Carver 1978; Cohen 1994) and are all too willing to believe that it allows an “automaticity of inference” that “remove[s] the burden of responsibility, the chance of being wrong, the necessity for making inductive inferences, from the shoulders of the investigator and place[s] them on the tests of significance” (Bakan 1966, 430).         "
"12","Few have bucked the trend and offered spirited defenses of NHST. Frick (1996) emphasizes the point that NHST is especially well suited to ordinal claims, conceding that the approach is not sufficient when the magnitude of a relationship is important. Mogie (2004), while ostensibly writing in defense of NHST, recommends confidence intervals to supplement testing in this manner. Chow (1998, 193), in one of the more enthusiastic defenses of NHST, calls into question the “putative importance of the effect size,” claiming that it “is not an index of the evidential support for the substantive hypothesis offered by the data.” He concedes that “statistics and practical importance belong to two different domains,” but he seems to believe that the two should therefore be segregated.         "
"13","Unfortunately, we have seen what can result when analysis is constrained to statistical significance without consideration of substantive importance. We wind up placing all emphasis on differentiating signal from noise, leading to such distortions as publication bias, due to the infamous “file‐drawer problem” (Rosenthal 1979), the presentation of models that are “only the final tip of an iceberg of dozens if not hundreds of unpublished alternative formulations,” rendering estimated standard errors questionable (Schrodt 2006), p‐values in published work clustering suspiciously around .05 (Gerber, Green, and Nickerson 2001), and so on—with the likely result that “most claimed research findings are false” (Ioannidis 2005, e124).         "
"14","With so many drawbacks (just a few of which are listed above), why has this form of significance testing survived and thrived? Clearly, force of habit and the desire for automaticity are difficult to curtail. Yates (1951, 32), blaming a methodological setting of “utmost confusion” at the time of Fisher's major contributions, explains that “in the interpretation of their results research workers in particular badly needed the convenience and the discipline afforded by reliable and easily applied tests of significance.” The simplicity and concreteness offered researchers scaffolding on which to build reasonable and reliable habits. Nearly a century after Fisher, we may be ready to let some of that scaffolding fall away in order to discover more flexible approaches to statistical and scientific reasoning.         "
"15","               "
"16","When do we stand up and say “Enough already!”? When do we decide that ample arguments have been uttered and sufficient ink spilled for us to stop talking about it and instead start doing something about it? (Levin 1998, 43)                  "
"17","In stark contrast to the scarce attention given the NHST debate in political science, many psychologists have come to regard their discipline as having reached a saturation point, with little to be gained by further rumination over the troubling aspects of null hypothesis significance testing. The above quote comes from J. R. Levin's 1998 article entitled “What If There Were No More Bickering about Statistical Significance Tests?” a not so subtle jibe at the similarly titled book published the previous year (Harlow, Mulaik, and Steiger 1997). The criticisms of NHST had long been piling up, without much serious rebuttal; yet one could detect little change in the practice of scientific communication. Despite near unanimity of sentiment against the dominant practice by those who have spent time thinking and writing about it, no consensus emerged regarding a remedy. The very diversity of proposed options may well have contributed to sustaining the inertia, as it appeared safer (and easier) to just stick with the status quo. Given that one of the key criticisms of NHST is its rigidity and the perceived automaticity with which it is supposed to generate insights, it is unfortunate that the appropriate antidote (a menu of acceptable options to be utilized at the discretion of individual researchers) would fail to attract acceptance. As Cohen (1994 , 1001) warned at the top of his own list of suggestions: “First, don't look for a magic alternative to NHST, some other objective mechanical ritual to replace it. It doesn't exist.” This sentiment, as sensible as ever today, is not new; long ago, Rozeboom (1960, 428) was urging journal editors to “allow the researcher much more latitude in publishing … statistics in whichever form seems most insightful. In particular, the stranglehold that conventional null‐hypothesis significance testing has clamped on publication standards must be broken.” Rozeboom's insistence that the individual researcher be permitted to use his “own clinical judgment and methodological conscience [in making] a final appraisal” should be embraced as a guiding principle going forward. Certainly, we should resist the strong temptation to demand a one‐size‐fits‐all set of steps, no matter how apparently sensible any one approach may be in certain situations. When pressed, most scientists will agree with Rozeboom that the “aim of a scientific investigation” is not to reach a binary decision, per se, but instead to conduct “a cognitive evaluation of propositions.” In that case, we should emphasize the inferential rather than decision‐making model of science in the expository tools we choose.            "
"18","The best practices of those doing empirical political research today would render the entire debate somewhat irrelevant if more widely adopted; a number of political scientists regularly incorporate the sorts of approaches that NHST critics have long endorsed. Graphical representation of confidence intervals, accompanied by careful interpretations of estimated parameters in the range of plausible values, have become more common, though surprisingly not yet the norm in political science. Bayesian methods (Bakan 1966; Gill 1999), which largely avoid the problems of frequentist significance testing, are increasingly embraced by social scientists. Authors with a sophisticated grasp of statistical methods, and a deep understanding of what these tools can and cannot tell us, find ways to appropriately and imaginatively communicate their results to their audience. Whether or not their publications also happen to include p‐values or asterisks alongside estimates is then beside the point.            "
"19","One suggestion that deserves greater attention is that of Meehl (1978, 817), who urges a more sincere form of falsificationism than that reflected in contemporary caricatures of Popperian procedure, so that theories are genuinely subjected to “grave danger of refutation.” Excellent theoretical work on “severe testing” in the philosophy of science literature, most notably by Mayo and Spanos (2006), has connected this notion to lesser known work of Neyman and Pearson. As these authors point out, “a main task for statistical testing is to learn, not just whether H is false, but approximately how far from true H is with respect to parameters in question” (Mayo and Spanos 2006, 329). These authors also seek to move statistical reasoning in science away from a behavioristic (decision‐making) philosophy and toward an inferential one. Still, they privilege the testing over estimation mind‐set. My own approach, outlined below, reverses these priorities so that estimation and interpretation become the primary concern, with the joint “test” of practical and statistical significance serving as a diagnostic aid in this interpretation. The distinction is subtle: In the severity testing approach, while inferences are no longer dichotomous but rather a matter of degree, they still involve degree of support for a claim of existence (rather than of magnitude). Mayo and Spanos insist that confidence intervals fall within the same “error‐statistical paradigm” as testing and do not completely avoid comparable problems such as the arbitrariness of the chosen confidence level. However, this is most consequential if one requires a final determination rather than discussion of plausible values within a range of uncertainty. “Although CI's can be used … as surrogates for tests, the result is still too dichotomous to get around fallacies: it is still just a matter of whether a parameter value is inside the interval (in which case we accept it) or outside it (in which case we reject it)” (Mayo and Spanos 2006, 347). Here, the up‐down decision‐making perspective persists; indeed, when a confidence interval is considered from a pure testing perspective, it is equivalent to a corresponding test. However, as we shall see in the examples below, confidence intervals may also be considered on their own terms.            "
"20","When conducting hypothesis tests, what researchers typically have in mind is not a literal interpretation of the null hypothesis as a single point hypothesis, but rather that “the value of μ is close to some specified value μ0 against the alternative hypothesis that μ is not close to μ0” (DeGroot and Schervish 2002, 481). It makes more sense to replace the idealized simple hypothesis with “a more realistic composite null hypothesis, which specifies that μ lies in an explicit interval around the value μ0” (482). It is this suggestion that I take as the basis for synthesizing statistical and substantive significance within one procedure. Serlin and Lapsley (1985) suggest a similar approach, calling this interval a “good‐enough band” around the point null value.4"
"21","One may reasonably protest that such a procedure requires an arbitrary choice of length of the interval constituting a composite null set. DeGroot and Schervish (2002, 519) recommend a posterior probability plot for different values of the null interval diameter δ, but no such plot is available to the frequentist. It is nonetheless possible to conduct analyses of sensitivity to the choice of δ (as well as the choice of confidence level ), as a robustness check. In any case, explicit discussion of what difference or relationship would be meaningful is an essential, but frequently overlooked, task for the researcher. As DeGroot and Schervish (2002, 520) write, “forcing experimenters to think about what counts as a meaningful difference is a good idea. Testing the (simple) hypothesis … at a fixed level, such as 0.05, does not require anyone to think about what counts as a meaningful difference.” This observation cuts straight to the heart of why the conventional NHST approach encourages bad habits. Indeed, demanding that political scientists articulate and even debate what would constitute a meaningful effect in context of their particular research problems, rather than encouraging arbitrary cut points, puts the emphasis back on experts' subject‐area knowledge; political scientists should welcome the opportunity rather than shrink from it.            "
"22","Given parameters of substantive interest, be they real‐world quantities (e.g., difference between mean incomes for two subpopulations) or quantities whose meaning is derived only within a proposed model (e.g., a Poisson regression coefficient), a researcher wishing to simultaneously test for statistical and substantive significance should begin by declaring a set of parameter values to be taken as effectively null. This should be based, when feasible, upon context and defended by the author. Such a choice should emerge from reflection on the following question: If one could know precisely the “true” value of a parameter, what values would seem inconsequential and which would seem worthy of note? The resulting null set would include any value that seems practically indistinguishable from the (sharp) null value, or effectively null. The very process of thinking this through and the resulting conversation with others would itself be a healthy development. Indeed, while the precise distinction between what values are effectively null and which ones are of interest may be somewhat arbitrary, thoughtful consideration should in most cases reveal a range of values that all knowledgeable individuals would take to be effectively null and a range of values that anyone would consider noteworthy. In the next section, I will illustrate how one may go about proposing such a null set, as well as the usefulness of conducting a simple sensitivity analysis in order to indicate how robust one's results are to both this partition of the parameter space and the chosen level of confidence/statistical significance.            "
"23","The effective null set may be used as a heuristic for the reader who wishes to get a quick sense of what the authors purport to be of value in their results. Suppose, for example, we wish to know whether two groups of laborers, comparable other than with respect to gender, earn the same hourly wage, on average. We are unlikely to care if the true difference is only a few cents, even if this difference were known with absolute certainty. Suppose we declare the effective null set to be , so that any discrepancy of 25 cents or less is considered inconsequential or insufficiently notable to merit intervention. Then once a confidence interval is constructed from the data (at the preferred level, say 95%), simple qualitative distinctions may be drawn:               "
"24","This represents an oversimplification of results, but at least an oversimplification that points in the direction of what the reader should care about. If the confidence interval lies mostly within the null set, we might say the evidence “leans against” a meaningful difference; if barely overlapping Θ0, we might say the difference is “likely” meaningful. Note that this notion of what is to be taken as meaningful incorporates both statistical and substantive significance. Having an agreed‐upon shorthand that may draw the casual reader to closer inspection could be helpful. In the PASS‐test presentation, the simplification addresses what is of scientific interest (the magnitude of a parameter) while simultaneously providing evidence as to whether we can have some confidence that a seemingly meaningful result is not a phantom. Not inconsequentially, the “test” requires presentation of confidence intervals and promotes careful thinking about the set of plausible parameters. In this way, it is a test that actually weans us from excessive dependence on the testing paradigm itself.            "
"25","Two especially troubling aspects of NHST are addressed by the combined practical and statistical significance testing framework outlined below. First, by never requiring a point null hypothesis to compete with a composite (interval) research hypothesis, one abandons the aforementioned practice of using H0 as a straw man that is known to be false before data are even examined. Instead, it will be at least hypothetically possible to legitimately find support for the null hypothesis. As n gets large, the width of the resulting confidence interval will shrink until it lies entirely in either the effective null interval or the alternative. Furthermore, the often disingenuous proposal of one‐sided hypotheses, of the form H, may be replaced by the consideration of genuinely commensurable parameter sets: H. The use of one‐tailed tests has itself long been the subject of controversy (see, e.g., Eysenck 1960), in part because of the suspicion that they are employed more out of a desire to compensate for poor power to reject the null than out of any theoretical motivation. In principle, the frequentist test of point null versus one‐sided interval acts as a proxy for the two‐interval test we really have in mind; the point null is chosen to have the sampling distribution most likely to generate a test statistic in Θ1, the set of parameters consistent with the research hypothesis H1. Bayesian analysis offers the most natural way to allow competition between two interval hypotheses, as one may simply integrate the posterior distribution over each subset of the parameter set and compare directly or consider the corresponding odds ratio. In lieu of the Bayesian alternative, inspection of the confidence interval relative to the partitioned parameter space, with the border between the two sets chosen so that only nontrivial values lie in Θ1, would seem to be more informative than the usual implementation.            "
"26","In Figure 1, I plot confidence intervals for hypothetical results one might obtain in addressing the wage difference question. Since the results are imagined, let's suppose for the sake of concreteness, that they are 95% confidence intervals (one could also superimpose two or three confidence intervals with different α's). I have indicated with dashed segments the thresholds of my effective null set and $0 as the corresponding sharp null from a conventional analysis. For each interval, consider how results would be interpreted under the proposed PASS‐test in contrast to how the corresponding NHST would be interpreted (Table 1). In neither case should the researcher be satisfied with simply reporting simple direction and significance. Such a simplified summary of results is useful, but it cannot replace a discussion that considers the range of plausible values falling in a confidence interval and interpreting their meaning.            "
"27","Stated precisely, in algorithmic form, a combined practical and statistical significance test includes the following steps, assuming a continuous parameter space for a parameter reflecting some relationship of interest:"
"28","Practical and Statistical Significance Test (PASS‐Test)                              "
"29","               "
"30","A major problem involved in adjudicating the scientific significance of differences is that we often deal with units of measurement we do not know how to interpret. (Carver 1978, 389)                  "
"31","I next illustrate how the recommended approach may serve as one way to enrich the presentation and interpretation of results. In certain instances, the PASS framework for considering results may strengthen authors' arguments; in other cases, it makes more obvious the tentativeness with which the results should be viewed."
"32","In their article exploring the three most widely studied types of media effects (agenda setting, priming, and framing) in the lead‐up to the Persian Gulf War of 1990–91, Iyengar and Simon (1993) consider whether exposure to television news may predict support for a military response to Iraq's invasion of Kuwait and the subsequent crisis. Having studied eight months of prime‐time newscasts during the relevant time interval, they note the predominant use of episodic over thematic framing; based on extant theory and previous research, they suspect that this will lead to viewers' attribution of responsibility to particular individuals and groups rather than broader historical, societal, or structural causes, and they anticipate that this will translate into support for the use of military force against Saddam Hussein rather than diplomatic strategies by those who consume such media. They regress a variable measuring respondent support for a military over diplomatic response on several predictors, including presumed indicators of news exposure. The measurements are taken from the 1991 ANES Pilot Study (Miller et al. 1999).            "
"33","According to the logic of combined practical and statistical significance testing, it is essential that one consider what magnitudes of coefficients would be impressive if one were able to observe the parameter values themselves without sampling error. Iyengar and Simon (1993) are primarily concerned with the expected effect on military support corresponding to variation in the values of TV News Exposure and Information, measures of, respectively, television news consumption and awareness of political information via identification of political figures in the news. Conditioning on party, gender, race, education, and general support of defense spending, what sort of coefficients should we view as effectively zero, and, conversely, what values would indicate at least a somewhat meaningful relationship? This sort of question, as previously noted, is too often left unasked. To the extent that it does arise, it is almost always handled completely informally. To their credit, the authors here distinguish between the two types of significance: “Overall, then, there were statistically significant traces of the expected relationship. Exposure to episodic news programming strengthened, albeit modestly, support for a military resolution of the crisis” (Iyengar and Simon 1993). From a PASS‐test—rather than NHST—perspective, the assessment of the degree to which this type of programming corresponds to greater military support is of principal concern; the claim of a “modest” relationship should therefore be more carefully explained and supported.            "
"34","The Iyengar–Simon model may be written in the following manner:               "
"35","with departures from conditional expectation assumed distributed . The predictors of primary interest are , the number of self‐reported days per week watching TV news, and  (called  in the original article), the respondent's score from 0 to 7 on a quiz of recognition of political figures, taken as another proxy for news consumption. In Iyengar and Simon's (1993) model, the contribution of —but not —is allowed to vary by race and gender (through the inclusion of interaction effects), so that, in addition to the coefficient on , one should wish to learn whether the following parameters are of a meaningful magnitude:               "
"36","Within each demographic category, the associated composite parameter is interpretable as the difference in expected level of support for a military solution associated with an additional point on the political knowledge quiz. Thus, for example, if , this means that one might expect an extra correct answer on the quiz taken by a white female to correspond to an additional quarter point on the intervention support scale ranging from 1 to 4, assuming that the ordinal scale of support for diplomacy versus military action can be sensibly interpreted as if it were an interval‐level measurement. A large difference of 4 points on the quiz (e.g., correctly identifying six rather than two political figures, or four rather than zero) would be expected to translate into a full extra unit in the support for a militaristic solution on the scale ranging from 0 to 4.5 Understanding this allows the researcher to set up reasonable expectations of what might be considered a truly meaningful “effect”6 and then evaluate whether the data support such a finding in light of sampling error.            "
"37","Following the steps outlined above, one would begin by declaring a reasonable null set. Here is an opportunity for the researcher to utilize his or her applied knowledge. Two options for handling this are as follows:               "
"38","The table of results found in the original article closely matches the results from my own replication,7 shown here in Table 2. This table reflects typical NHST‐style presentation and thus is not, on its own, especially informative, and so the p‐values attract the bulk of the reader's attention. The authors discuss these results somewhat vaguely:               "
"39","Partisanship, race, gender, and education—all affected respondents' policy preferences concerning resolution of the conflict. Republicans, males, those with more education, and Whites tended to support the military option. Support for increased defense spending was strongly associated with a more militaristic outlook toward the conflict. Both indicators of exposure to television news exerted significant effects—more informed respondents and respondents who watched the news more frequently were most apt to favor a military resolution. The effects of information were markedly stronger among women and minorities.…(Iyengar and Simon 1993, 380, emphasis added)                  "
"40","Using the benchmark approach discussed above, consider a possible PASS analysis based on the estimates of coefficients and standard errors.8 The 95% confidence interval for support of defense spending is (0.15, 0.23), but as a simple point of comparison, consider an estimate of around 0.20. The variable measures response to whether the nation should decrease or increase defense spending on a scale ranging from greatly decrease (1) to greatly increase (7). The median, and by far the modal, response is support for the status quo; 43% of respondents preferred neither an increase nor a decrease in defense spending.  and , so the interquartile range (IQR) = 1. There is little variation in responses to this question, so that even a single unit (level) difference is relatively large and provides a key benchmark for comparison with predictors of interest. One would not expect that a more subtle predictor such as TV news watching would possibly match the predictive power of the respondent's general predisposition toward a muscular defense, but it would be tough to argue that a magnitude absolutely dwarfed by this benchmark variable would still be noteworthy. It should not be controversial to consider as effectively zero anything less than a 0.01 expected difference in response level per predictor IQR. Recall that the index of support for a military strategy in Iraq ranges from 1 to 4, so in absolute terms, this means that anything less than one‐hundredth of a full‐level jump in response per large change in predictor value, conditioning on the other explanatory variables, is to be considered negligible. In relative terms, no less than one‐twentieth of the per‐IQR explanatory power of Defense, or around two‐fifths that of  party identification, will be deemed meaningful.            "
"41","Consider what is communicated in Figure 2, as opposed to the NHST‐based Table 2. Aside from Defense, the baseline used to anchor our judgment of effect size, where can we distinguish meaningful associations? The authors assert a “markedly stronger” effect of information on minorities and women, but is this so? Looking at the results, no such relationship is detected for nonwhite men. Information makes a meaningful difference specifically for some women, it would seem. For nonwhite women, in particular, greater information exposure clearly corresponds to higher expected level of support for military intervention, all else equal. For white women, the relationship likely exists as well; the per‐IQR point estimate is comparable to that for party identification, Republican, and Education, with a 95% confidence interval entirely outside the null interval. In both cases, the association may actually be quite large; a sample with more women would increase the estimate precision and allow us to find out. Controlling for the measure of political knowledge, as well as the other variables, exposure to television news is not predictive of the dependent variable at a level distinguishable from the identified set of null values. In fact, a two‐way table reveals no discernible marginal relationship between self‐reported TV news exposure and hawkishness with regard to the Gulf crisis.9 With little evidence of a relationship even before conditioning on covariates and relying on linearity assumptions, it is not surprising no more than a tiny association is detectable according to the regression results presented in Figure 2.            "
"42","As others have pointed out, confidence intervals rely upon a seemingly arbitrary choice of confidence level; similarly, the width of the effective null appears subjective, if not exactly arbitrary. Thus, one should indicate how sensitive one's claims are to the choice of these two quantities. In Figure 3, three confidence levels are displayed, and a more conservative [ − .03, .03] effective null set is superimposed upon the initially proposed [ − .01, .01] set. An examination reveals how robust the results are to these two choices. For example, using the wider null set, any reasonable confidence interval for TV news exposure indicates a substantively insignificant relationship with the response variable. For white males, the null hypothesis of no meaningful relationship between political knowledge militarism is accepted at the 67% confidence level. On the other hand, this null hypothesis is rejected in the case of nonwhite females at both 67% and 95%, and white females at 67%. Sticking with the original narrow null set, the PASS‐test for knowledge versus militarism, controlling for the rest, is inconclusive for all four demographics at the more rigorous 99% level.            "
"43","Finally, in considering the presentation of results in Figures 2 and 3, some may worry that the analysis relies too heavily on verbal description. I happen to find verbal interpretation a strength, rather than a weakness, but note that all useful information contained in conventional tabular presentation is contained in the figures as well. The explanations superimposed on Figure 2 are for pedagogical purposes only and need not be an expected feature of such a graph. I have found there to be clear consensus that it is poor practice to simply instruct the reader to “see results in the table provided,” without offering the author's own interpretation. This is true whether one summarizes results in a conventional table or in a figure depicting confidence intervals or posterior distributions. It is incumbent upon the author to refer to tables, figures, and other quantitative and qualitative evidence in making his or her argument. It is then up to the engaged reader to consider the evidence presented, alongside previous work, and determine whether the author's particular interpretations are convincing. There is no denying that interpretation of graphs involves subjectivity. The avoidance of this subjectivity can only be bought through the acceptance of automaticity, taking us again down a path that has not served social science well. Our instinct to seek a one‐size‐fits‐all criterion for practical significance is understandable, but it is even less tenable than such a standard for statistical significance.            "
"44","One might suspect that this subjectivity will be an invitation to choose one's null set solely to obtain the illusion of significance, but this is already the case with the status quo, as we have seen. Similarly, the potential for meta‐analysis of multiple related studies may seem to be hindered by the lack of uniformity among investigations that employ different null sets, but of course, studies purporting to examine the same phenomena have always differed in all sorts of ways. In fact, while variability in the choice of measure, model specification—and now the effective null set—requires extra time and care on the part of the meta‐analyst, this heterogeneity of study detail offers its own advantage: Apparent consensus will not be attributable a particular arbitrary choice, repeated reflexively by each subsequent researcher."
"45","Overreliance on arbitrary but agreed‐upon thresholds simply leads to consensual mass manipulation, in which passing a magical threshold ends rather than begins the conversation. Just as there is no formula that generates theories for us, there is no statistical technique that can produce objective standards of what facts should be scientifically impressive. The job of convincing a critical audience that a certain state of the world would be interesting, surprising, or even shocking—if true—is entirely our own, whether done so informally, by suggesting an effective null set, or by continuing to pass off statistical distinguishability alone as sufficient. This essential task of the social scientist should be embraced; to do otherwise is an abrogation of our responsibility as scholars and an underestimation of the capacity of our audience. Rather than supposing that a particular effective null set is somehow acceptable to all, it would be preferable to use sensitivity analysis to investigate the robustness of one's results to other defensible null sets (and confidence levels), as I have done briefly in this illustration. If readers find our chosen null set to be suspiciously convenient, or our consideration of alternative null values overly restrictive, they should challenge us and debate the point."
"46","The criticisms of null hypothesis significance testing as commonly put into practice are widely recognized and not especially controversial among statisticians. Despite having been aired in numerous forums over the decades—at least outside of political science—not much has changed in terms of standard practice. Bayesian approaches to hypothesis testing, focusing attention directly on the relative probabilities of competing hypotheses in light of data, do not suffer the principal failings of the NHST, and so the growing acceptance of the Bayesian toolkit has been a source of improvement on this front. Within a frequentist framework, however, debates over the merits of significance testing have had little impact on practice. It is difficult to change habits while the status quo is rewarded and no consensus nor even clear guidance on a preferable alternative emerges. In employing non‐Bayesian forms of analysis, it would be wise to develop a habit of summarizing the results of parametric empirical analyses in a manner that emphasizes statistical distinguishability from a practically inconsequential set of parameter values. The expectation that scholars consider magnitudes of parameters of interest, and not be content to simply distinguish signal from noise, requires little additional work and no special training, but guides us in the direction of meaningful discussion and away from the seductive lure of empty formalism. Additionally, to convincingly argue about what results should be deemed significant in practical terms provides incentive for creative intertwining of qualitative with quantitative knowledge of subject matter.         "
"47","In addition to encouraging fellow political scientists to join our colleagues throughout the social sciences in reflecting upon potential improvements to common implementation of NHST, I would be gratified to see others give thought specifically to how we might choose sets of effectively null parameter values when working with more complicated models. It remains to be seen whether the PASS‐test might find fruitful application in fitting, say, generalized linear models, exponential random graph models, and others from among various families of sophisticated parametric models that have been proposed for use in various applications. And yet, even the attempt to bring the question of practical significance into our methodological decision‐making process would be a worthwhile exercise. Social scientists may recall a time, as logistic regression was just beginning to gain popularity in our disciplines, when it was common to simply avoid any interpretation of parameter estimates whatsoever, usually with some sort of disclaimer that the model coefficients were not easily interpretable. Fortunately, this is no longer typically tolerated; we may disagree over whether it is more meaningful to speak in terms of log odds or interpret results on the odds scale, or whether we should always present predicted probabilities (evaluated at the mean of covariates or using some other convention), but it has become less common to eschew any attempt at interpretation whatsoever. The simple process of grappling honestly with the question of what parameter values would seem noteworthy for a given model substantially improves the quality of subsequent analyses. Furthermore, should we discover that we have absolutely no idea how to interpret the basic components of our model to the extent that we can say—even as a purely subjective judgment—what parameter values would be substantively impressive to us, then perhaps we should ask ourselves whether we might not be better served by developing an alternate approach.         "
"48","Whether one chooses to take the particular approach I have suggested here is far less consequential than a commitment to faithfully represent what matters from a theoretically informed, substantively oriented perspective. In fact, the simple approach that I have called the PASS‐test is but a formalization of certain principles of sound statistical reasoning, which are no doubt second nature to many seasoned scientific professionals. If formally stated results of hypothesis tests continue to be the norm in scientific journals, it would restore some amount of balance to insist that scientific or practical significance be given at least equal attention in summaries of results such as those typically provided in tables and figures. Here I have outlined the bare essentials of what this sort of presentation might look like. Since presenting early versions of this work, I have encountered two sets of writings that address related themes within biostatistics and medical diagnostics literatures; these literatures, largely unknown among social scientists, offer a more extensive framework for just this sort of balanced approach. Two excellent books outline the process of generating and evaluating “informative hypotheses” (Hoijtink 2011; Hoijtink, Klugkist, and Boelen 2008) within a Bayesian perspective. Also especially relevant is work on minimal clinically important difference. Key articles include Jaeschke, Singer, and Guyatt (1989), Wells et al. (2001), Copay et al. (2007), and Revicki et al. (2008). The consequences of mistaking statistical significance for practical significance surely have higher stakes in such matters as pain reduction or medical risk assessment, so it is not surprising to see such areas begin to embrace this sort of approach. As political scientists, the immediate consequences of research may be less tangible, but if we believe that the fruits of our labor are nonetheless meaningful, we would do well to follow suit.         "
