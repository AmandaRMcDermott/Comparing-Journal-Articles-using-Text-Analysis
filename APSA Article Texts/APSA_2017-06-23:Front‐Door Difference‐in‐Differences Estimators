"","x"
"1","In this article, we develop front‐door difference‐in‐differences estimators as an extension of front‐door estimators (Pearl 1995). This extension removes bias when there is a violation of the key front‐door assumption. In this sense, front‐door difference‐in‐differences estimators are analogous to standard difference‐in‐differences estimators (Abadie 2005). Importantly, under a certain set of assumptions detailed below, front‐door difference‐in‐differences estimators allow credible causal inference without the use of traditional control units. As we demonstrate with assessments of a job training program and an early in‐person voting program, the ability to make inferences in the absence of comparable control units provides an important research strategy.         "
"2","The front‐door criterion (Pearl 1995) and its extensions (Kuroki and Miyakawa 1999; Shpitser and Pearl 2006; Spirtes, Glymour, and Scheines 2000; Tian and Pearl 2002a; Tian and Shpitser 2010) provide a means for nonparametric identification of treatment effects using posttreatment variables. Importantly, the front‐door approach can identify causal effects even when there are unmeasured common causes of the treatment and the outcome. Figure 1 presents the directed acyclic graph associated with the front‐door criterion. The formal definition of this graph can be found in Pearl (1995, 2009), but for our purposes, it will suffice to note the following: A represents the treatment/action variable, M represents a set of mediating variables (often a singleton), Y represents the outcome, X represents covariates, U and V represent sets of unobserved variables, and arrows represent the possible existence of effects from one set of variables to another.1 Solid arrows are allowed for the front‐door criterion to hold; dashed arrows are not allowed for the front‐door criterion to hold. Note the existence of solid arrows from U to both A and Y. Hence, unmeasured common causes of the treatment and outcome are allowed. As seen in Figure 1, the front‐door approach works by identifying the effects of A on M and the effects of M on Y, and then putting them back together into the effect of interest: A on Y.         "
"3","Front‐Door Directed Acyclic Graph"
"4","Note: A represents the treatment/action variable, M represents a set of mediating variables, Y represents the outcome, X represents covariates, and U and V represent sets of unobserved variables. To simplify presentation, we have assumed that X, U, and V are independent (this is implied by the lack of arrows between them), but this is not required. Solid arrows are allowed for the front‐door criterion to hold within this group. Dashed arrows are not allowed for the front‐door criterion to hold in this group.                     "
"5","However, the front‐door adjustment has been used infrequently (VanderWeele 2009) due to concerns that the assumptions required for point identification will rarely hold (Cox and Wermuth 1995; Imbens and Rubin 1995). These assumptions are represented by the dashed arrows in Figure 1. Hence, whereas common causes of A and Y are allowed for the front‐door criterion to hold, common causes of M and Y (not mediated by A) are not allowed. Additionally, the front‐door criterion will not hold when A has a direct effect on Y.         "
"6","A standard difference‐in‐differences (DD) estimator uses observations for which there should be no effect (often pretreatment observations for treatment and control units) to estimate and remove the bias from a selection on observables approach (often posttreatment observations for treatment and control observations). The front‐door difference‐in‐differences (front‐door DD) approach developed in this article works in a similar manner to DD, with two major differences. First, the goal of front‐door DD is to remove bias due to common causes of M and Y. Therefore, the differencing is done with respect to the effect of the mediator, and then the estimated “mediator effect” is scaled to estimate the effect of the treatment. Second, front‐door DD with pretreatment observations is only possible when mediator information is available from the pretreatment period. This information may not be available in repeated cross‐section designs.         "
"7","With these differences in mind, the front‐door DD proceeds analogously to the DD approach. In the over‐time version, the estimated effect of the mediator M in the pretreatment period is subtracted from the estimated effect of the mediator M in the posttreatment period (after adjusting for covariates). This corrected mediator effect is then scaled according to the estimated effect of A on M, so the effect of A on Y can be estimated. In non‐over‐time examples, we may find units of observation for which we know there can be no effect. These are analogous to the pretreatment observations from an over‐time approach. Generally, we refer to such observations, for which there can be no effect, as the differencing group and observations for which there can be an effect (e.g., posttreatment observations) as the group of interest. For an over‐time example, we consider a job training program with the preprogram observations on individuals as the differencing group. In a non‐over‐time example, we estimate the effects of an early in‐person (EIP) voting program on turnout for elections by leveraging voters who used an absentee ballot in the previous election as a differencing group.2"
"8","As with DD estimation, front‐door DD estimation will only provide credible estimates when the assumptions hold. Outside of perhaps over‐time front‐door DD analysis, it will often be hard to make this case. However, we demonstrate in the applications that credible bounds can sometimes be estimated when the assumptions do not hold. In particular, we demonstrate that front‐door and front‐door DD estimates can be used in a bracketing approach under certain circumstances (although great care must be taken that such circumstances hold)."
"9","In this section, we simplify and restate some of the results from Glynn and Kashin (2013) on large‐sample bias formulas for the front‐door approach to estimating the average treatment effect on the treated (ATT). Throughout this article, all references to bias will mean large‐sample bias in the context of nonparametric estimation. This allows us to avoid questions of modeling.         "
"10","ATT is often the parameter of interest when assessing the effects of a program or law. For an outcome variable Y and a binary treatment/action A, we define the potential outcome under active treatment as  and the potential outcome under control as .3 Our parameter of interest is the ATT, defined as . We assume consistency, , so that the mean potential outcome under active treatment for the treated units is equal to the observed outcome for the treated units such that . The ATT is therefore the difference between the mean outcome for the treated units and mean counterfactual outcome for these units, had they not received the treatment.         "
"11","We also assume that  is potentially identifiable (perhaps with additional data) by conditioning on a set of observed covariates X and unobserved covariates U. To clarify, we assume that if the unobserved covariates were actually observed, the ATT could potentially be estimated by standard approaches (e.g., matching). For simplicity in presentation, we assume that X and U are discrete, such that            "
"12","The front‐door adjustment for a set of measured posttreatment variables M can be written as the following:            "
"13","For the difference‐in‐differences estimators we consider in this article, we use the special case of nonrandomized program evaluations with one‐sided noncompliance. Following the literature in econometrics (Heckman, LaLonde, and Smith 1999) on program evaluation, we define the program impact as the ATT where the active treatment (a1) is assignment into a program, and when  indicates whether active treatment was actually received and  if it was not.            "
"14","Assumption 1. ([One‐Sided Noncompliance]) for all .               "
"15","Assumption 1 implies that only those assigned to treatment can receive treatment.4 The front‐door large‐sample estimator can be rewritten in the following manner.            "
"16","Roughly speaking, standard covariate adjustment matches units that were assigned treatment to similar units that were assigned control. On the other hand, front‐door estimates match units that were assigned treatment to similar units that were assigned treatment but did not receive treatment. This sort of comparison is not typical, so it is helpful to consider the informal logic of the procedure before presenting the formal statements of bias. The fundamental question is whether the treated noncompliers provide reasonable proxies for the missing counterfactuals: the outcomes that would have occurred if the treated units had not been assigned treatment. Therefore, in order for the front‐door approach to be unbiased in large samples, one must effectively assume that (1) assignment to treatment has no effect if treatment is not received and (2) after conditioning, those who are assigned but do not receive treatment are comparable in some sense to those who receive treatment.5 This will be made more precise below.         "
"17","The front‐door formula in Equation 2, with the proportions  and  multiplying the estimated effect of receiving the treatment, is helpful when considering the simplified front‐door ATT bias, which can be written as the following (see Sections 1.2 and 1.3 of the SI for proofs):            "
"18","The unobservable portion of this bias formula (everything after the ) can be difficult to interpret, but there are a number of assumptions that allow us to simplify the formula. For example, we might assume that treatment does not have an effect on the outcome for noncompliers:            "
"19","Assumption 2. ([Exclusion Restriction])No direct effect for noncompliers: .               "
"20","When combined with the consistency assumption, Assumption 2 can also be written as . If this exclusion restriction holds, then the bias simplifies:            "
"21","Assumption 3. ([Constant compliance rates across values of u within levels of ]) for all x and u,               "
"22","then due to the binary measure of treatment received,  (see Section 1.4 of the SI), and the bias simplifies:            "
"23","Assumption 4. ([ Is Constant among Treated Units within Levels of ])For any two units with a1 and covariate values  and , .               "
"24","In some applications, the bias  may be small enough for the front‐door estimator to provide a viable approach; for others, we may want to remove the bias. In the next section, we discuss a difference‐in‐differences approach to removing the bias.         "
"25","If we define the front‐door estimator within levels of a covariate x as , then the front‐door estimator can be written as a weighted average of strata‐specific front‐door estimators where the weights are relative strata sizes for treated units:            "
"26","In order for the front‐door DD estimator to remove the large‐sample bias from the front‐door estimator of the ATT for the group of interest, we need the following assumption to hold (where we denote bias within levels of x for the interest group g1 as ):            "
"27","Assumption 5. ([Bias for  Equal to Scaled Front‐Door Formula for  within Levels of ]) for all x.               "
"28","There are two things to note about Assumption 5. First, when using an over‐time approach, the compliance rates of the two groups will be equal, , because time does not alter an individual's definition as a complier. Hence, Assumption 5 simplifies to  for all x in the over‐time case. Second, Assumption 5 can often be weakened if only a bound is needed. For example, if the estimated effect for the differencing group is positive, and we believe the front‐door bias for the group of interest is also positive, but smaller than the scaled estimated effect for the differencing group, then subtracting the scaled estimated effect from the differencing group will remove too much from the estimated effect in the group of interest. Hence, the front‐door difference‐in‐differences approach will produce a lower bound.         "
"29","Therefore, if we believe that the front‐door estimator and front‐door DD estimator have bias of different signs, then these can be used in a bracketing approach. For example, if we believe the bias in the front‐door estimator is positive prior to the differencing, and we believe the bias of the front‐door DD estimator is negative, then the front‐door and front‐door DD estimator can be used together to bracket the truth in large samples. This will be discussed in the context of the illustrative applications in the following sections."
"30","If Assumptions 1 and 5 hold, then  has no large‐sample bias for  (see SI Section 2.1 for proof). However, the interpretation of Assumption 5 will often be simplified when Assumptions 2, 3, or 4 hold. This will be discussed in the context of the applications, but one special case is useful to consider. When Assumptions 1 through 4 hold, then Assumption 5 is equivalent to the following:            "
"31","We now illustrate how front‐door and front‐door DD estimates for the average treatment effect on the treated (ATT) can be used to estimate and bracket the experimental truth in the context of the National Job Training Partnership Act (JTPA) Study, a job training evaluation with both experimental data and a nonexperimental comparison group (see SI Section 3 for details). We measure program impact as the ATT on 18‐month earnings in the post‐randomization or post‐eligibility period, where active treatment is assignment into the program (perhaps self‐selected assignment). We focus on the effect of sign‐up on earnings for three reasons: (1) We can compare front‐door estimates to the experimental benchmark, (2) this effect is the same parameter of interest as in much of the econometrics literature utilizing JTPA data (Heckman, Ichimura, and Todd 1997; Heckman and Smith 1999), and (3) this is often the policy‐relevant causal effect when considering whether or not to extend the opportunity for job training. Furthermore, Heckman et al. (1998, 1032) showed that for the National JTPA Study, matching adjustments using the nonexperimental comparison group can come close to the experimental estimates only when one has “detailed retrospective questions on labor force participation, job spells, [and] earnings.” In the following, we discuss the use of front‐door DD estimators to provide similar information in the absence of detailed labor force histories, and in fact, in the absence of any individuals who did not sign up for the program.         "
"32","As discussed below, the simple front‐door estimator is anticipated to exhibit positive bias when estimating ATT for the JTPA program for adult males. In the following subsections, we consider two front‐door DD approaches to correcting this bias. First, we consider using an over‐time approach to remove positive bias from the front‐door estimator. Second, we consider the more conservative approach of using single adult males as a differencing group, which allows us to provide a lower bound on the effect of the program for married adult males (due to the job training effect being smaller for single males; Korenman and Neumark 1991). Because the front‐door estimator provides an upper bound, these two estimators can be used in a bracketing approach.         "
"33","The most simple front‐door estimator for the effects of the JTPA program takes the mean 18‐month earnings of those who both signed up for the program and showed up for their training and subtracts the mean 18‐month earnings of those who signed up for the program but failed to show up for their training,6 and then scales this estimate by the rate at which those who signed up actually showed up. Because we have not yet used covariates, this estimator can be written as a simplified version of Equation 2:               "
"34","In an attempt to remove the anticipated positive bias, we can use the baseline earnings of these individuals as a differencing group. The simplest version of this estimator does the following: (a) takes the mean 18‐month earnings of those who both signed up for the program and showed up for their training and subtracts the mean 18‐month earnings of those who signed up for the program but failed to show up for their training, (b) takes the mean baseline (0‐month) earnings of those who both signed up for the program and showed up for their training and subtracts the mean baseline earnings of those who signed up for the program but failed to show up for their training, (c) takes the difference between these two estimates, and (d) scales this difference by the proportion that showed up among those who signed up. As above, because we have not used covariates at this point, this estimator can be written as a simplified version of Equation 6:               "
"35","The front‐door and front‐door DD estimates for the effect of the JTPA program on adult males are presented in Figure 2. The experimental benchmark (solid horizontal line) is the only estimate that uses the experimental control units. While the front‐door estimator appears to exhibit some of the anticipated positive bias, the estimate lies within the 95% confidence interval from the experiment. The front‐door DD estimator gets a bit closer to the experimental benchmark, and its 95% interval more clearly covers the benchmark.            "
"36","Comparison of Front‐Door and Over‐Time Front‐Door DD Estimates for the JTPA Effect for Adult Males"
"37","Note: The solid line is the experimental benchmark, and the dashed lines represent the confidence interval. All intervals are 95% bootstrapped confidence intervals based on 10,000 replications.                        "
"38","Although the improvement from the front‐door DD estimate is minimal here, this may be due to the relatively good quality of the front‐door estimate. If we did not see the experimental results (as would be true for nonillustrative applications), the similarity between the front‐door and front‐door DD estimates would give us some confidence as to the robustness of the findings (and this confidence would not be misplaced for this example). However, if even after seeing these results we prefer a more conservative estimate of the effect of sign‐up, we can define a different differencing group using the observed covariates."
"39","If we did not have the experimental benchmark, we might not be confident that the bias in the pretreatment period is equal to the bias in the posttreatment period, and hence we may want to use an additional differencing group as a robustness strategy. In this subsection, we discuss the use of never‐married men (henceforth referred to simply as single men) as the differencing group and currently or once‐married adult men as a the group of interest (henceforth referred to simply as married men)."
"40","The use of a differencing group that is a subset of the individuals (single men as a subset of adult men) produces an additional complication: We must consider whether the effect of interest is the average effect of the program for all individuals or just the average over the individuals in the group of interest. Fortunately, conversion between the two effects is straightforward due to the assumption that the effect of the program is zero for the differencing group. Specifically, the average effect over all individuals is the average effect for the group of interest multiplied by the proportion of individuals in the group of interest. In order to simplify the presentation and because this conversion is straightforward, we continue this section focusing on the effect for the group of interest. All of the following results are substantively replicated when we convert to the analysis for all individuals."
"41","With single men as the differencing group, we include baseline earnings as a covariate, which rules out the use of the simplified versions of Equations 2 and 6 from the previous subsection. However, the use of covariates in the analysis also allows us to compare the performance of the front‐door and front‐door DD estimators to standard covariate adjustments like regression and matching.            "
"42","The front‐door and front‐door DD estimates for the effect of the JTPA program on married males are presented in Figure 3 across a range of covariate sets. Additionally, we present the standard covariate adjusted estimates for comparison. We use ordinary least squares (OLS) separately within experimental treated and observational control groups (the eligible nonparticipants, or ENPs) for the standard estimates. For front‐door estimates, we use OLS separately within the “experimental treated and received treatment” and “experimental treated and did not receive treatment” groups. Therefore, these estimates assume linearity and additivity within these comparison groups when conditioning on covariates, albeit we obtain similar results when using more flexible methods that relax these parametric assumptions. The experimental benchmarks (solid horizontal line segments) are the only estimates that use experimental control units.            "
"43","Comparison of Standard Covariate Adjusted Estimates, Front‐Door, and Front‐Door DD Estimates for the JTPA Effect for Married Adult Males"
"44","Note: Solid lines represent the experimental benchmark. The 95% bootstrapped confidence intervals are based on 10,000 replications.                        "
"45","First, note that the front‐door estimates exhibit uniformly less estimation error than estimates from standard covariate adjustments across all conditioning sets in Figure 3. The error in the standard estimates for the null conditioning set and conditioning sets that are combinations of age, race, and site is negative. The error becomes positive when we include baseline earnings in the conditioning set. In sharp contrast, the stability of front‐door estimates is remarkable. Thus, front‐door estimates are preferable to standard covariate adjustment when more detailed information on labor force participation and historic earnings is not available.            "
"46","In spite of the superior performance of front‐door estimates compared to standard covariate adjustment, the front‐door estimates are slightly above the experimental benchmark across all covariate sets. As mentioned above, without seeing the experimental benchmark, we might believe these estimates are affected by positive bias because those who fail to show up to the job training program are likely to be less diligent individuals than those who show up. Given the anticipated positive bias in the front‐door estimates, we use the front‐door DD estimator to either recover an unbiased point estimate or obtain a lower bound, depending on our assumptions as to the effect of the program in the differencing group. If we believe that the JTPA program had no effect for single males, and that Assumptions 1 and 5 hold, then the difference‐in‐differences estimator will return an unbiased estimate of the effect for the group of interest in large samples. If, on the other hand, we believe there might be a nonnegative effect for single males, then we would obtain a lower bound for the effect for the group of interest. In this application, it is more likely that there was a positive effect of the JTPA program for single males, albeit one smaller than for married males (Korenman and Neumark 1991). Hence, the front‐door DD estimator will likely give us a lower bound for the effect of the JTPA program for married males. In fact, in many applications, we may be unable to find a differencing group with no effect, yet still be able to use front‐door and front‐door DD approaches to bound the causal effect of interest given our beliefs about the sign and relative scale of effects in the group of interest and the differencing group. Figure 3 shows exactly this result. The front‐door DD estimator forms a lower bound for across all conditioning sets, and the front‐door estimator provides an upper bound.            "
"47","In this section, we present front‐door DD estimates for the average treatment effect of an early in‐person voting program in Florida. We want to evaluate the impact that the presence of early voting had on turnout for some groups in the 2008 and 2012 presidential elections in Florida. In traditional regression or matching approaches (either cross‐sectional or difference‐in‐differences), data from Florida would be compared to data from states that did not implement early in‐person voting. These approaches are potentially problematic because there may be unmeasured differences between the states, and these differences may change across elections. One observable manifestation of this is that the candidates on the ballot will be different for different states in the same election year and for different election years in the same state. The front‐door and front‐door DD approaches allow us to solve this problem by confining analysis to comparisons made among modes of voting within a single presidential election in Florida."
"48","Additionally, by restricting our analysis to Florida, we are able to use individual‐level data from the Florida voter registration statewide database, maintained since January 2006 by the Florida Department of State's Division of Elections. This allows us to avoid the use of self‐reported turnout, provides a very large sample size, and makes it possible to implement all of the estimators discussed in earlier sections because we observe the mode of voting for each individual. Section 4 of the SI provides additional information regarding the preprocessing of the Florida data."
"49","Information on mode of voting in the voter history files allows us to define compliance with the program for the front‐door estimator (those who utilize EIP voting in the election for which we are calculating the effect are defined as compliers). Additionally, we use information on previous mode of voting to partition the population into a group of interest and differencing groups. In order to maximize data reliability, we define our group of interest as individuals who used EIP in a previous election. In other words, we are assessing what would have happened to these previous EIP voters in 2008 if the EIP program had not been available in 2008. For the 2008 EIP effect on turnout, we rely upon 2006 EIP voters as our group of interest. An attempt to define the group of interest more broadly (e.g., including nonvoters) or in terms of earlier elections (e.g., the 2004 election) would involve the use of less reliable data, and it would therefore introduce methodological complications that are not pertinent to the illustration presented here.7 Therefore, the estimates presented in this application are confined only to those individuals who utilized EIP in a previous election, and hence we cannot comment on the overall turnout effect.         "
"50","We consider two differencing groups for each analysis: those who voted absentee and those who voted on Election Day in a previous election. When considering the 2008 EIP effect for 2006 EIP voters, for example, we use 2006 absentee and Election Day voters as our differencing groups. The existence of an EIP program in 2008 might have induced some 2006 absentee ballot users to change their mode of voting in 2008 (e.g., from absentee to EIP), but it is unlikely to have caused them to vote. This is because 2006 absentee voters who voted EIP in 2008 would likely have just voted absentee in 2008 if the EIP program did not exist in 2012. For example, experimental evidence suggests that while mobilizing people to vote early increases turnout, it does not significantly alter the proportion of people who vote by mail and slightly reduces the proportion voting on Election Day (Mann and Mayhew 2012). It thus seems reasonable to assume that EIP offers alternative, not additional, opportunities for voting to past absentee and Election Day voters. In this case, any apparent effects on turnout estimated for these groups will be primarily due to bias, which can then be removed from the estimates for the group of interest. If in fact these apparent effects represent real effects for these groups, then our results will produce a lower bound. As discussed in earlier sections, the estimates from the differencing groups must be scaled according to the level of compliance for the group of interest. Finally, the existence of two differencing groups allows us to conduct a placebo test by using Election Day voters as the group of interest and the absentee voters as the differencing group. This analysis is explored below.         "
"51","Despite the limited scope of the estimates presented here, these results have some bearing on the recent debates regarding the effects of early voting on turnout. There have been a number of articles using cross‐state comparisons that find null results for the effects of early voting on turnout (Fitzgerald 2005; Gronke, Galanes‐Rosenbaum, and Miller 2007; Gronke et al. 2008; Primo, Jacobmeier, and Milyo 2007; Wolfinger, Highton, and Mullin 2005), and Burden et al. (2014) find a surprising negative effect of early voting on turnout in 2008.8 However, identification of turnout effects from observational data using traditional statistical approaches such as regression or matching relies on the absence of unobserved confounders that affect both election laws and turnout (Hanmer 2009). If these unobserved confounders vary across elections, then traditional difference‐in‐differences estimators will also be biased. See Keele and Minozzi (2013) for a discussion within the context of election laws and turnout. Additionally, a reduction in Florida's early voting program between 2008 and 2012 provided evidence that early voting may encourage voter turnout (Herron and Smith 2014).         "
"52","The front‐door estimators presented here provide an alternative approach to estimating turnout effects with useful properties. First, front‐door adjustment can identify the effect of EIP on turnout in spite of the endogeneity of election laws that can lead to bias when using standard approaches. Second, unlike traditional regression, matching, or difference‐in‐differences‐based estimates, the front‐door estimators considered here only require data from Florida within a given year. This means that we can effectively include a Florida/year fixed effect in the analysis, and we do not have to worry about cross‐state or cross‐time differences skewing turnout numbers across elections. We also include county fixed effects in the analysis in order to control for within‐Florida differences."
"53","However, in addition to the limited scope of our analysis, the exclusion restriction is violated for this application. Since early in‐person voting decreases waiting times on Election Day, it is possible that it actually increases turnout among those who only consider voting on Election Day. This would mean that front‐door estimates would understate the effect if all other assumptions held because the front‐door estimator would be ignoring a positive component of the effect. Alternatively, Burden et al. (2014) suggest that campaign mobilization for Election Day may be inhibited, such that early voting hurts Election Day turnout. This would mean that front‐door estimates would overstate the effect because the front‐door estimator would be ignoring a negative component of the effect. This can also be seen by examining the bias formula in Equation 4 (because the EIP treatment is assigned at the state level, Assumptions 1 and 4 hold).         "
"54","Taken together, the overall effect of these exclusion restrictions is unclear and would depend on the strength of the two violations. The predictions also become less clear once we consider the front‐door difference‐in‐differences approach, where additional bias in front‐door estimates might cancel with bias in the estimates for the differencing group. For the remainder of this analysis, we will assume that all such violations of the exclusion restriction cancel out in the front‐door difference‐in‐differences estimator. This is implicit in Assumption 5.         "
"55","In order to construct the front‐door estimate of the 2008 EIP effect for our group of interest, we calculate the turnout rate in 2008 for all individuals who voted early in 2006. We also calculate the noncomplier turnout rate in 2008 by excluding all individuals who voted early in 2008 from the previous calculation. The front‐door estimate of the 2008 EIP effect for 2006 early voters is thus the difference between the former and latter turnout rates. Quite intuitively, the counterfactual turnout rate without EIP for the group of interest is the observed turnout rate of noncompliers in that group. We do not devote much attention to the front‐door estimates, seeing as they are implausibly large.9 The positive bias stems from the fact that 2006 EIP voters would be more likely to vote in 2008, even in the absence of EIP, than the 2006 non‐EIP group (this group includes individuals who did not vote in 2006). In terms of the bias formula in Equation 4, this is equivalent to saying that .            "
"56","In order to address this bias, we present front‐door DD estimates for the 2008 EIP program in Figure 4. The estimates all utilize county fixed effects and are calculated separately across the racial categories.10 The front‐door difference‐in‐differences estimates for the group of interest (2006 EIP voters) are presented for both differencing groups: with 2008 absentee voters (triangles) and 2008 Election Day voters (squares). The former, for example, is constructed as the difference between front‐door estimates for 2006 early voters and the front‐door estimates for 2006 absentee voters, with the front‐door estimates for the differencing group scaled by the ratio of early voter compliance to absentee voter compliance as shown in Equation 5. The diamond estimates in Figure 4 represent the placebo test, with 2006 Election Day voters standing in as the group of interest and the absentee voters as the differencing group. In general, we note that if there exists more than one plausible differencing group, then one should conduct the analysis using each differencing group separately, as well as a placebo test to verify the plausibility of Assumption 5.            "
"57","Front‐Door DD Estimates for the Turnout Effect in 2008 for 2006 Early Voters (by Race)"
"58","Note: All estimates include county fixed effects. The 99% block bootstrapped confidence intervals are based on 10,000 replications.                        "
"59","The EIP program estimates are positive and significant at the 99% level. All placebo tests, with the exception of the white estimate, are indistinguishable from zero, giving us confidence in the estimated EIP effects. Even if the slightly negative placebo estimate for whites indicates a true negative effect of the 2008 EIP program, and not bias, the weighted average of the green and the purple effects (i.e., the 2008 EIP effect for the 2006 EIP and Election Day voters together) again produces a slightly positive estimate. Therefore, we generally find evidence that early voting increased turnout for the subset of individuals who voted early in 2006. Moreover, comparing the point estimates across races, we find some evidence that the program had a disproportionate benefit for African Americans."
"60","Our methodology uses voting behavior in 2006 only to define groups and does not compare turnout of voters across elections. Thus, any differences between presidential election and midterm election voters (see, e.g., Gronke and Toffey 2008) do not pose a prima facie problem for the analysis. However, robustness checks using the 2012 election are presented in Section 5 of the SI.            "
"61","In this article, we have developed front‐door DD estimators for nonrandomized program evaluations with one‐sided noncompliance and an exclusion restriction. These estimators allow for asymptotically unbiased estimation, even when front‐door estimators are biased. Additionally, even when the front‐door DD assumptions do not hold exactly, these estimators sometimes allow for informative bounds."
"62","We illustrated front‐door DD with an application to the National JTPA Study and with an application to the effects of Florida's early in‐person voting program on turnout. For the job training application, we showed that front‐door and front‐door DD approaches could be used to recover the experimental benchmark. For the application to the effects of an early in‐person voting program on turnout in Florida in 2008 and 2012, we found that for two separate differencing groups, the program had at least small but significant positive effects. While the scope of the analysis is limited, this result provides some evidence to counter previous results in the literature that early voting programs had either no effect or negative effects."
"63","More broadly, this approach is most likely to be helpful in one of three scenarios. First, with longitudinal/panel data, the over‐time front‐door DD approach can be used (as in the JTPA study). Second, for some cross‐sectional applications, a differencing group will be apparent. The absentee voters provide one example, whereas others may derive from eligibility cut‐offs. Third, even if a perfect differencing group is not available, a bound may be possible if we are willing to make assumptions about heterogeneity of effects between the group of interest and the differencing group. We showed an example of this with the married versus single analysis of the JTPA study, and there are likely to be a number of applications where this is possible (e.g., when heterogeneity has been studied in prior randomized studies). If we also have beliefs about the direction of bias for the front‐door approach, then we can use the front‐door and front‐door DD in a bracketing approach."
"64","Finally, the results in this article have implications for research design and analysis. The bracketing of the experimental benchmark in the JTPA application shows that control units are not always necessary for credible causal inference. This is a remarkable finding that should make a number of previously infeasible studies possible (e.g., when it is unethical or impossible to withhold treatment from individuals)."
