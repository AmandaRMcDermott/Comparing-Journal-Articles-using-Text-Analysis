"","x"
"1","Before we characterize error in the CMP dataset, we must understand the processes by which this error arises. These are essentially the same processes that underlie any human interpretative coding based, wholly or partially, on text sources. They therefore apply more generally to the many social science datasets that include variables generated by humans who read some text and then record a quantitative coding conditioned on this. To aid exposition, however, we focus on the data generation processes underlying the CMP. These are summarized in Figure 1.         "
"2","                 Overview of the Positions to Text to Coded Data Process                      "
"3","The premise of all content analysis is that there is something to be analyzed. Here, we think of this as the true policy position, μ, of the author of some text. This is fundamentally unobservable even, arguably, to the author. If the author is not a hermit, she may want to send signals about this position to others. These may represent “sincere” attempts to communicate μ or “strategic” attempts to communicate some other position. There is a strategic model of politics, M, that characterizes the author's incentives to signal a policy position that may or may not be μ—we can think of this signal as the intended message, π. Note that π exists only in the brain of the author and is also fundamentally unobservable.         "
"4","Having formed the intention to communicate π, the author generates some text, τ, to do this job. Every time the author sets out to communicate π, she is likely to generate a slightly different τ. As an aid to intuition here, consider what happens when an author's hard disk crashes after a long, hard day of manifesto writing. First, hair is torn out. Then an attempt is made to re‐create the day's work. The re‐created text is very unlikely to be identical to the lost text; indeed the author may well think of “better” ways to say the same thing, when given the job of saying it all over again. Now think of different authors, with somewhat different literary styles, all trying to convey precisely the same message. In a nutshell, there are many different versions of τ that could be generated with the sincere intention of conveying the same π. There is a stochastic text generation process, T, that maps π into τ.         "
"5","We now have an observed text τ, which we can take as having a “certain” content, at least to the extent there are unambiguous text characters deposited on the page. The process of reading the text now begins. In terms of a project such as the CMP, this involves a human expert reader first breaking the text into units, “quasi‐sentences” in the argot of the CMP, and then subjectively assigning these text units to categories in a predefined coding scheme. This scheme is a measurement instrument, I. In the CMP's case, I is a 56‐category scheme describing different types of policy statements the author might make, or 57 categories if the “uncoded” category is also included. The CMP scheme was defined by a particular group of scholars meeting in the mid‐1980s. It is almost certain that a different group of scholars meeting at the same time, or the same group of scholars meeting at a different time, would have defined a different coding scheme. The realized CMP coding scheme I is thus one of a huge number of possible coding schemes that could have been realized.         "
"6","Given an observed text τ and a realized coding scheme I, expert human readers interpret text units in τ and allocate these to coding categories in I. This coding process has both subjective and stochastic elements. The same human reader at different times, or a different human reader at the same time, may well allocate the same text unit to different coding categories. There is thus a stochastic text coding processC that, given I, maps τ into δ, a database of text codings. Given the stochastic processes we have outlined above, the codings in δ are associated with considerable uncertainty.4"
"7","The analyst wants the database of text codings in the first place because she wants to estimate something about the text's author. This involves scaling the data, using some scaling modelS. Clearly, there are many different scaling models that could be applied to the same database of text codings. The result of applying scaling model S to the database of text codings in δ will be a set of scales λ. In relation to the CMP, a very well‐known scale is the left‐right scale called “rile.” This is the feature of the scaled CMP dataset that is overwhelmingly the most commonly used in published work. There are, of course, many different possible sets of scalesλ that could be developed by applying scaling model S to database δ.         "
"8","Finally, the circle is closed as the analyst uses a text's measured scale positions, given λ, to make inferences about the text's author. These inferences may concern the author's text deposits τ, “true” position μ, or intended message π. Statistical inference in these matters can rely on conventional techniques. Logically valid inferences are increasingly dependent on underlying theoretical models as they move back the causal chain from τ to π to μ.         "
"9","We have been very explicit about all of this because it is important to focus carefully on particular features of the long process of causal inference summarized in Figure 1. Lack of clarity about this can, for example, lead to misplaced criticisms of the CMP data. Many of the alleged shortcomings attributed to the estimation of party positions from manifestos, for instance, concern the validity of using manifestos as unbiased, observable implications of true party positions. It is frequently argued, for example, that party manifestos are strategic documents that do not convey the “true” party position, in effect that μ≠π. But this is not a measurement issue. Assuming we can measure the intended message π from the observed text τ in an unbiased way, this is a matter of specifying the correct strategic model M that maps μ into π. The claim that manifestos are strategic documents does not therefore have any bearing on CMP text codings, but rather on the logical inferences that are drawn from these about unobservable “true” policy positions μ. The solution to this problem is not better text codings in δ but a better strategic model of politics, M. Similarly, it is perfectly reasonable to argue that the CMP's additive left‐right scale “rile” is flawed and that other left‐right scales using the same data, for example those proposed by Gabel and Huber (2000), or by Kim and Fording (1998), are more valid bases for drawing inferences about the policy positions, μ or π, of text authors. Again, this does not concern the database of CMP text codings, δ, but rather the validity of the scaling model S that maps these into a set of derived scales λ. The solution to this problem is a better scaling, not better text codings.         "
"10"," Figure 1 also helps us focus on features of the CMP dataset that are indeed intrinsic to the data collection project itself, further distinguishing between problems that can be fixed without recourse to additional data collection and those that cannot be addressed without new data on the coding of party manifestos. Thus little attempt has been made to take account of the fact that the CMP's core measurement instrument I, its 57‐category coding scheme, is but one realization of the many possible coding schemes that could have been devised.5 Clearly the CMP coding scheme is an utterly integral feature of the CMP dataset. Equally clearly, assessing the implications of this involves recoding the same documents using different schemes, and thus a major new data collection enterprise.         "
"11","Very little attempt has been made, furthermore, to characterize the stochastic coding process, C, by estimating the extent of variation between coders in applying the same coding scheme I to the same text τ. This cannot be investigated without conducting multiple human codings of the same document using the same coding scheme and thus also involves a major new data collection enterprise. Considerable attention has, however, been paid to the reliability and validity of scales derived from the CMP database of text codings, reflected in extensive discussion of the validity of the CMPs “rile” scale.6 Such discussions about scaling do not hinge on the collection of a new database of new text codings, δ, but rather on how a given dataset should be scaled.7"
"12","We are not concerned here with building scales from the CMP data, but with another aspect of the CMP manifesto dataset that can be addressed without a major new data collection exercise. This concerns the fact that there is a stochastic text generation process, T, that maps the intended message μ into an observed text τ. We model this process below, using both analytical techniques and simulations, allowing us to formalize the intuition that longer political texts, other things being equal, convey more information about their authors.         "
"13","In what follows, we want to estimate the level of uncertainty in CMP estimates of party policy positions that arises from the stochastic process of text generation. Before going forward, therefore, it is important to be clear about which of the processes mapped in Figure 1 we are going to hold constant. Taking things from the top, we are not concerned with modeling the text authors' strategic incentives to dissemble. We thus in effect assume that μ=π. Readers who do not believe this must specify a strategic model M of politics, mapping μ into π, that we do not consider here. Nor are we concerned here with the stochastic process, C, of human text coding, although this is something we directly estimate in a companion article. What we do assume here is that this stochastic process is unbiased. We take the CMP's 57‐category coding scheme as given and do not concern ourselves with the datasets that alternative coding schemes might have produced. While the scaling model S that has been applied to the database of CMP codings clearly raises crucial issues, we take two core features of this as given in what follows. The first is the scaling assumption that measures a text's relative emphasis on a CMP coding category as the percentage of coded text units assigned to that category. The second is the precise definition of the CMP's “rile” scale. What we do focus on in what follows is the stochastic process T that maps text authors' unobservable policy positions μ (=π) into observable text deposits τ.            "
"14","For a given policy category j, define πij as the true but unobservable intended policy message from the text's author, represented as country‐party‐date unit i. The j categories in this case are the 56 policy categories in the CMP coding scheme, plus an additional category for “uncoded,” giving a total of k = 57 categories. Since, according to the CMP's measurement model, true policy positions are represented by relative or “contrasting” emphases on different policy categories within the manifesto, these policy positions are relative proportions, with .8 For example, party i's emphasis, for a given election, on the 20th issue category in the CMP coding scheme (401: Free Enterprise), is represented as πi20.            "
"15","We can never observe the “true” policy positions of manifesto authors, πij. It is possible, however, to have a human coder analyze party i's manifesto using the CMP's coding scheme, and thereby to measure the relative emphasis given in the manifesto to each πij. This is measured as p1, … pk, where pj≥ 0 for j = 1, … , k and . In the absence of systematic error (bias):               "
"16","In other words, the observed relative emphasis given to each coding category in a party's manifesto will on average reflect the true, fixed, and unobservable underlying position πij. The realization of πij in any given manifesto, however, reflects the stochastic process of text authorship, yielding the observed proportions pij. Every time a manifesto is written with the intention of expressing the same underlying positions πij, we expect to observe slightly different values pij.            "
"17","Given this characterization of both observed and unobservable policy positions, which directly follows the CMP's own assumptions, we can postulate a statistical distribution for observed policy positions. If we assume each text unit's allocation to a policy category is independent of the allocation of each other text unit, then we can characterize the CMP's realized manifesto codings as corresponding to the well‐known multinomial distribution with parameters ni and πij, where ni refers to the total number of quasi‐sentences in manifesto i. The probability for any manifesto i of observing counts of quasi‐sentences xij from given categories j is then described by the multinomial formula:               "
"18","In the context of the CMP coding process for a given manifesto, each xk represents the number of text units coded to a given category j, since through the multinomial expectation, E(xij) =pijni. In terms of the “PER” or percentage categories reported by the CMP for each manifesto, what is actually reported is xij/nij100, or the estimate of manifesto i's “true” percentage (πij100) of the quasi‐sentences from category j. We have no additional information that might lead us to conclude there is a systematic function mapping (in a biased way) the true position to a different expected observed position—already expressed by equation (1). Our concern here is with nonsystematic (unbiased) error, which is the extent to which Var (pij) > 0, even though πij is fixed at a single, unvarying point.9"
"19","So far we have considered only the case of a “given” manifesto, but of course the combined CMP dataset deals with many such units—a total of 3,018 separate units representing different combinations of country, election date, and political parties for the combined (MPP+MPP2) datasets.10 If we are to fully characterize the error from the stochastic process whereby texts are generated, then this will mean estimating Var(pij) for every manifesto i for all k = 57 categories.11"
"20","The lengths (ni) of the coded manifestos underlying the CMP dataset vary significantly, although this valuable information is almost never referred to by subsequent users of CMP data. About 30% of all coded manifestos had fewer than 100 quasi‐sentences, coded into one of 56 categories. Some had fewer than 20 quasi‐sentences; some had more than 2,000. Despite very wide variation in the amount of policy information in different manifestos, policy positions estimated from CMP data are almost always treated in the same way, regardless of whether they are derived from coding 20 text units or 2,000.12 The total number of text units found in a manifesto appears to be, absent systematic information or prior expectation on this matter, unrelated to any political variable of interest. Yet, while assuming that the proportions πij remain the same regardless of document length, increasing the length of a manifesto does increase confidence in our estimates of these proportions. This reflects one of the most fundamental concepts in statistical measurement: uncertainty about an estimate should decrease as we add information to that estimate.13 Given that our characterization of the stochastic process that produces observed text categories depends directly on the length of the text, we show next how to use this information to produce error estimates directly reflecting this basic uncertainty principle.            "
"21","One way to assess the error variance of estimated percentages of text units in any of the CMP's 56 coding categories is through the analytic calculation of variance for the multinomial distribution we have used to model category counts. The goal is to determine the variance of each of the policy (“PER”) categories reported by the CMP, which in the language described above represent  for each category j and each manifesto i. Here we assume no coding bias (by equation 1), where each πij represents the true but unobservable position of country‐party‐date unit i on issue j.            "
"22","Returning to the definition of the multinomial distribution in equation (2), for any multinomial count Xij, the variance is defined as               "
"23","In part, then, the error will depend on the size of the true percentage of mentions pij100 for each “PER” category j. Assuming this quantity is fixed for each party‐election unit i, however, what is variable as a result of the data‐generating process is the length ni of the manifesto. This aspect of the error in the CMP estimates, therefore, is inversely proportional to the (square root of the) length of the manifesto. This should be reassuring, since it means that longer manifestos reduce the error in the estimate of any coding category j, irrespective of pj. Longer manifestos provide more information, and we can be more confident about policy positions estimated from them.            "
"24","The situation is more complicated for additive measures such as the pro‐/anti‐EU scale (PER108 ‐ PER110) or for the CMP's widely used left‐right scale, an additive scale obtained by summing percentages for 13 policy categories on the “right” and subtracting percentages for 13 categories on the “left.” This is because, for summed multinomial counts, the covariances between categories must also be estimated, since it is a property of variance that Var(aX+bY) =a2Var(X) +b2Var(Y) + 2abCov(X, Y). There are several strong reasons, including the limited observations we have of nonrandom ways in which different human coders code the same text unit into different categories, as well as innate substantive relationships between coding categories, to suspect that these covariances will be nonzero. For these reasons, we do not recommend using analytically derived errors for composite scales aggregated from the CMP's 56‐category scheme; instead we advocate a more general, nonparametric approach: simulation.            "
"25","Given potential analytical problems we identify at the end of the previous section, we suggest an alternative way to assess the extent of error in CMP estimates. This uses simulations to re‐create the stochastic processes that led to the generation of each text, based on our belief that there are many different possible texts that could have been written to communicate the same underlying policy position. We do this by bootstrapping the analysis of each coded manifesto, based on resampling from the set of quasi‐sentences in each manifesto reported by the CMP. Bootstrapping is a method for estimating the sampling distribution of an estimator through repeated draws with replacement from the original sample. It has three principal advantages over the analytic derivation of CMP error in the previous section. First, it does not require any assumption about the distribution of the data being bootstrapped and can be used effectively with small sample sizes (N < 20) (Efron 1979; Efron and Tibshirani 1994). Second, bootstrapping permits direct estimation of error for additive indexes such as the CMP “right‐left” scale, without making the assumptions about the covariances of these categories required to derive an analytic variance. Since exact covariances of these categories are unknown, sample dependent, and influenced by nonrandom coder errors, it is highly speculative to make the assumptions needed for analytical computation of variance for additive scales. Finally, simulation allows us to mix error distributions, a key requirement in our case if we wish to incorporate additional forms of error. For instance, we might also wish to simulate coder variances such as the (possibly normally distributed) differences in text unitization mentioned by Volkens (2001), although we do not do so here. For all of these reasons, we always prefer the bootstrapped error variances over an analytic solution for additive CMP measures such as the left‐right scale.            "
"26","The bootstrapping procedure is straightforward. Since the CMP dataset contains percentages of total manifesto sentences coded into each category, as well as the raw total number of quasi‐sentences observed, we convert percentages in each category back to raw numbers. This gives a new dataset in which each manifesto is described in terms of the number of sentences allocated to each coding category. We then bootstrap each manifesto by drawing 1,000 different random samples from the multinomial distribution, using the pi as given from the reported PER categories. Each (re)sampled manifesto looks somewhat like the original manifesto and has the same length, except that some sentences will have been dropped and replaced with other sentences that are repeated. We feel this is a fairly realistic simulation of the stochastic text generation process. The nature of the bootstrapping method applied to texts in this way, furthermore, will strongly tend to reflect the intuition that longer (unbiased) texts contain more information than shorter ones.            "
"27","One problem that is not addressed by bootstrapping the CMP manifesto codings is that, as anyone who has a close acquaintance with this dataset knows, many CMP coding categories are typically empty for any given manifesto—resulting in zero scores for the variable concerned. No matter how large the number we multiply by zero, we get zero. Thus a user of CMP data dealing with a 20‐sentence manifesto that populates only 10 coding categories out of 56 must in effect assume that, had the manifesto been 20,000 sentences long, it would still have populated only 10 categories. In extremis, if some manifesto populated only a single CMP coding category, then every sampled manifesto would be identical. We cannot get around this problem with the CMP data by bootstrapping, unless we make some very interventionist assumptions about probability distributions for nonobserved categories. We prefer to assume that zero categories—for example, zero mentions of the European Union by Australian party manifestos in 1966—reflect a real intention of the text author not to refer to the matter at issue. We thus, for want of better information, take zero categories at face value. In addition, tests using simple methods to deal with observed zeros—e.g., “add‐one” smoothing (Jurafsky and Martin 2000, chap. 6.3)—showed no noticeable differences to our results.15"
"28","The great benefit of bootstrapping CMP estimates to simulate the stochastic process of text generation is that we can generate standard errors and confidence intervals associated with the point estimates, not only for each coding category but also for scales generated by combining these categories. Furthermore, even though we have strong reasons to believe CMP estimates follow a multinomial distribution, bootstrapping provides error estimates without needing to assume any distributional information not present in the observed quasi‐sentences from the texts themselves. Finally, simulating rather than deriving error also allows for the possibility of adding in additional error, such as coding error, although we do not do so here."
"29","The results of this bootstrapping provide error variances that decline as exponential functions of text length, something that holds true both for single categories and for additive scales such as the CMP “right‐left.” In addition, comparing bootstrapped error variance with variance computed analytically (per equation 5), we get nearly identical results.16 The near equivalence of these two very different methods for estimating standard errors adds to our confidence in both the analytical derivation of CMP error variance and the method of bootstrapping text units in manifestos. In particular, it suggests that the violation of the assumption of independence between coding category probabilities across text units does not seem to be a serious problem, although this assumption deserves attention in future work. It also adds confidence to our belief that the number of text units identified is not systematically related to the coding of these units into policy categories. When we apply our new error estimates to specific empirical research problems in the next section, we use the bootstrap‐estimated error as our best approximation of overall nonsystematic error in the CMP's reported estimates.            "
"30","There are two main reasons to estimate policy positions of political actors. The first is cross‐sectional: a map of some policy space is needed, based on estimates of different agent positions at the same point in time. The second is longitudinal: a time series of policy positions is needed, based on estimates of the same agent's policy positions at different points in time. Alternative techniques can estimate cross‐sectional policy spaces; the signal virtue of the CMP data, and the dominant reason for its use by third‐party scholars, is that it purports to offer time‐series estimates of party policy positions. However, neither cross‐sectional nor time‐series estimates of policy positions contain rigorously usable information if they do not come with associated measures of uncertainty. Absent any such measure, estimates of “different” policy positions may either be different noisy estimates of the same underlying signal, or accurate estimates of different signals.         "
"31","A substantial part of the discussion found in MPP and MPP2 of the face validity of the CMP data comes in early chapters of each book, during which policy positions of specific parties are plotted over time. Sequences of estimated party policy movements are discussed in detail and held to be substantively plausible, with this substantive plausibility taken as evidence for the face validity of the data. But are these vaunted changes in party policy “real” or just measurement noise? We illustrate how to answer this question with a specific example related to environmental policy in Germany, a country where environmental policy is particularly salient, and also where the CMP has been based for many years. Figure 2 plots the time series of the estimated positions of the CDU‐CSU, for a long time Germany's largest party, on PER501 (Environment: Positive in the CMP coding scheme). The dashed line shows CMP estimates; error bars show our bootstrapped 95% confidence intervals around these estimates.            "
"32","                 Movement on Environmental Policy of German CDU‐CSU over Time                         "
"33","Movement of dashed line is % environment with 95% CI; dotted line is the number of quasi‐sentences per manifesto coded PER501."
"34","Error bands around CMP estimates are large in this case. Most estimated “changes” over time in CDU‐CSU environmental policy could well be noise. Statistically speaking, we conclude that the CDU‐CSU was more pro‐environmental in the early 1990s than it was either in the early 1980s or the early 2000s; every other observed “movement” on this policy dimension can easily be attributed to noise in the textual data."
"35"," Table 1 reports the result of extending this anecdotal discussion in a much more comprehensive way. It deals with observed “changes” of party positions on the CMP's widely used left‐right scale (RILE) and thus systematically summarizes all of the information about policy movements that is used anecdotally, in the early chapters of MPP and MPP2, to justify the face validity of the CMP data. The table reports, considering all situations in the CMP data in which the same party has an estimated position for two adjacent elections, the proportion of cases in which the estimated policy “change” between one election to the next is statistically significant. These results should be of considerable interest to all third‐party researchers who use the CMP data to generate a time series of party positions. They show that observed policy “changes” are statistically significant in only 38% of relevant cases. We do not of course conclude from this that CMP estimates are invalid. We do conclude that many policy “changes” hitherto used to justify the content validity of CMP estimates are not statistically significant and may be noise. More generally, we argue that, if valid statistical (and hence logical) inferences are to be drawn from “changes” over time in party policy positions estimated from CMP data, it is essential that these inferences are based on valid measures of uncertainty in CMP estimates, which have not until now been available.            "
"36","While one of the CMP's biggest attractions is undoubtedly the time‐series data it appears to offer, another common CMP application involves comparing different parties at the same point in time. Considering a static spatial model of party competition, realized by estimating positions of actual political parties at some time point, many model implications depend on differences in policy positions of different parties. It is crucial, therefore, when estimating a cross‐section of party policy positions, to know whether estimated positions of different parties do indeed differ from each other in a statistical sense. Figure 3 illustrates this problem, showing estimates of French party positions in 2002, on the CMP left‐right scale. Taking into account the uncertainty of these estimates, four quite different parties—the Communists, Socialists, Greens, and Union for a Popular Movement (UMP)—have statistically indistinguishable estimated positions, even though the CMP point estimates seem to indicate differences. Only the far‐right National Front had an estimated left‐right position that clearly distinguishes it from other parties. On the basis of these estimates we simply cannot say, notwithstanding CMP point estimates, whether the Greens (Verts) were to the left or the right of the Socialists (PS) in 2002. The role of uncertainty in cross‐sectional comparisons will differ according to context, but the French case demonstrates—for a major European multiparty democracy—that inferences of difference from CMP point estimates can be ill informed without considering measurement error.            "
"37","                 Left‐Right Placement of the Major French Parties in 2002. Bars Indicate 95% Confidence Intervals                         "
"38","When covariates measured with error are used in linear regression models, the result is bias and inefficiency when estimating coefficients on error‐laden variables (Hausman 2001, 58). These coefficients are typically expected to suffer from “attenuation bias,” meaning they are likely to be biased towards zero, underestimating the effect of relevant variables. This conclusion must, however, be qualified, since it depends on the relationship between the “true” predictor and the noisy proxy available to the researcher, and possibly other variables in the model. More precisely, the effect of measurement error depends on the estimation model and the joint distribution of measurement error and the other variables (Carroll et al. 2006, 41). In the case of linear regression the effects of measurement error can range from simple attenuation bias, to masking of real effects, appearance of effects in observed data that are not present in the error‐free data, and even reversal of signs of estimated coefficients compared to the case in the absence of measurement error.            "
"39","By far, the most common use of policy scales derived from CMP data tends to be as explanatory variables in linear regression models. Of all the studies using CMP data as covariates in linear regression models, however, to our knowledge not a single one has explicitly taken account of the likelihood of error in CMP estimates, or even used the length of the underlying manifesto as a crude indication of potential error. As a result, we expect many reported coefficients in studies using CMP data to be biased."
"40","We address this issue by replicating and correcting two recent high‐profile studies using CMP data, both published in this journal: Adams et al. (2006), and Hix, Noury, and Roland (2006). In both cases we obtained datasets (and replication code) from the authors and replicated the analyses, correcting for measurement error in CMP‐derived variables. We do this using a simple error correction model known as simulation‐extrapolation (SIMEX) that allows generalized linear models to be estimated with correction for error‐prone covariates whose variances are known or assumed (Carroll et al. 2006; Stefanski and Cook 1995). While not widely used in political science, SIMEX has been applied recently by Hopkins and King (2007) as a means to correct misclassification errors in text analysis. Here, by contrast, we apply the method to correct for random measurement error in observed covariates.            "
"41","The basic idea behind SIMEX is fairly straightforward. If a coefficient is biased by measurement error, then adding more measurement error should increase the degree of this bias. By adding successive levels of measurement error in a resampling stage, it is possible to estimate the trend of bias due to measurement error versus the variance of the added measurement error. Once the trend has been established, it then becomes possible to extrapolate back to the case where measurement error is absent. Following Carroll et al. (2006, 98–100) the SIMEX algorithm can be succinctly described as a sequence of steps that we illustrate in Figure 4. The example taken is the EU Integration variable from Hix, Noury, and Roland (2006, Model 6) replicated fully below. First, in the simulation step additional random pseudo errors are generated from a normal distribution with mean 0 and variance ζmσ2u and added to the original data. Since m is known and chosen to satisfy 0 =ζ1 < ζ2 < … < ζM (we use typical values {0.0, 0.5, 1.0, 1.5, 2.0}), the simulation step creates m datasets with increasingly larger measurement error variances. The total measurement error variance in the mth dataset is σ2u+ζmσ2u= (1 +ζm)σ2u. In the estimation step the model is fit on each of the generated error‐contaminated datasets. The simulation and estimation steps are repeated a large number of times (500 times in our replication example), and the average is taken for each level of contamination. These averages are plotted against the values of ζ (the filled circles in Figure 4), and an extrapolant function is fit to the averaged, error‐contaminated estimates. In terms of ζm an ideal, error‐free dataset corresponds to (1 +ζm)σ2u= 0, i.e., ζm=−1.17 Extrapolation to the ideal case (ζ=−1) yields the SIMEX estimate (the hollow circles in Figure 4). The quadratic extrapolant function is usually preferred, since it has been shown to result in more conservative corrections for attenuation and is often more numerically stable than the alternative nonlinear function (also shown in Figure 4; Carroll et al. 2006; Hardin, Schmiediche, and Carroll 2003; Lederer and Küchenhoff 2006). (In our replications below, we report corrections based on the more conservative quadratic extrapolation.)            "
"42","                 SIMEX Error Correction in EU Integration with Quadratic and Nonlinear Extrapolant Functions, from Hix, Noury, and Roland (2006)                         "
"43","More complicated error corrections are of course possible, but here we deliberately chose a method that is simple, applicable to a wide class of generalized linear models, and for which freely available software is available that can be used with popular statistical packages.18"
"44","Adams, Clark, Ezrow, and Glasgow (2006)  Adams et al. (2006) analyze whether political parties in Western Europe adjust their ideological orientations in response to shifts in voters' policy preferences. The authors extend the “dynamic representation” model by empirically analyzing whether the type of political party affects the causes and consequences of their movements on policy. In particular the article is concerned with whether “niche” parties (typically Communists, Greens, or extreme‐right) respond differently to public opinion shifts compared to mainstream parties (e.g., Labor, Socialist, Social Democratic, Liberal, Conservative, and Christian Democratic).               "
"45","The first model analyzed in the original article and replicated here deals with whether mainstream and niche parties differently adjust their policies in response to public opinion shifts. Party policy shifts are operationalized as changes in a party's CMP left‐right scale position in successive elections. This measure is regressed on public opinion shifts, a dummy variable for niche party status, the interaction of these two variables, lagged dependent variable, lagged vote share change, the interaction of these two terms, and a set of country dummies. The authors' expectation is that if the coefficient on Public opinion shift is positive and statistically significant, then mainstream parties are responsive to shifts in public opinion along the lines of the dynamic representation model. They also expect to find a negative and statistically significant coefficient on the Niche Party × Public opinion shift variable, providing evidence that niche parties are less responsive to public opinion shifts than mainstream parties, thereby supporting the main “policy stability” hypothesis of the article. In our replication of Adams et al. (2006, Table 1), we focus on the effect of measurement error in both the dependent variable on the left‐hand side, its lagged value on the right‐hand side, and an interaction of the lagged dependent variable and lagged change in vote share. In the classical measurement error (CME) domain, it is known that measurement error in the dependent variable, if uncorrelated with other covariates, will only inflate standard error of the regression (Abrevaya and Hausman 2004), while measurement error in independent variables will bias the results.19 We assume here and in subsequent replications that all other covariates are measured without error. The error estimate in contaminated covariates is derived from our bootstrapped standard error.20"
"46","The second model in Adams et al. (2006) tests whether policy adjustments (shifts in policy towards the center of the voter distribution or away from it) affect parties' electoral support and whether this relationship differs between mainstream and niche parties. Key explanatory variables are constructed from the CMP and thus are expected to be error‐prone: Centrist policy shift, Noncentrist policy shift, Niche Party × Centrist policy shift, Niche Party × Noncentrist policy shift. The first variable is measured as the absolute value of the change in a party's position on the CMP left‐right scale when a leftist party shifts right or rightist party shifts left, and zero otherwise. The variable measuring the shift away from the center is similarly constructed. The next two variables pick up the differences in electoral effects for niche and mainstream parties in relation to centrist and noncentrist policy shifts.21Adams et al. (2006) expect mainstream parties to gain votes in the centrist policy shift and lose votes in noncentrist shift, thus leading to the expectation of a positive and statistically significant coefficient on Centrist policy shift and a negative and statistically significant coefficient on Noncentrist policy shift. The authors suggest that niche parties are electorally penalized for policy adjustments regardless of the direction of this adjustment (centrist or noncentrist) in what they call the “costly policy shift” hypothesis. This leads to the expectation of statistically significant and negative coefficients on both Niche Party × Centrist policy shift and Niche Party × Noncentrist policy shift. At the same time another hypothesis put forward by Adams et al. (2006) states that niche parties lose votes in comparison to mainstream parties for moderating their policy stance (“costly policy moderation” hypothesis). In turn this results in the expectation of a negative and statistically significant coefficient only on the Niche Party × Centrist policy shift variable.               "
"47"," Table 2 presents results of our error correction for both models, taken from the two regression tables of Adams et al. (2006). For each model, we compare our replication of the published results with SIMEX estimates.22 The most profound effect of the SIMEX correction of Model 1 is the expected inflation of the standard error of the regression and drop in explained variance as the consequence of measurement error in the dependent variable. The effect of error correction in the covariates decreases the key explanatory variables in size but they remain statistically significant. The full extent of SIMEX error correction effects can be gleaned from the changes in coefficients and standard errors presented in Table 2. The results show support for the hypothesis that niche parties' policy programs are less responsive to shifts in public opinion compared to mainstream parties (the grayed row in Model 1). Evidence for this claim, however, is drawn from a model with much weaker explanatory power.               "
"48","In the original article, the negative and statistically significant coefficient on Niche Party × Centrist policy shift (Model 2) is meant to support the “costly policy moderation” hypothesis that, in comparison to mainstream parties, niche parties are penalized by voters for moderating their policy positions. Results in the original article substantively mean that a one‐unit shift closer to the center of the voter distribution along the 1–10 left‐right scale, results, ceteris paribus, in niche parties' electoral loss of nearly 4% (i.e., approximately −5.67 + 1.45, see 523). Evidence for this conclusion weakens as the result of the SIMEX correction. The coefficient on Niche Party × Centrist policy shift becomes smaller in size and remains statistically significant only at the 0.10 level. In turn, depending on the take on statistical significance cutoff points, this may force the rethinking of some of the theoretical implications of the article. The conclusion that for niche parties “both vote‐seeking and policy‐seeking objectives motivate a stand‐pat strategy” because moderation in policy positions is penalized by voters (525, emphasis in original) is not supported by empirical evidence based on the error‐corrected estimates at the conventional 0.05 level of significance.               "
"49","Moreover, Adams et al. (2006, 525) claim that their empirical results support the “costless spatial mobility” assumption typically used in spatial modeling—i.e., that political parties are not electorally penalized for shifting positions in policy space—with respect to mainstream parties. In fact, as Table 2 shows, the corrected coefficient for Noncentrist policy shift almost doubles as the result of the SIMEX correction. Indeed, if a one‐tailed hypothesis test were applied to the coefficients for both Noncentrist policy shift and Niche Party × Centrist policy shift, both would be considered statistically significant. In terms of the conclusions of the original article, the error‐corrected results challenge its categorical conclusion that mainstream parties are not penalized for shifting policies away from the center—suggesting that this effect occurs with at least as much confidence as the conclusion that niche parties are punished for shifting their policies to the center.               "
"50","Hix, Noury, and Roland (2006)  Hix, Noury, and Roland (2006) are concerned with the content and character of political dimensions in the European Parliament (EP). Following an inductive scaling of roll‐call votes in the EP from 1979 and 2001, Hix, Noury, and Roland (2006) set out to validate their interpretation of the derived policy dimensions by regressing the mean position of each national party's delegation of MEPs on two sets of independent variables. The first set includes exogenous measures of national party positions on the left‐right, social and economic left‐right, and pro‐/anti‐EU dimensions. The second set relates to government‐opposition dynamics and consists of categorical variables describing whether a national party was in government and whether the party had a European Commissioner, as well as dummy variables for each European party group, each EU member state, and each (session of) European Parliament. Measures of national party positions are taken directly from the CMP dataset or constructed from it. National party positions on the EU are taken as the difference between positive (category PER108) and negative (category PER110) mentions of the EU. Party positions on economic and social policy are also constructed from the CMP categories (see Laver and Garry 2000, 628–29). The authors expect that national party ideal point estimates on the first dimension will be explained by the exogenous left‐right policy positions, while exogenous policy positions on the EU Integration dimension explain national party ideal point estimates on the second dimension (501). The expectation then is roughly that the first dimension is predominantly about left‐right and the second dimension is about Europe.               "
"51"," Table 3 contrasts coefficients from our replications of the models using CMP variables in Hix, Noury, and Roland (2006) with error‐corrected measurements based on our bootstrapped variances. (Due to space constraints we present replications of only the two models that related to the structure of the first dimension in the European Parliament.) Model 3 aims to explain the mean positioning of political parties on the first derived EP dimension in terms of their positions on the economic left‐right, social left‐right, and European Integration dimensions; categorical variables relating to whether a party was in government and had a European Commissioner; and dummy variables for each session of the EP. Model 6 extends Model 3 to also include dummy variables for each European party group.               "
"52","It is clear from Table 3 that the SIMEX error correction has the most important effect on the “EU Integration” variable. The SIMEX estimate of EU Integration is about double the size of the naive estimate in both models presented and becomes statistically significant in the corrected estimates of Model 6. Substantively, the effect of noise in the CMP measure of EU policy is that, if we set out to explain the position of a party's MEP delegation, the national party's position on the EU is shown to be more important than its position on the substantive economic and social left‐right dimensions, rather than unimportant as Hix, Noury, and Roland conclude. SIMEX correction of the key EU Integration variable thus forces a rethinking of some of the substantive conclusions of this article. In the words of Hix, Noury, and Roland (2006) interpreting their results from the naive model:               "
"53","                                    "
"54","EU policies of national parties and national party participation in government are only significant without the European party group dummies. This means that once one controls for European party group positions these variables are not relevant explanatory factors on the first dimension. (502)"
"55","In a direct challenge to this conclusion, results from the error‐corrected model suggest that EU policies of national parties only appear not to be relevant because of attenuation bias caused by noise from the textually derived CMP measures of positioning on EU policy. Once this error is corrected for, the primary dimension of EP voting is shown to be influenced even more by EU policy than by general left‐right positions."
"56","Bodies of text are data. We can analyze these data using well‐known statistical tools. The implications of this are deep and general. Our discussions in this article apply to the analysis of most bodies of text, and in particular to analyses of text based on interpretative coding by human experts. While we focus here on text observed in party manifestos and analyzed by the CMP, the problems we identify and set out to correct apply to any dataset based on human interpretative coding. Our focus on the CMP reflects the very widespread use of this dataset within the profession, generating a large number of publications in the best professional journals. These publications never take account of the fact that the data analyzed clearly contain measurement error and that this measurement error can clearly bias research findings."
"57","We approach this problem by considering ways in which manifestos provide systematic information about the policy positions of their authors, in the form of text units deposited as random variables in a process of authorship that is inherently stochastic, even when the author's underlying position is fixed. We simulate this process, thereby computing error estimates for the entire CMP dataset, and show how such errors affect descriptive and causal inferences based on CMP measures. Building on this method, we offer a “corrected” version of the CMP dataset with bootstrapped standard errors for all key estimates, available our website from http://www.politics.tcd.ie/cmp/.         "
"58","The substantive consequences of our new estimates of error in CMP data are far from trivial. Many apparent “differences” in CMP estimates of party policy positions—differences over time in the position of one party or differences between parties at one point in time—are probably attributable to stochastic noise in textual data rather than real differences in policy positions. Only about one‐quarter of all CMP‐estimated “movements” in parties' left‐right policy positions over time were assessed on the basis of our simulations to be statistically significant."
"59","Replicating two recently published articles in which error‐prone CMP variables are used as covariates, we show how to correct these using a SIMEX error correction model, based on bootstrapped estimates of likely error. The probable systematic effect of error‐contaminated variables is the inflation of the standard error of the regression in the case of measurement error in the dependent variable, and bias with measurement error in the covariates. While error in covariates typically causes attenuation bias in linear models, as our replication of the Adams et al. results has shown this is not always true for more complicated models. Some error‐corrected effects are stronger, and more significant, than those estimated in models taking no account of error in the covariates. Other times the effect of error correction is the opposite: making covariates statistically insignificant. Measurement error correction can cause substantively important reinterpretation of results. A good example is what emerges as the potentially flawed inference that national party policy positions on the EU have no influence on their EP roll‐call voting behavior, an inference that is reversed once account is taken of error contamination in the CMP dataset's sparsely populated variables measuring EU policy. Similarly, a conclusion that in comparison to mainstream parties niche parties are penalized by voters for moderating their policy positions has also been cast into doubt once the effects of measurement error are corrected."
"60","The importance of estimating and making use of uncertainty in political science data, of course, is not limited to manifesto coding and the CMP dataset. Many commonly used measurements, such as survey data, roll‐call votes, expert surveys of party policy (Benoit and Laver 2006), categories of legislation (e.g., Mayhew 1991), the democraticness of regime type (see Bollen and Jackman 1989), and a myriad of other commonly used variables are measured with levels of error. Even when estimates of measurement error are provided—as is the case with surveys, expert surveys, and more recently, roll‐call votes (e.g., Clinton, Jackman, and Rivers 2004)—political scientists rarely, if ever, make use of these estimates in the ways we encourage here.         "
"61","While we have taken an important first step towards providing a practical and theoretically supported means to estimate nonsystematic measurement error in CMP estimates, the solution we provide here is hardly the last word on the topic. Analyses of coder differences and/or coder error, for example, could uncover systematic error leading to bias, something not addressed in this article but acknowledged to be a problem warranting serious attention. In ongoing work using coder experiments with multiple independent codings of the same texts, we have found strong evidence that coding is not only stochastic but also appears to suffer from systematic forms of error. While we have chosen to focus purely on nonsystematic error in this article, a full accounting for error in the CMP ought to consider both stochastic features of textual data as well as systematic and nonsystematic errors from the coding of that text. Finally, other means of implementing error correction models are certainly possible, including Bayesian‐MCMC methods that can take into account the unit‐specific nature of error in our error estimates. Indeed, we hope our focus on error in the widely used CMP estimates will stimulate a broader dialogue on measurement error in many of the most commonly used measures in political science, such as opinion survey results, expert survey measures, or other computed quantities. Given our knowledge of measurement error and the wide availability of techniques for dealing with this, there is no longer any excuse for scholars to use error‐prone measures as if these were error free."
