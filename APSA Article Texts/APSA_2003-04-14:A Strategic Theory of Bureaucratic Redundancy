"","x"
"1","To illustrate very simply why a strategic perspective on redundancy matters, consider the following example. A principal can choose up to two agents to share jurisdiction over a single task. The task might be the gathering of intelligence that potentially involves the “turf” of both civilian and military agencies (e.g., Federal Bureau of Investigation, Central Intelligence Agency, National Security Agency, military intelligence, State Department). Each agent may individually succeed or fail. For simplicity, assume that the probability of success, or reliability, is r∈ (0, 1).3 This probability is independent across agents, which may reflect the fact that agents use different methodologies in their work.3 The overall outcome of the task is “good” for the principal if either agent succeeds in gathering the crucial intelligence, and “bad” otherwise. The principal receives one unit of utility for a good outcome, and zero for a bad one.         "
"2","In a nonstrategic environment, each agent's reliability is unaffected by the presence of other agents. Thus, the probability of a good outcome is simply r with one agent, and r+ (1 −r) r > r with two. The redundant agent therefore raises the probability of a good outcome and the principal's expected utility, by (1 −r) r.         "
"3","Now suppose that agents are strategic; that is, they receive payoffs from outcomes and choose effort levels. Agents receive one unit of utility for a good outcome, and zero otherwise. Each agent can now choose either to work or shirk. Its reliability, if it should work, is r; otherwise, it is zero. Working imposes a cost c∈ (0, r), while shirking costs nothing. Thus, each agent shares the principal's desire for a good outcome and would exert effort if acting alone. However, each also understands that its effort is wasted if the other agent succeeds. The following payoff matrix summarizes the game.         "
"4","Notice that the outcome of the nonstrategic model, (Work, Work), is a Nash equilibrium if and only if 2r−r2−c≥r, or . In other words, r must be moderate and c low for both agents to work. When r is low, the marginal contribution of the second agent is too low to justify the cost, while a high r leaves little room for improvement. In both cases, agents face a collective action problem, and the Nash equilibrium consists of one agent working and the other shirking.         "
"5","The game extends easily to more agents. Generally, with n > 1 agents, all agents will work in a Nash equilibrium only if 1 − (1 −r)n−c≥ 1 − (1 −r)n−1. It is then straightforward to demonstrate that for a given c, the maximum number of agents that will work in a pure strategy equilibrium is given by the highest n satisfying c≤r(1 −r)n−1. Beyond this threshold, additional agents have no effect on the probability of a good outcome. This expression also implies that as costs increase, the number of working agents must decrease.         "
"6","This example shows that the classic redundancy model is not robust to the introduction of strategic agents. Even with the simplest possible generalization to a game theoretic formulation, the prediction of redundancy theory is a special case occurring only under a certain set of parameter values. In this particular example, agents face the familiar collective action problem. In the games developed in the following sections, other effects of strategic interaction between principals and agents will also become evident."
"7","The direct redundancy game greatly generalizes the game‐theoretic model of the preceding example. It addresses the basic question raised by redundancy theory: in a given period, how many agents should a principal choose? The model also forms the basis of the extensions developed in later sections."
"8","Environment and players  The game describes a single period of policy making under uncertainty between two kinds of players: a principal (P) and up to N > 0 agents, denoted A1, … , AN. P may be construed as any actor with authority over the number of units to assign to a particular task. Such actors are found at different points throughout the executive and legislative branches. For example, Congress, department heads, governors, and the president are all empowered to some degree to create bureaucratic agencies or assign their jurisdictions. Alternatively, P can represent an agency head deciding the number of divisions to assign to a given job. I therefore use the general term ‘agent’ to refer to any unit that is given jurisdiction over the task in question.            "
"9","P chooses n, the number of agents, where 0 ≤n≤N, but faces a moral hazard problem because she cannot dictate their policy choices directly. Each agent Ai shares jurisdiction over the task in question and can independently set an unobservable effort level or policy, φi∈[0, 1]. Denote the n‐element vector of policies φ. The order in which agents are created is fixed, with A1 first, A2 second, etc. Therefore, P cannot give A2 jurisdiction over the policy if n= 1. This may reflect the possibility that an “incumbent” agent already exists for the given task.            "
"10","All players are interested in an observable outcome x∈{0, 1} (where x= 1 corresponds to the “good” outcome of the previous section). Outcomes are determined as follows. Each Ai's policy results in a success with probability φi, and a failure otherwise. If any agent succeeds, then x= 1; otherwise, x= 0. Thus,               "
"11","Payoffs  All preferences are common knowledge. P has linear preferences over policy outcomes, receiving x, and therefore desires policies that maximize the likelihood that x= 1. P additionally pays a cost k≥ 0 for each agent. This represents a fixed level of resources (or “budget”) that must be committed before an agent can participate in policy making.3 Thus, P's expected utility is:               "
"12","Each Ai also has linear preferences over outcomes, receiving rix, where ri∈ℜ. Additionally, setting policy incurs a quadratic cost , which captures the effect of decreasing returns.3 Unless otherwise noted, I assume throughout that ri≥rj for all i < j, so that sequentially prior agencies desire the highest effort or policy levels. As will be clear in the analysis below, this is exactly the sequence that P would choose if it could. Ai's expected utility is then:               "
"13","Sequence  Game play proceeds as follows.            "
"14","                              "
"15"," Agent Selection. P chooses the number of agents n.                     "
"16"," Policy‐Making. Each agent Ai simultaneously chooses an unobservable policy φi(n).                     "
"17"," Policy Outcome. Nature randomly determines outcome x according to (1).                     "
"18","A subgame‐perfect Nash equilibrium of this game consists of an optimal number of agents, n*, and N n‐vectors (n= 1, …, N) of policies, each denoted φ*(n). When P is indifferent between different values of n, it is assumed that the tie is broken in favor of the lowest number of agents. Denote Ai's optimal policy choice in the n‐agent subgame φ*i(n), and let Φ*(n) represent the equilibrium probability that x= 1 in that subgame.         "
"19","To build some intuition I begin by examining the reduced game where N= 2. In this case I also relax the assumption that r1≥r2, so that second agent can be less “friendly” to P's interests than the first. Thus, the case corresponds well to situations in which principals face short‐term constraints on creating or choosing agents.            "
"20","Policy Choice  Solving backwards, when n= 1, A1 simply balances marginal policy utility and marginal cost. Differentiating (2), its ideal policy is:                  "
"21","Now consider the subgame in which n= 2. It will be convenient to denote with a subscript −i parameters belonging to the agent that is not Ai. Then since vi(·) is concave, the following first‐order condition is sufficient to characterize Ai's best response:                  "
"22","Comment 1:  Policy with Redundant Agents. If N= 2, the equilibrium policies are:                  "
"23","Proof:  Proofs of all comments and propositions are in the Appendix.          ▪               "
"24","While this equilibrium is not unique, it is very nearly so. When ri≥ 1 for both agents (with the inequality strict for at least one), there are two equilibria: φ*1(2) = 1 and φ*2(2) = 0, and φ*1(2) = 0 and φ*2(2) = 1. To maintain continuity with the n= 1 subgame, I simply impose the former as the solution.3"
"25","As Table 1 illustrates, the comparative statics of these subgames behave in a sensible manner. When policy choices are interior, Ai's policy is increasing in ri and decreasing in the other agent's marginal utility for a good outcome (r−i). The addition of A2 also weakly reduces A1's policy choice.3 Thus, to some degree increasing the number of agents introduces a collective‐action problem. But this reduction does not necessarily reduce the probability that x= 1. The following comment characterizes the effect of increasing the number of agents on Φ*(n).               "
"26","Comment 2:  Redundancy and Effectiveness. Φ*(2) < Φ*(1) if and only if  and .          ▪               "
"27","To understand this result, note that the addition of A2 always weakly reduces A1's incentive to produce policy. Because of increasing marginal costs, this reduction is most acute when A1 would choose a “high” policy. Further, when r2 is sufficiently low A2's success probability will be too small to offset the decrease in A1's effort. Thus, as case (c) of Table 1 illustrates, the collective action problem has a particularly serious bite when the agents' policy preferences are far apart.               "
"28","The Optimal Number of Agents  Given the policy responses, P simply chooses n to maximize the Pr{x= 1}, net of the costs of adding new agents, k. P will thus prefer a redundant system to a nonredundant one if k≤Φ*(2) −Φ*(1). Returning to case (a) of Table 1, the marginal benefit of adding A2 is 0.124. Thus if k < 0.124, P chooses two agents, and if k∈[0.124, 0.55), P chooses one. In case (b), the lower value of r2 decreases the marginal value of the second agent, and thus redundant agents are chosen only if k < 0.022.               "
"29","Clearly, a redundant system would not be chosen if k≥ 0.5. Nor would it be chosen if it would reducePr{x= 1}. Comment 2 is therefore a special case (where k= 0) of the following result, which characterizes n*.               "
"30","Proposition 1:  If N= 2,                  "
"31","Thus, n* depends on the relative effectiveness of the agents and is decreasing in k. Interestingly, there are two cases in which n* does not decrease from 2 to 1 to 0 as k increases. First, interval (iii) is empty when φ*1(2) + (1 −φ*1(2)) φ*2(2) < φ*1(1); i.e., when two agents do worse than one (see Comment 2). Second, interval (ii) may also be empty if A1 is an “unfriendly” agent (i.e., r1 is low). In this event, P chooses two agents if r2 is sufficiently high relative to k, and none otherwise.               "
"32","An alternative way of stating Proposition 1 would be with respect to ri. This would be considerably more complicated because there are numerous corner conditions to be taken into account. However, to get a sense of the comparative statics with respect to these parameters, Figure 2 illustrates the relationship between r1 and n* for r2∈ (0, 1).               "
"33"," Number of Agencies as a Function of k and                                                                                                   r                                    1 (N= 2)                                                             "
"34","In the figure, the solid lines represent the cost thresholds that make P indifferent between two values of n. It is straightforward to verify in general that these cutlines intersect at some . These lines are gray where the cutpoint is irrelevant to P's decision. For example, for , the two cutlines determining whether P prefers one agent to zero or two are irrelevant, because one agent is P's least‐preferred option (this situation corresponds to the case where interval (ii) in Proposition 1 is empty). The figure also indicates that the equilibrium relationship between agent preferences and n* is not always monotonic. An increase in r1 often makes one agent more desirable, and it is easily shown that for r1 sufficiently high, n*= 1. But for some moderate values of k, n* can increase from zero to two and then decrease to one as r1 increases. Thus the following, somewhat rougher monotonicity conditions hold: A1 will be part of a redundant system only if it is sufficiently “unfriendly,” and it will not be part of a redundant system if it is sufficiently “friendly.”               "
"35","I now establish the principal results of the general game, where N is arbitrary (and possibly infinite).            "
"36","Policy Choice  Solving backwards, since vi(·) is concave, the following first‐order condition characterizes Ai's best response:                  "
"37"," Game Theoretic and Classical Redundancy                            "
"38","Closed forms for equilibrium policies are considerably more difficult to derive when n > 2, since the system of equations defined by (5) is nonlinear. However, the main results of the model do not require such a characterization, and many of the comparative statics carry over directly from the n= 2 subgame. The following comment establishes some of these.               "
"39","Comment 3:  Equilibrium Characteristics.               "
"40","                                    "
"41","(Uniqueness) The subgame‐perfect Nash equilibrium is unique."
"42","(A1's policy) φ*1(n) is decreasing in n and increasing in r1 for r1∈[0, 1].3"
"43","(System effectiveness and r1) Φ*(n) is increasing in r1 for r1∈[0, 1].3          ▪                        "
"44","Part (i) assumes (as in the N= 2 case) that the agents coordinate on the equilibrium where A1 chooses φ*1(n) = 1 when ri≥ 1 for more than one agent. Part (ii) can be extended easily (if tediously) to show that φ*i(n) is increasing ri for all Ai.               "
"45"," Table 2 provides examples of equilibria for eight multiple‐agent subgames, corresponding to the n≤ 4 cases of the models in Figure 3. It confirms Comment 3 and suggests a few other intuitive comparative statics. Within and across equilibria, policies are increasing in ri. Additionally, the effect of each additional agent on Φ*(n) tends to be diminishing. As a result, redundancy decreases “policy efficiency” from the principal's perspective (as measured by Φ*(n) /nk) but increases it from the agents' (as measured by Φ*(n) /∑ni=1c(φ*i(n))). This happens because new agents usually force incumbent agents to choose collectively cheaper policies. In Table 2, for example, total policy costs decrease as n increases, even while Φ*(n) rises for n≥ 3.               "
"46","Model 1 of Table 2 and Figure 3 also illustrate two counterintuitive phenomena that are central to the main results of this section. First, Φ*(n) is not necessarily monotonic in n. In the table and “Model 1” of Figure 3, the addition of A2 and A3 actually reduces Φ*(n), and Φ*(n) < Φ*(1) for n≤ 10. Second, while A1's policy is always decreasing in n, other agents' efforts are not necessarily so. That is, in some cases agents other than A1 face less of a collective action problem—and thus increase their effort—as n increases. As the table suggests, these phenomena are related in part to high effort levels by A1. When φ*1(n) ≤ 0.5, new agents drive all of the incumbents' effort levels down, but the probability of a good outcome increases.               "
"47","The following proposition, which is a generalization of sorts of Comment 2, ties these observations into the central result on system effectiveness. Φ*(n) is decreasing in n until A1 chooses a policy at or below 0.5, after which it is increasing. Consequently, the probability that x= 1 is maximized either at n= 1 or n=N.               "
"48","Proposition 2:  Nonmonotonicity of System Effectiveness. Φ*(n) < Φ*(n− 1) for  and Φ*(n+ 1) ≥Φ*(n) otherwise, where                  "
"49","The primary import of Proposition 2 is that collective action problems are especially harsh when one agent (necessarily A1) is “friendly,” in the sense that r1 > 0.5. To see why, it is helpful to think in terms of the dynamics as the n+ 1‐th agent is added to an n‐agent equilibrium. Since A1 chooses the highest policy, it stands to gain the most by reducing its effort in response to a new agent. Agents A2‐An compensate for this reduction by increasing their effort. For , these adjustments plus the contribution of An+ 1 do not offset the reduction in A1's effort. But beyond , A1's policy (now less than 0.5) is relatively cheap at the margin, causing it to shirk less. Consequently, the new agent's contribution more than compensates for the incumbent agents' reduced efforts.               "
"50","The Optimal Number of Agents  P's optimal choice of the number of agents is:                  "
"51","Because Φ*(n) may be nonmonotonic, n* depends on . In the simplest case, if , then Φ*(n) is increasing n. The relation between Φ*(n) and n is roughly “concave” (especially for large n), so P approximately balances marginal benefit with marginal cost. If , then as in Model 1 in Table 2 a nonredundant system becomes more attractive because the marginal benefit of the first few agents after A1 is negative. Proposition 3 uses these facts to establish the main result of the direct redundancy model, which relates n*, ri, and k.               "
"52","Proposition 3:  Optimal Redundancy.               "
"53","                                    "
"54"," n* is nonincreasing in k.                        "
"55","If , and  or , then n*= 1.                        "
"56","If  and r2 > 0, then n* > 1 for k sufficiently low.          ▪                        "
"57","Part (i) of the result generalizes Proposition 1 in a straightforward way: extra agents are less appealing as their costs increase. But part (ii) shows that if A1 is “friendly” and the other agents are collectively unfriendly, then n*= 1regardless of k. This is a direct consequence of a high value of , as no number of new agents can compensate for the losses imposed by agents prior to . Finally, part (iii) provides some conditions under which low costs do matter—i.e., when agents are unfriendly and . This is illustrated by Model 2 in Table 2, where (approximately) n*= 4 for k < 0.027, n*= 3 for k∈[0.027, 0.052), n*= 2 for k∈[0.052, 0.056), and n*= 1 for k∈[0.056, 0.5).               "
"58","These results contrast usefully with those of the non‐game theoretic formulation of redundancy. In the latter, new agents always raise the probability of a good outcome and redundancy is strictly decreasing in k. But in the game studied here, this is true only when there is no agent inclined to choose a high policy or effort level. Otherwise, redundant agents may contribute little if anything to the probability of success. Thus, despite the greater difficulty in deriving closed form solutions when n > 2, the primary intuition about the desirability of redundant systems remains the same.               "
"59","In the previous section the principal had no leverage over agents other than the (limited) option to give them common jurisdiction over the task. However, in many cases her leverage is likely to be considerably greater. Typically principals are in a position to assess performance over time and may also be able to terminate the jurisdictions of agencies or wayward bureaucrats.3 The latent redundancy model shows how this feature may affect bureaucratic effectiveness with a repeated game in which P faces a single, replaceable agent in each period. The results are suggestive in nature, as a more complete analysis would give P the ability to add or terminate multiple agents in each period.3"
"60","As a baseline for comparison, consider the following repeated variant of the direct redundancy game. P chooses a set of irreplaceable agents to open the game. In each period, P pays the cost k for each agent, the agents choose policy, and Nature reveals outcomes. Because P cannot replace agents, it is easy to see that—regardless of whether the game is finitely or infinitely repeated—she would choose n*, and in each period Ai would choose φ*i(n*). Thus the equilibrium for this repeated game is identical to that of the single‐period direct redundancy game.         "
"61","The latent redundancy game is infinitely repeated, with future payoffs discounted by a common factor δ∈ (0, 1). Each period is identical to the n= 1 subgame of the direct redundancy game, except in the following respect: at the beginning of each period, P may terminate the incumbent agent and replace it with a new one of her choosing. So as not to “rig” the results excessively in favor of superior agent performance, I assume that agents face no costs from being terminated.3 As before, each agent costs k in each period. Once terminated, an agent cannot return to the game. P may therefore potentially induce better policy performance by making her future choice of agents contingent upon observed performance.3"
"62","To maintain comparability with the direct redundancy game, I assume that the set of replacement agents in each period is time‐invariant, with preferences identical to agents {A 1, …, A N} in the direct redundancy game. This reflects the possibility that P has relatively little control over the formation of agent preferences; for example, a newly promoted division chief might have the same professional background as her predecessor.3"
"63","There are many Nash equilibria of this game. I therefore restrict attention to a general class of intuitive, sequentially rational equilibria. The class is defined by two rules which are defined formally in the appendix. First, there is a termination rule, such that P tolerates a fixed number of failures from an incumbent agent of each “type” (defined by ri) before terminating it and choosing a new one. Second, the replacement rule is a sequence specifying a deterministic order in which new agent types are selected. These two simple rules thereby capture a wide range in both the requirements for termination (including the ability to discriminate across agent types), as well as the choice of replacements. If, out of equilibrium, P does not terminate the agent or chooses the wrong agent, the agent simply chooses its one‐shot best policy (i.e., ri) in each period for as long as it is not replaced.            "
"64","As an example of such an equilibrium, suppose that P always chooses type‐A1 agents, which are terminated after each period (i.e., after zero failures). Since that agent has no control over its future, it simply chooses its one‐shot best response. P never chooses a type‐A2 agent because its one‐shot best response is not as good as A1's, although she is indifferent in equilibrium between terminating an agent and continuing with it."
"65","The results of this section focus on the equilibrium that is optimal for P. Aside from being a natural focal point, this equilibrium accords intuitively with the role that principals may play in institutional design. Since principals are to some degree responsible for establishing “rules” for subordinates, it is reasonable to conjecture that P would be the player most able to coordinate play on a particular equilibrium.3"
"66","Proposition 4 derives policy choices in the optimal equilibrium. Let φiequation/tex2gif-sup-29.gif represent a type‐Ai agent's optimal policy choice given that it can have Mi failures before termination. In this equilibrium, type‐A1 agents (whose preferences most closely aligned with P) are always chosen, and these agents are subjected to a harsh termination rule, where a single failure results in its replacement (i.e., M1= 1). The result is intuitive because A1‐type agents are the most willing to choose “high” policies, and harsh termination rules maximize their incentives to reduce the risk of failure. Crucial to the result is P's ability to draw a fresh A1‐type agent costlessly after each termination. Changing this assumption would cause P to prefer more forgiving termination rules.            "
"67","Proposition 4:  Policy Under Latent Redundancy. In the equilibrium with the optimal termination rule, policy in all periods is:                  "
"68","Clearly, φ1*1≥φ*1(1), with the inequality strict when r1 < 1. Thus, the ability to terminate agents raises policy and thereby limits the extent to which additional agents will be desired. This result goes much further, however. For δ sufficiently high, the threshold for the agent to choose a policy of 1—in other words, to do exactly as P wants—is quite low. In the direct redundancy game, φ*1(1) = 1 for r1≥ 1, but in the latent redundancy game, φ1*1= 1 if . Alternatively, if , then for any δ≥ 2(1 −r1), P can attain her ideal outcome with only a single agent.               "
"69","In the previous sections, the assumed policy technology required only one success for a good outcome to result. In many applications, however, more than a single success is required: airliners may require more than one working engine to fly, and drug interdiction requires the effective collaboration of multiple agencies. To use the example of the second section, one can imagine that because of resource or jurisdictional constraints, no agency can uncover all of the required information unilaterally, but that a combination of two effective agencies can. This type of system is easily incorporated into the framework developed in the fourth section."
"70","Let the minimum number of successes required to achieve x= 1 be s≥ 1. Such systems are often referred to as “s×n” or “s‐by‐n” systems. Clearly, the number of agents n(n≤N) must be at least s for x= 1 to result. In the simplest case, where the reliability r is constant across agents, the probability of a good outcome in a nonstrategic environment is . As in the direct redundancy game (where s= 1), raising the number of agents strictly increases the probability that x= 1.            "
"71","In the strategic environment studied here, reliability rates will continue to vary across agents, and the probability of each outcome will depend on agents' policy choices in more complex ways than previously. As a result, the probability of success is less straightforward to calculate than in the previous sections. To begin, it will be convenient to define the probability that exactly q agents other than Ai succeed as follows:               "
"72","Policy Choice  Clearly, μq−i(φ) does not depend on φi, and as a result Ai can only affect the outcome if exactly s− 1 agents succeed. Ai's best response thus generalizes from (5) as follows:                  "
"73","The Optimal Number of Agents  While a closed‐form solution for the optimal number of agents, ns*, is clearly more difficult to derive than in the direct redundancy game, the following partial characterization is easily demonstrated.               "
"74","Proposition 5:  Necessary Redundancy. If ri < 1 for all Ai(i≤N), then ns*= 0 or ns* > s.          ▪               "
"75","This result essentially reverses the central intuition of the direct redundancy model. Just as agents create negative externalities (by the collective action problem) when s= 1, agents can create positive externalities when s > 1. When n=s, all externalities are positive, because any effort is wasted unless all other agents also contribute. Thus, best responses are increasing (and linear) in other agents' efforts. This is easily seen in equations (6) and (7), as the last product in (6) is empty for n=s.               "
"76","The proof uses these facts to show that if no agent would ever be willing to choose a policy of 1 (i.e., ri < 1), then all agents must choose 0. At an equilibrium, P therefore chooses either ns*= 0 or, if k is sufficiently low, some ns* strictly greater than the minimum technically necessary. An easily proved corollary of this result is that if all agents are perfectly reliable (ri= 1), then Pr{x= 1}= 1 when n=s, and thus ns*=s if , and ns*= 0 otherwise.               "
"77"," Table 3 provides an example of the interplay of positive and negative externalities in some 2 ×n systems. Since ri < 1 for all Ai, agents choose zero policies when n= 2. When n increases to 3 and then to 4, agents individually choose higher policies because of the positive externalities induced by the new agents. When A5 is introduced, however, the number of agents becomes large relative to s, and the probability that only a single agent succeeds (μ1−i(φ)) drops, while the probability that each agent's effort will be unnecessary rises. As in the direct redundancy game, this creates negative externalities. Thus, the individual efforts of A1–A4 are lowered, but the collective reliability of the system still increases.               "
"78","Up to this point it has been assumed that the policies chosen by agents did not impose any technical interdependencies or externalities on the effectiveness of other agents' actions. Externalities were exclusively generated endogenously, by the agents' strategic incentives. As normal accident theory points out, however, technical externalities may be common in more complex task environments. Here I explore some implications of these externalities for redundant bureaucratic structures.            "
"79","For simplicity, I return to the case examined in the fourth section, where N= 2 and the assumption that r1≥r2 is relaxed. Outcomes are determined in the same way as in the direct redundancy model, but with the generalization that the effectiveness of each agent may now depend on the effort of the other, as follows. The vector of success probabilities is given by π=φ·Ψ, where Ψ is an n×n matrix with elements ψji∈ℜ. Ai's probability of succeeding is thus πi (as opposed to φi), where:               "
"80","Accordingly, Pr{x= 1}= 1 −∏j=1n (1 −πj). P's expected utility is thus given by:               "
"81","I refer to Ψ as the weighting matrix. This matrix operationalizes the externalities that agents' activities may impose on one another, thus causing success probabilities to be nonindependent. If Ψ is the identity matrix, I, then π=φ. Agents' success probabilities are then independent, and the model is identical to the direct redundancy game. But in contrast with equation (3), a nontrivial weighting matrix may force an agent to consider the consequences of other agents' policy choices on its outcomes and to consider the impact of its choice on other agents' outcomes. For example, if r2 > 0 and ψ21 > 0, then A2 has an incentive to increase its policy choice compared to a situation in which ψ21= 0. This happens because A2's effort will help A1 to succeed, thus increasing the marginal value of A2's effort. Similarly, if ψji < 0, then any φj > 0 reduces the probability that Ai succeeds.            "
"82","Policy Choice  Solving backwards, if n= 1 then there are no externalities and policy is as determined in the fourth section. If n= 2, then since Ai's objective remains concave, the following first‐order condition is sufficient to characterize Ai's equilibrium policy at an interior solution:                  "
"83","Note that the conditions under which a corner solution occurs are no longer trivial. Even if ri < 0, it is possible that φ*i(2) > 0 because Ai may wish to impose a negative externality on Aj's (j≠i) effort.               "
"84","The effects of the weighting matrix on the comparative statics of the n= 2 subgame are generally straightforward. If Ψ is symmetric and agents have identical preferences and costs, then equilibrium policies are identical. Generally, for weighting matrices “close” to I, the comparative statics resemble those of the direct redundancy game. As Ψ diverges from I, however, agent incentives become increasingly distorted. Consider the example in Table 4, where all cost and utility parameters are as in Table 1. Since ψ11= 0.6 and ψ21= 0.4, A2 has almost as much control over A1's output as A1 itself. Thus A2 chooses a much higher policy than in Table 1, while A1 chooses a lower policy.               "
"85","The Optimal Number of Agents  P's decision is essentially similar to that of the direct redundancy game, in that Proposition 1 holds. However, when Ψ≠I, the marginal value of the second agent, Φ*(2) −Φ*(1), is different. In case (a) of Table 4, P chooses to have two agents if k < 0.194, as opposed to k < 0.124 in case (a) of Table 1. Adding A2 increases Φ*(n) more in Table 4 because ψ21 > 0 and ψ22= 1, thus raising the return to effort of an agent that already desires a high policy. A1's effort is correspondingly reduced, but not by enough to reduce Φ*(n) overall. A similar analysis holds for case (b) in both tables. By contrast, in case (c), A1 is the high demander of policy, and its incentive to produce is inhibited by the low values of ψ11 and ψ12. Consequently, P does worse with two agents here than it did without externalities.               "
"86","While the models of bureaucratic politics in this article are quite simple, they predict the assignment of agents to tasks as a function of some readily measurable variables. In particular, two tests of the relationship between Congress and federal bureaucracies would complement the many cases examined thus far in the literature. First, the direct redundancy model predicts that redundant structures will be chosen more often as agency preferences coincide less with Congress'.3 Measures of agency preferences can be constructed in two ways. In policy areas where political appointees play an important role and preferences have a “spatial” component, data on the preferences of presidents or enacting coalitions (for agencies that are relatively “insulated,” such as independent commissions) may be used. In other areas, survey data of career officials will be more appropriate.            "
"87","Second, the latent redundancy model additionally predicts that redundant structures will be more prevalent as the ability of Congress (or the President) to affect personnel decreases. The substitutability of personnel within an agency could be measured by the extent to which career or political appointees staff the organization's leadership. Alternately, it could be measured by the level of specialization or expertise required for its personnel, which may be crudely estimated from the distribution of civil service ranks within the agency."
"88","For both types of tests, the onset of civil service reform suggests some interesting possibilities. In the U.S., the 1883 Pendleton Act initiated an extensive transition from political to career appointees in executive agencies. The law shifted both bureaucrats' preferences and the ability of principals to replace them. Both the direct and latent redundancy models make predictions about the reallocation of tasks that should result."
"89","These tests can also be performed at the intra‐agency level. For example, agencies like the FDA frequently consult a variable number of advisory boards before making decisions. Managers in law enforcement agencies routinely confront choices over the number of divisions that will be delegated a task. These and other questions about the design of reliable organizations can be subjected to systematic empirical scrutiny with this model or its extensions as a basis."
"90","Theories in the wake of Landau's contribution to bureaucratic design and performance have steadily formalized and expanded upon the original model. The model presented here makes two additional contributions. First, consistent with more recent theoretical work in bureaucratic politics, it explicitly casts agents as strategic actors with preferences over policy outcomes. The primary insight for organizational design is that strategic interdependencies can play an important role, even in an environment with no technical interdependencies. Second, it examines a set of common policy environments that differ from the classical setting. Among these are s×n systems and settings where agents face the risk of termination. The resulting models therefore capture a variety of important features shaping the interaction of agents with political principals.            "
"91","The incentives posed by the strategic environment suggest some serious limits on the amount of redundancy principals desire, when compared to its nonstrategic counterpart. Redundant structures tend to help most when the set of agents available for a task are relatively “unfriendly,” or disinclined to choose policies that P would like. Here the collective action problems are not serious enough to hurt aggregate policy production, which is increasing in n. But if P has access to a friendly agent, then adding agents will tend not to help performance. As Proposition 2 establishes, policy production will first decrease, and then increase, as n rises. This effect greatly reduces the average value of new agents and moreover may make a single agent optimal in environments where n is constrained to be small. Finally, if the principal can terminate agents for poor outcomes, she can achieve her ideal outcome with only moderately friendly agents if the discount factor is sufficiently high. Simply stated, principals can make agents compete against themselves with relative ease.            "
"92","It is useful to reconsider the link between redundancy and normal accident theories in light of these results. By developing a theory of strategic interdependencies among system components, the models developed here begin to bridge the two. As the example of the second section first illustrated, the basic intuitions of both theories may be correct, depending on the parameter values assumed. But despite the constant presence of interactive complexity—arising from either strategic interaction or policy externalities—redundancy is not always rendered undesirable. In fact, redundancy becomes necessary when the agents' interaction creates positive externalities, as in s×n systems.            "
"93","While the models developed here were intended to capture the essential features of redundancy in a parsimonious manner, they can be usefully extended in two directions. First, the theory should be generalized to a broader range of policy problems by encompassing different policy technologies. Two examples suggested by the literature are “three‐state” outcomes that incorporate Type I and Type II errors, and the problem of agent sabotage. Other extensions might examine policy technologies used in the economic literature on collective action.3"
"94","Second, the theory can potentially also speak to other aspects of bureaucratic politics. One example is the issue of agent incentives raised by the latent redundancy model. In many contexts, principals might assume a more active role in structuring these incentives. Within an agency, this might entail the design of employment contracts for bureaucrats. In some contexts, principals may also encounter common agency problems (e.g., Dixit 1995; Gailmard 2002b). Another example is the delegation of authority to agents, which is greatly simplified by the present models. The decision to delegate may depend on the agents' private information, thus shifting the problem from moral hazard to adverse selection. Finally, the occasional charges of wasteful redundancy suggest a range of questions about budgets and efficiency. Instead of a fixed cost for each agent, the budget may be modeled as a choice that limits each agent's feasible policy set (e.g. Silver 1996; Ting 2001). In equilibrium, the budget would be tied to the principal's anticipation of the agent's costs. The principal would then bear the marginal costs of policy and also care explicitly about policy efficiency.            "
"95","The models developed here therefore move the foundations of redundancy theory from reliability engineering to game theory. In so doing, they link redundancy with modern theories of bureaucratic politics and collective action, and furthermore establish a framework for considering a variety of new issues in organizational design."
