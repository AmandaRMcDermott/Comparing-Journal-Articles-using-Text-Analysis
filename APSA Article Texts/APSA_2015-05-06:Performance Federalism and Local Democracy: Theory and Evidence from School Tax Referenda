"","x"
"1","America's federal system is increasingly marked by the intermingling of policy authority and fiscal resources between different levels of government (Walker 1995). Scholars have offered a number of explanations for growing federal involvement in functions traditionally performed by state and local governments, including federal officials' desire to take credit for the provision of popular public services (Volden 2005) and efforts to overcome the institutional collective action problems that plague state and local policymaking on issues ranging from environmental protection to economic redistribution (Oates 1972; Peterson and Paul 1989).         "
"2","But federal policy makers face significant principal‐agent problems as they seek to influence public programs administered at the state and local levels (Scholz and Wei 1986). Merely outlining the goals that state and local actors ought to pursue as they spend federal funds is unlikely to elicit maximal effort toward realizing those goals. State and local agents possess significant expertise that federal officials wish to harness, but the discretion that these agents need in order to mobilize their expertise may also enable them to pursue their own divergent objectives. In an effort to address this problem, federal principals have increasingly sought to measure the extent to which state and local actors—both elected and appointed—realize federal policy goals. Such a strategy can, in theory, enable the federal government to design mechanisms to incentivize state and local agents to pursue federal policy goals without binding them with discretion‐limiting regulations (Holmstrom 1979). This institutional logic, inspired by the economics of organization and often promoted under the banner of “new public management,” has motivated sweeping administrative reforms across the globe (Pollitt and Bouckaert 2004). And dramatic improvements in our ability to collect and analyze data increasingly make such performance federalism a viable strategy.         "
"3","The devolution of federal authority, growing interrelationships between levels of government, and adoption of performance management approaches have all attracted substantial attention from the scholarly community (e.g., Conlan 1998; Heinrich and Marschke 2010), but their collective implications for local democracy have not been explored. Incentive mechanisms tied to performance measures can motivate lower‐level actors to pursue federal policy goals, but such metrics may also create significant informational spillovers that reach ordinary voters. Indeed, performance measures collected for purposes of federal oversight can influence voter behavior in state and local elections in unintended ways that may actually undermine federal objectives. Such consequences are particularly likely in American local elections because they are often characterized by low levels of voter engagement and the absence of information about the performance of incumbent officials and public agencies.         "
"4","While some local contests feature lively campaigns, result in substantial spending, and attract media attention, the vast majority do not. In this information‐poor context, voters weighing down‐ballot races must either invest substantial time and personal effort to investigate local issues and candidates—which, rationally, few do (Downs 1957)—or turn to heuristics and cues (Lau and Redlawsk 2001). Because the vast majority of local elections in the United States are nonpartisan, voters lack easy access to partisan labels that prove so informative in state and federal races. Many instead turn to alternative cues, often relying on candidate race and ethnicity (Lorinskas, Hawkins, and Edwards 1969; Matson and Fine 2006), incumbency (Schaffner, Streb, and Wright 2001), occupation (McDermott 2005), and other shortcuts. Shared policy authority among multiple actors within and between levels of government also complicates the cognitive task of evaluating government performance by requiring voters to correctly apportion credit and blame for observed policy outcomes (Brown 2010; Malhotra and Kuo 2008).         "
"5","In principle, federally imposed performance measures can improve democratic accountability by clarifying responsibility and generating widely disseminated, easy‐to‐interpret benchmarks that voters can use as the basis for retrospective evaluations (Berry and Howell 2007). In practice, however, performance measures can also impede accountability at the local level if such measures are poorly designed or misinterpreted and misapplied by voters. As we demonstrate with an examination of the performance measurement system required by the No Child Left Behind Act, this possibility indeed became a reality in U.S. public education.         "
"6","The provision of K–12 public education has historically been a local responsibility in the United States, with increasing state and, later, federal participation over the past five decades. By the 1980s and 1990s, amid growing concern that poor school performance undermined America's economic prosperity and global competitiveness, federal policy makers began to demand more rigorous assessment and reporting of student educational outcomes (Manna 2011, 7–9). In 2001, federal lawmakers seeking to hold states and local school districts accountable for eliminating achievement gaps and raising overall educational performance triumphed with the No Child Left Behind Act. Though the federal share of public education spending climbed only from around 7% to around 10% of total K–12 spending, the accountability system that NCLB imposed as a condition for receiving this money marked perhaps the largest increase in the federal role in public education since the 1960s (Manna 2006).         "
"7","Specifically, NCLB required states to set annual performance targets that increased steadily through the 2013–14 school year, by which time the law required 100% of students to be “proficient” in mathematics and reading. Schools and districts whose students met the yearly proficiency rate targets would be labeled as having made “adequate yearly progress” toward full proficiency by 2014. NCLB also required states to develop academic content standards and tests aligned to those standards so that student progress could be tracked. For purposes of determining whether individual schools and districts made AYP, districts had to administer mathematics and reading tests in grades 3–8 and once in high school. For schools and districts to satisfy the AYP standard in a given year, every student subgroup (according to race, gender, economic disadvantage, disability, and English‐language status) had to meet the AYP proficiency rate target for that year.3"
"8","Unlike previous federal education reforms, NCLB featured strict enforcement mechanisms to incentivize schools and districts to meet the escalating proficiency requirements. First, the law required states to publicize school and district AYP designations (“met” or “not met”) on report cards released just prior to the following school year. The federal government also required districts to directly notify parents whose children attended schools or districts that failed to meet the AYP performance requirement for two consecutive years. These requirements meant that many educational stakeholders received a signal of school and district quality based on the dichotomous AYP designation. Second, NCLB required invasive administrative interventions when schools or districts failed to meet AYP proficiency targets for at least two years in a row.4"
"9","The implementation of NCLB performance standards resulted in significant variation over time and across states in AYP performance targets and the resulting measures of student performance (Davidson et al. 2013). One reason is that the U.S. Department of Education allowed states to alter over time how they calculated AYP designations and the conditions under which administrative interventions were required. Another reason for the variation is that states possessed significant discretion in measuring and defining student educational performance. For example, states set their own cut scores for identifying “proficient” students on state tests, which resulted in many states relaxing their proficiency requirements in order to help schools and districts avoid failing AYP. Moreover, working within general federal guidelines, states set their own yearly AYP proficiency schedule. Many states, for example, back‐loaded dramatic increases in the proficiency rate required to meet AYP to the last few years leading up to 2014, in the hope that, before that time, federal policy makers would relax the requirement of 100% proficiency or lessen sanctions associated with failing to meet it.            "
"10","These features of NCLB implementation led to dramatic year‐to‐year and within‐state volatility in the performance standards to which schools and districts were held accountable. Figure 1 illustrates the volatile AYP proficiency requirements in Ohio, the focus of our empirical analysis. The figure plots the math and reading proficiency rates for Ohio students from 2004 through 2011, and it juxtaposes these proficiency rates with the climbing proficiency target required for making AYP along with the overall rate at which districts failed to make AYP. The figure also plots the reputable National Assessment of Education Progress (NAEP) proficiency rates in the same subject areas. As the figure indicates, between 2004 and 2005—when AYP sanctions were set to kick in for schools that failed AYP two years in a row—Ohio adjusted its proficiency cut score from one that identified about 35% of students as proficient (similar to the NAEP level) to one that identified about three‐quarters of students as proficient. After that, however, the overall proficiency rate used to determine AYP held steady.            "
"11","Trends in Proficiency Rates, AYP Proficiency Targets, and AYP Failure, 2004–11"
"12","Note: The Ohio proficiency rate tracks the proportion of students meeting the state's cutoff for proficiency in both math and reading. The NAEP proficiency rate is the average for both math and reading for fourth‐ and eighth‐grade students. The AYP proficiency requirement is the aggregate required proficiency rate, which is created by weighting the proficiency targets across different grades and tests. The district AYP failure rate reflects the proportion of districts that failed AYP.                        "
"13","The figure also documents how, in accordance with federal requirements, the AYP proficiency rate targets increased over time. For example, a district with 65% of students demonstrating proficiency in reading would meet the district‐wide AYP goal in 2005 but would fall short with identical performance in 2006. Yet, while initial increases in the required proficiency rates corresponded to an increasing number of districts falling short of AYP, the failure rate actually decreased dramatically in 2008 in spite of a growing proficiency target. The central reason for this divergence appears to be that Ohio was granted an exemption that enabled schools and districts to meet AYP requirements if the state projected that students would meet proficiency requirements down the road. In other words, tinkering and politically motivated schedules for increasing proficiency targets led to unpredictable variability in the rate of AYP failure over time. This volatility in district AYP designations occurred in spite of relatively constant levels of actual student proficiency as documented by scores on separate NAEP exams. Schools and districts moved in and out of their AYP “not met” designations for reasons that were difficult to anticipate and that were largely unrelated to changes in their performance in educating students.5"
"14","Although undesirable from the point of view of voters who might wish to rely on AYP as an accurate measure of school quality, the arbitrary nature of AYP provides an important source of causal identification in our design. That district AYP designations changed year to year due to changing standards and definitions helps ensure that they are not correlated with time‐varying, difficult‐to‐measure contextual factors that may independently affect voters' subjective evaluations of local schools and that are not captured by district fixed effects. In other words, an inherent problem with the AYP measure, from the point of view of using it as a tool to improve accountability, allows for an ideal empirical test of the impact of federal performance measures on voter behavior in local elections."
"15","In addition to the problems described above, mathematics and reading proficiency rates—the key performance measures used as the basis of NCLB's accountability framework—are themselves invalid measures of school and district performance. To gauge how successful a school or district is in carrying out its mission to educate students, one must measure how much students actually learn during a school year. Proficiency rates, on the other hand, largely capture the aptitude students possess prior to entering school (see Downey, von Hippel, and Hughes 2008). Thus, the most effective schools—those that post the biggest gains in student achievement—may have failed AYP simply because the students they serve started off at a lower base.6 Indeed, across all 50 states, there is a strong positive relationship between AYP failure and the proportion of a school's student population that is economically disadvantaged or that includes members of a racial or ethnic minority group (Davidson et al. 2013). During our period of study, the probability of failing AYP in any given year was more than 0.67 among Ohio districts identified by the state as serving a high number of poor students, compared to 0.42 for wealthier districts. In other words, the risk of failure among high‐poverty districts was nearly 60% greater than in other districts.            "
"16","In the supporting information, we provide direct evidence that higher rates of AYP failure among economically disadvantaged districts were due primarily to lower levels of baseline proficiency attributable to students' socioeconomic status rather than the quality of their schools. One piece of evidence is the relationship between district AYP status and “value‐added” measures of district quality that Ohio introduced into its own separate state accountability system in 2007. The value‐added method compares the gains a district's students made on state exams with those of students in other districts.7 Unlike absolute levels of proficiency, these value‐added scores reward districts for growth in student performance. Districts whose students made lower‐than‐expected gains are labeled “below,” and those whose students made expected gains or significantly exceeded expectations are labeled “met” and “above,” respectively.            "
"17","Figure 2 contrasts district AYP designations with Ohio's value‐added benchmarks for the years 2007–12, the only period for which both measures are available. As the figure illustrates, there is only a weak relationship between whether a district meets federal AYP targets and how much progress its students post on state exams (Goodman‐Kruskal ). Overall, about 19% of districts that met AYP over this period actually received the “below” value‐added designation. By contrast, almost 70% of districts that fell short of AYP nevertheless met or exceeded their value‐added benchmark.            "
"18","In summary, the NCLB measurement approach suffered from three serious shortcomings that undermined its potential for helping voters accurately assess the performance of local school districts. First, the AYP designations were based in large part on arbitrary, changing, or politically determined benchmarks that varied across states and within states over time. Second, even ignoring these limitations, the underlying metric used for calculating AYP designations—absolute levels of student proficiency—was largely determined before students ever set foot in a classroom and only partially (if at all) reflected the effectiveness of their teachers or the quality of their schools."
"19","Nevertheless, the federal benchmarks attracted substantial publicity and attention. That districts and schools failing to make AYP were required to notify parents of their status likely made this a particularly salient consideration in the minds of many voters. Although earlier research found only limited evidence that voters rely on test‐score performance to hold incumbent education officials accountable at the ballot box (Berry and Howell 2007), the prominence of annual AYP designations made it easy for voters to incorporate these signals into their retrospective judgments. Indeed, recent studies have shown strong evidence that voters update their beliefs about school quality in response to changing designations on widely disseminated official school “report cards” (e.g., Chingos, Henderson, and West 2012; Jacobsen, Saultz, and Snyder 2013).            "
"20","Interestingly, it appears that few federal policy makers explicitly considered how the law's reporting requirements would affect local politics. The issue did not come up at the five hearings held on education reform in 2000 and 2001—the congressional session during which NCLB was adopted—including two hearings held after its passage that focused on the law's implementation.8"
"21","We examine how the flawed AYP performance measure affected voter behavior in local elections.9 Specifically, our empirical analysis focuses on property tax levies considered by voters during the period that NCLB was operational in Ohio. Districts place such levies on the ballot regularly to fund their operating or capital needs. In Ohio, funding approved through such levies represents one‐third of local school budgets, just above the national average.10 In addition to being a large state with many, diverse school districts—as well as a population that is demographically similar to the country as a whole—Ohio provides an ideal research setting due to state laws that prevent property taxes from growing automatically when property values increase, ensuring that school levies appear on the ballot quite frequently as district costs rise and new revenues are required. Many local levies also remain operational only during an exogenously determined period of time, requiring districts to return to the voters with replacement levies. These institutional features allow us to rule out the possibility that districts can be too strategic in their timing of levy proposals11 and provide us with a sufficient number of observations (nearly 4,000 levy votes from 2003 to 2012) to estimate within‐district effects over time.         "
"22","There are currently 611 school districts in Ohio, although two additional districts were operational during at least one year in our period of study, 2003–12. The empirical analysis employs data on the 556 districts that placed at least one levy on the ballot during these years. Districts not included in the analysis were primarily those that operate at Ohio's minimum statutory tax rate floor, set at 20 mills.12 Importantly, districts from all of Ohio's major metropolitan areas are included in the analysis.            "
"23","The data employed in the analysis are publicly available and were obtained from the websites of a number of organizations. AYP designations, state performance designations, and student enrollment and demographic characteristics came from the Ohio Department of Education. The variables that capture student demographic characteristics—percent poor, percent black, percent Hispanic, percent English‐language learners, and percent disabled—were created according to federal guidelines established for the purpose of disaggregating academic achievement statistics by student subgroup. District expenditure data were taken from the National Center for Educational Statistics, and the revenue and property values data are from the Ohio Department of Taxation. Details about each levy, including the electoral outcome, were obtained from the Ohio School Boards Association. The remaining variables, including rates of homeownership and the proportion of school‐age students attending private schools, were extracted from the American Community Survey.13"
"24","In addition to student demographic characteristics and several measures of district finances, our full specifications also control for each district's rating under Ohio's own school accountability system. These ratings incorporated a number of measures of student performance, including many of the metrics used to calculate AYP. Starting in 2008, Ohio's rating system also incorporated district value‐added benchmarks. Unlike the dichotomous federal AYP designation, Ohio places districts into one of six categories, ranging from “Academic Emergency” to “Excellent with Distinction.”"
"25","The purpose of including covariates capturing district and community demographics, property values, existing millage rates, and district expenditures is to account for factors that may affect or reflect voters' inclination to approve school levies. The set of covariates we employ is unusually complete, capturing factors known to affect voting in local elections and attitudes toward tax levels. Nevertheless, our preferred specifications also include year and district fixed effects to capture potentially omitted factors, and we provide direct evidence of the causal mechanisms to further rule out the possibility of spurious results.14"
"26","The analysis examines the effect of districts receiving a designation of “AYP not met,” as opposed to “AYP met,” on the probability of levy passage.15 It may seem ambiguous how AYP failure should shape voter support for local school funding. One might expect, for example, that failure would increase the willingness of local taxpayers to fund local schools at higher levels, either as a sincere response meant to improve student outcomes or a self‐interested goal of protecting their property values. Research indicates that voters do not respond to performance information in this way, however. Few voters appear to make a coherent connection between the taxes they pay and the quality of services they expect to receive (e.g., Beck, Rainey, and Traut 1990; Benton and Daley 1992; Lowery and Sigelman 1981). Many, it seems, expect “something for nothing” (Sears and Citrin 1982). Studies consistently show that support for tax increases and other financial contributions is positively associated with citizen satisfaction with existing public services (e.g., Collins and Kim 2009; Figlio and Kenny 2009; Glaser and Hildreth 1999; Simonsen and Robbins 2003). Consequently, we expect that a signal of negative school district performance indicated by AYP failure reduces voter support for proposed school levies.            "
"27","In addition, we hypothesize that the timing of the dissemination of annual AYP designations affects the extent to which voters incorporate this information into their decision making. Both theoretical and empirical literature on voting decisions illustrate how voters' use of performance information is subject to behavioral biases, such as myopia. Recent performance information, such as macroeconomic outcomes in the six months before an election, has a disproportionate effect on electoral outcomes (e.g., Healy and Lenz 2014; Huber, Hill, and Lenz 2012; Kramer 1971). We expect voters to exhibit similar myopia in our local context. In particular, we hypothesize that voters put greater weight on a district's most recent AYP designation and that this information is most electorally relevant when the AYP designations are released in close temporal proximity to the levy election. Empirical support for these hypotheses would also provide some evidence that district AYP status affects levy passage through voter attitudes rather than alternative mechanisms, such as the strategic use of federal performance measures by interest groups campaigning against local tax increases.            "
"28","Finally, we hypothesize that the magnitude of the electorate's response to the first AYP failure is greater than that of subsequent failures. This hypothesis is based on Bayesian learning models of voter behavior (Achen 1992; Gerber and Green 1998).16 These models indicate that initial information shocks produce a larger shift in beliefs than additional information reinforcing these messages.            "
"29","In the body of the article, we examine the empirical relationship between AYP failure and the probability of levy passage.17 Our basic specification models levy passage probability as a linear function of AYP failure, a large battery of district covariates, and year fixed effects, . The year fixed effects allow statewide shocks in voter willingness to support taxes to evolve nonparametrically over the course of our sample period. Formally, our base specification is               "
"30","In addition to our rich set of district covariates that include demographic, economic, and educational data for all districts, we further account for unobserved time‐invariant school district characteristics by estimating district fixed‐effects models. The fixed‐effects specifications can be written as the linear model               "
"31","where  are the district fixed effects. To remove the  nuisance parameters from the estimation, we transform the data through demeaning and estimate an ordinary least squares (OLS) regression on the demeaned data. The fixed‐effects specifications only exploit within‐district variation, so it is less plausible that unobserved district characteristics are falsely driving the association between AYP status and levy passage probability.18 In all of our specifications, we report heteroskedasticity robust standard errors that are clustered at the district level.            "
"32","After evaluating the relationship between AYP failure and levy passage probability, we directly test the more subtle informational predictions about voter response to AYP failure. To investigate the possibility of myopia in voters' use of district performance information, we first estimate the effect of AYP failure in the previous year on levy passage in the current year. In our OLS specification, we estimate the following model:               "
"33","A second test of the myopia hypothesis is to determine whether AYP failure has a larger effect for levy elections held in the fall, shortly after district designations are released in late August, as opposed to earlier in the year. In the November elections, voters have just learned whether their school district met or failed AYP. For elections that take place earlier in the calendar year, voters have had a longer period of time to forget whether the school district met AYP. In Ohio, general elections are held every November—for state and federal offices in even‐numbered years and local offices in odd‐numbered years—whereas primary elections occur in either May (for odd‐numbered years) or March (for even‐numbered years).19"
"34","We then investigate how voters respond to the informational shock that occurs when a school district fails to meet the AYP standard for the first time. As discussed above, voters' attitudes toward and evaluations of the school district might exhibit a more pronounced response the first year a school district fails AYP compared to subsequent years. We model this hypothesis as an additive effect in which the coefficient on the first instance of AYP failure is allowed to differ from the coefficient on subsequent instances of AYP failure. Specifically, we use the following specification:               "
"35","To further test the credibility of our estimates, we conduct placebo tests using future realizations of AYP failure as our independent variable of interest. Future years' AYP status is clearly not known to voters when they cast their ballots and should have no effect on levy passage once we include other district characteristics in our specification. For these placebo tests, we estimate variants of the following specification:               "
"36","If the results are driven by unobserved factors that are correlated with AYP failure and citizens' vote choices, we would expect the placebo test to also generate a spurious association between future AYP failure and levy passage probability."
"37","The base linear probability specifications are reported in Table 1. The first four columns report cross‐sectional results with an increasingly large battery of controls that account for both district‐level and levy‐level factors. The fifth column reports the fixed‐effects specification with the complete battery of controls. In all models, AYP failure is associated with a large and statistically significant decrease in levy passage probability, ranging from approximately 4.9 to 9 percentage points.20 In the fixed‐effects specification, which provides the most convincing identification of a causal effect, AYP failure decreases levy passage probability by approximately 4.9 percentage points.21 With voters approving approximately 52% of the levies in our sample, this translates into a substantively meaningful effect that has important implications for public school finance (see the supporting information). Because the probability of AYP failure was more than 50% greater in high‐poverty districts than in low‐poverty districts—and because the former also started off with lower levels of local funding—the federal designation exacerbated the disparity in local tax resources between advantaged and disadvantaged districts.22"
"38","We next turn to investigating the more subtle informational predictions about voter responses to AYP failure. We first examine how voters incorporate AYP status from the previous year in their voting decisions. The first column of Table 2 reports a cross‐sectional regression with the full battery of controls and the indicator for AYP failure in the previous year as the independent variable of interest. The point estimate is close to zero and statistically insignificant, which is consistent with evidence that voters' retrospective evaluations of governmental performance tend to discount information from the past. In the next column, we add district fixed effects to the specification and reach a similar conclusion. In the final two columns, we report finite‐distributed lag specifications that include both lagged AYP failure and contemporaneous AYP failure as independent variables of interest. The third column is cross‐sectional, and the fourth column includes district fixed effects. In both specifications, the coefficient estimate on lagged AYP failure is statistically insignificant. Strikingly, the coefficient estimate on contemporaneous AYP failure is negative, large in magnitude, and statistically significant, further corroborating our primary result. The fixed‐effects specification indicates that failing AYP within the past year results in a decrease of 6.9 percentage points in the probability of levy passage.         "
"39","To provide a second test of voter myopia, we separately examine the response to AYP failure in elections held in November and those scheduled earlier in the year. Nearly all levy elections held prior to November take place in either the summer or spring, prior to the dissemination of district report cards in late August. For most non‐November levies, the latest AYP information was released at least seven months prior. Theories of myopia predict that voters will respond more intensely to performance information disseminated in close proximity to the election."
"40","The first column of Table 3 restricts the sample to elections held before November. The point estimate for AYP failure is still negative, although it is much smaller in magnitude and is not statistically distinguishable from zero at conventional significance levels. The second column restricts the sample to November elections and shows that the coefficient estimate for AYP failure is larger in magnitude than in our pooled specifications and that it is statistically significant. The third and fourth columns report the results of models that include the entire sample and an interaction between AYP failure and a variable indicating whether the election was held in November.23 In both specifications, the joint effect of AYP failure on levy passage probability is more than 7 percentage points when the election is held in November. While this is not quite statistically distinguishable from the effect for elections held in other months, the point estimates are consistent with the myopia documented using lagged realizations of AYP. Because the composition of the electorate changes between elections in a way that might moderate the effect of performance information, these results are merely suggestive.         "
"41","As predicted by Bayesian theories of voter behavior, voters may respond differently to the first incidence of AYP failure compared to subsequent failures. To investigate this possibility, we create two new indicator variables capturing the first incidence of AYP failure and an indicator of subsequent failures, and we include these as independent variables in our models. While the statistical power of this test is quite limited, especially in the district fixed‐effects specifications (because we can only observe at most one first incidence of AYP failure for each district), the results reported in Table 4 suggest that the first AYP failure has a particularly large effect on the likelihood of levy passage. We cannot statistically distinguish the coefficient estimates on the first and subsequent AYP failure variables, but we do find that the magnitudes of the point estimates are larger for the first AYP failure. The results are consistent with a mechanism whereby voters react more sharply to the new, perhaps surprising information about district performance than to subsequent reminders of the same information.         "
"42","Finally, we conduct a placebo test that uses next year's AYP status as the independent variable of interest. We report two fixed‐effects specifications. In the first column of Table 5, we report estimates of the model with future AYP failure as the only AYP variable in the specification. In the second column, we report estimates with both future AYP failure and current AYP failure as independent variables. In both, we fail to reject the null hypothesis that future AYP failure is uncorrelated with levy passage in the current year. These null placebo test results corroborate the credibility of our research design and increase our confidence in the validity of the results.         "
"43","The touted goals of the federal No Child Left Behind Act were to promote academic achievement and close the achievement gap between student subgroups. This study documents how the flaws in the construction and use of NCLB's AYP performance metric may have undermined these objectives. First, the dissemination of AYP designations is unlikely to have promoted academic achievement because the metric did not accurately reflect the quality or performance of local school districts. As this study illustrates, district AYP status served as a proxy for poverty and was only minimally correlated with Ohio's value‐added measure of student achievement in math and reading (the subject areas emphasized in the AYP metric)."
"44","Second, the response of local voters to districts' AYP designations undermined the goal of narrowing the achievement gap between economically advantaged and disadvantaged students. The probability of passing a school district levy declined by more than 10% when districts failed AYP, resulting in a large financial penalty that disproportionately affected districts in impoverished communities and only widened the resource disparity between districts. Recent research indicates that this likely affected student achievement. For example, Jackson, Johnson, and Persico (2014) document the importance of financial resources for promoting student achievement in impoverished districts, and Cohen and Moffitt (2009) conclude that limited resources are largely responsible for these districts struggling to implement federal educational programs meant to close the achievement gap.         "
"45","Our findings also shed light on an important debate in the education policy community about how voters respond to information about school performance. Many proponents of education accountability reforms argue that performance information can help empower parents and target remedial efforts toward schools and districts facing the greatest achievement hurdles. By contrast, some NCLB skeptics have argued that the law actually had the opposite effect, undermining voter support for public education by presenting local schools in a negative light (e.g., Ladd 2012, 215). Our study provides evidence that is more consistent with the latter perspective. Our findings suggest that, at least among districts failing AYP, the law's performance benchmarks affected voter attitudes toward local school districts in a negative way and led to a reduction in district resources available to pursue academic missions. To the extent that levy failures leave lasting negative effects on academic achievement, the political impact of the federal law appears to have undermined, rather than supported, struggling districts serving low‐performing student populations.24"
"46","This study also documents the importance of considering the validity of performance measures when examining their use by voters. Invalid measures may not be too problematic when decision makers are highly informed and understand how much weight to assign to them. The performance management literature indicates that those who manage public agencies—and, therefore, have access to large amounts of quantitative and qualitative information about their own organizations—are less inclined to use performance information in making management decisions if they perceive that information to be invalid (Moynihan and Lavertu 2012). Voters, however, may lack other sources of information about public agencies and may simply assume that a summary designation (e.g., that a school has “met” AYP standards) provides a valid characterization of performance. The performance measurement literature has examined the perverse organizational responses to invalid measures (Heinrich and Marschke 2010), but there has been very little examination of the consequences when external and relatively uninformed audiences are exposed to invalid measures.         "
"47","Finally, our findings highlight an important but frequently overlooked aspect of inter‐governmental relations. In federal systems, different levels of government influence each other not only directly—through the adoption of policies that alter the set of feasible options available to other jurisdictions—but also indirectly through the electoral channel, by changing the types of considerations voters bring to the polls and the salience of individual issues. Although the fact that presidential approval affects election outcomes in state and local elections is well established (e.g., Carsey and Wright 1998), our study suggests that there exist many other political interdependencies between governments with overlapping constituencies. Policy actions by one level of government can thus produce electoral reactions in another, with consequences that may not be anticipated by officials at either level.         "
"48","Of course, our results do not represent a general indictment of performance measurement and its application in federal contexts. Given low levels of voter knowledge, it is possible that well‐designed and clearly explained measures may improve democratic accountability and transparency, providing constituents with important information about the performance of their government and offsetting other detrimental biases in voter behavior. But our study offers lessons that should inform the design of such rating systems. First, we show that the effects of performance information diminish substantially after the initial informational “shock.” Second, we provide suggestive evidence that the impact of district AYP failure on voter decision making was contingent on the timing of local elections. Both sets of findings suggest that the effects are short‐lived and point to the need for performance measures to incorporate information over longer time periods to counteract well‐established cognitive biases that lead to voter myopia. These findings also suggest that national variation in local political institutions and informational contexts complicates the use of performance measurement as a federal policy and management tool. Those who design performance measurement systems should keep in mind the potentially heterogeneous effects of these schemes in light of how voters use information. Finally, it is important to take account of how federal goals map onto local policy objectives, as local governments may experience goal displacement if performance measures cause local stakeholders to focus only on dimensions of import to federal overseers."
"49","The measurement of government performance and the wide dissemination of performance information are likely to continue to proliferate in tandem with rapid advances in statistics and technology. Our results highlight the potential downside of such performance federalism, illustrating how flawed measures may undermine political accountability in state and local democracy and undercut the very policy objectives these accountability systems are designed to achieve. Researchers should investigate the political and democratic impacts of performance federalism so that these significant issues may be sorted out."
