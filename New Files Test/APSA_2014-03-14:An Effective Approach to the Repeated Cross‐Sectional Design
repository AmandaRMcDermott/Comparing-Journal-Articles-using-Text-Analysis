"","x"
"1","As a collection of individual‐level data repeated at regular intervals, the RCS data structure can be extremely useful by adding a dynamic component to the study of cross‐sectional units and by allowing the investigation of time‐varying relationships.3 RCS designs are increasingly prevalent in part due to the many ways in which they can be created.4 The National Annenberg Election Study (NAES), for example, collected data on unique samples of voters for every day of three presidential election campaigns. Or one can create an RCS of over 300 months using CBS/NYT polls archived at the Interuniversity Consortium for Political and Social Research (ICPSR). The same can be done with hundreds of consecutive months of Gallup polls or Michigan's Survey of Consumers (e.g., Clarke, Stewart, Ault, and Elliott 2005; Hopkins 2012).5"
"2","And those are just some public opinion examples. Congressional roll calls nested within years since 1789 (Lebo, McGlynn, and Koger 2007), cases nested within Supreme Court terms since 1946 (Segal and Spaeth 2002), and public remarks by presidents nested within quarters since 1945 (Wood 2009) all fit the RCS format.6 RCS data are widespread, and, as with longer pseudo‐ and true panels, the dynamic implications are rarely explored. Looking at just the 2010–13 issues of American Political Science Review and American Journal of Political Science, we find 42 articles where the underlying data structure—RCS, pseudo‐panel, or true panel—could allow the use of our approach.7"
"3","Absent multilevel models, researchers have chosen to study RCS data in either the aggregate or the individual level. For the latter, some have simply pooled observations from all time‐points (e.g., Jerit and Barabas 2012; Moy, Xenos, and Hess 2006; Romer 2006; Stroud 2008) or pooled subperiods of data (e.g., Blaydes and Chaney 2013 pool data from 700–1500 ACE by century). Pooling treats observations as if they were collected in a single cross‐section. However, if units within time‐points share unmeasured commonalities, standard errors may be incorrect. Or if one filters the time component via fixed effects to control for between‐time‐point effects, it limits the exploration to static processes and also assumes that parameter estimates pool around a common value. So pooling has its problems.         "
"4","Alternately, one can skip individual‐level analyses by collapsing data into mean values and applying time‐series analyses to the aggregate data. Box‐Steffensmeier, DeBoef, and Lin (2004), for example, study the gender gap using all available CBS/NYT surveys dating back to 1977 but aggregate respondents by quarter. In all, they use the responses of over 250,000 unique individuals, yet they analyze just 87 quarters of data (2004, 525).8 Studying data in the aggregate has theoretical support if causal ordering at the individual level is in question. For example, in the economic voting literature, Kramer (1983) argues that the state of the economy is an objective fact, and individuals’ evaluations of it are either survey error or “partisanship, thinly disguised.”9"
"5","If some independent variables vary only over time (e.g., the inflation rate) there is a natural tendency to construct a model in the aggregate. Yet, aggregating participants by day/week/month/quarter can reduce data sets to a thousandth of their original size. Indeed, none of the pivotal aggregate studies mentioned has taken full advantage of the RCS framework where heterogeneity exists within as well as between time‐points. Since a multilevel model allows the use of all the data, the aggregate‐ versus individual‐level debate is a false dichotomy. Researchers can explore complex relationships rather than entirely avoid an important level of analysis. In doing so, they can also investigate individual‐level relationships that might vary over time.         "
"6","Thus, two problems are evident. First, most published work using RCS has relied on techniques that study static or dynamic processes, not both. Second, ventures into MLM with RCS data have barely considered the statistical consequences of the various modeling choices. Given the wealth of RCS data available, we explore its challenges and consider the efficacy of several modeling choices.         "
"7","In a true panel design, N units are observed repeatedly over time, yielding an N × T data set and making autocorrelation likely in two directions. First, unit i at time t will be more correlated with unit j at time t than with unit j at other times. Second, the values for each unit i are likely correlated with each other over time. For example, in a Country by Year data set, regression errors are likely to be correlated within years as well as for each country. Importantly, neither type of autocorrelation disappears in an RCS design even though units are not repeated. First, dynamic autocorrelation remains. Memory over time, traceable through aggregates, likely exists between units more proximate to one another. If  and  are correlated, then εi,t is correlated with εj,t+1 more than εi,t is correlated with εk,t+2.10 This holds since observations within each time‐point are dispersed around a mean correlated with the mean of the adjacent time‐point. That is, since  = E(yi,t) and corr ( and ) ≠ 0, then corr(E(yi,t), E(yi,t+1)) ≠ 0 and corr(E(εi,t), E(εi,t+1)) ≠ 0. Second, autocorrelated errors also exist due to day‐ (or month‐, quarter‐, or year‐) specific effects.            "
"8","How are these challenges to be handled? To begin, two common PCSTS approaches cannot or should not be used. Including a lagged dependent variable (LDV) is a popular way to handle problems of nonstationarity in PCSTS and traditional time series (Keele and Kelly 2006). A second alternative, looking at the differences in observations between time‐points to render a random‐walk series stationary, is also popular (Enders 2004). Yet, since each observation occurs but once in RCS data, these approaches cannot work. Values of yi,t–1 are not available, so an LDV is not possible, nor is the use of a differenced dependent variable, , created as yi,t – yi,t–1. A third unworkable approach, the use of panel‐corrected standard errors (Beck and Katz 1995), is premised on observations repeating in every time‐point (a true panel) and does not solve the potential bias in coefficients.11"
"9","As an RCS gets longer, the possibilities of modeling dynamic processes increase and should be pursued. Modeling both dynamic and static processes together is a challenge with promise, so long as the results are reliable. This can be done in a multilevel framework, and, within that structure, time‐series filtering techniques can correct for the problems presented by autocorrelation. Next, we discuss fractional integration and outline the specifics of our ARFIMA‐MLM approach."
"10","The statistical properties of longer RCS data—once aggregated—have been well established. For example, provided enough time‐points, monthly and quarterly public opinion data consistently prove to be fractionally integrated (“long‐memory”) when tested (Box‐Steffensmeier, DeBoef, and Lin 2004; Box‐Steffensmeier and Smith 1996, 1998; Byers, Davidson, and Peel 2000; Clarke and Lebo 2003; Lebo, Walker, and Clarke 2000).12 Fractional integration has also been found in aggregate studies of congressional roll‐call data (Lebo, McGlynn, and Koger 2007), Supreme Court decisions (Hurwitz and Lanier 2004; Lanier 2003), crime statistics (Greenberg 2001), and campaign expenditures (Box‐Steffensmeier, Darmofal, and Farrell 2009).         "
"11","The correct level of integration—be it zero, one, or something in between—must be estimated and used to difference a time series in order to get trustworthy inferences. This has been shown from early work by Granger and Newbold (1974) to Box and Jenkins's (1976) techniques to more recent work on fractional integration (Lebo, Walker, and Clarke 2000; Tsay and Chung 2000). To study RCS data at two levels of analysis, the strategy must account for the properties of the aggregate‐level (level‐2) data.13 Estimating the (p,d,q) parameters of an ARFIMA model at the aggregate level (level‐2) is our first step:14"
"12","            "
"13","The first filter regresses  on its noise model (p,d,q) to create , a stationary series of residuals free of autocorrelation:            "
"14","Thus,  is a function of two components: stochastic () and deterministic (). So,  leaves the stochastic component, , that is, the part of  influenced by Xs rather than by its own past history.         "
"15","For exogenous variables at the aggregate level, the same approach is followed. Where exogenous variables vary within each month,16 means should be calculated and noise models created for each . When an exogenous variable varies only across time and not within a time period (e.g., stock prices), one should find the appropriate noise model for it and create , the movement of Zt not due to the past history of Z.17 With , , and , autocorrelation has been modeled on both sides of the equation.         "
"16","Up to this point, our proposed model simply uses methods shown elsewhere to work for time series: find the appropriate noise model and filter (i.e., Granger and Newbold 1974 for I[0/1]; Box and Jenkins 1976 for [p,0/1,q]; Clarke and Lebo 2003 for [p,d,q]). But we would also like to study the individual‐level data. For that, we marry the logic of fractional differencing with multilevel modeling and move next to the within‐month study of RCS data. This involves a filter applied to the individual‐level data prior to the estimation of an MLM.         "
"17","Political scientists have increasingly relied on MLMs to deal with hierarchical data in which “level‐1” units are nested within “level‐2” structures (Bartels 2009a, 2009b; Gelman et al. 2008). MLMs allow one to analyze how both contextual and unit‐specific factors predict a dependent variable (e.g., Gelman and Hill 2007; Steenbergen and Jones 2002). Beyond the substantive motivation, there are also decisive statistical consequences if one ignores a hierarchical structure. Since observations are not independent, the error structure is a problem. For example, if cases are drawn according to geographic areas or regions, the data are no longer conditionally independent and errors are spatially correlated. As a consequence, standard errors will be biased downward and the probability of Type I error increases (Skrondal and Rabe‐Hesketh 2004).         "
"18","Temporal clustering is, of course, also a problem—error components should be orthogonal to independent variables. This is violated insofar as spatial or temporal autocorrelation exists. In an l‐level model, the residuals should be conditionally independent at l+1. This becomes tenuous with time in the model. And, if errors are correlated over time, standard errors will be incorrect.         "
"19","Various MLMs have led to useful advances with data indexed over time. For one, MLMs have been used to analyze true panel data, where multiple observations are clustered at the country level (Beck 2007; Beck and Katz 2007; Shor et al. 2007).18 Beck and Katz (2007) note that if one assumes that a dynamic process exists, an LDV can be included in the intercept equation. But where the LDV is not measured, a different solution is needed.         "
"20","In RCS designs, the individual‐level data are nested within multiple, sequential time‐points. As with geographic clusters in a single cross‐sectional data set, cases are not independently observed. MLMs are well suited for these structures, as individuals can be viewed as embedded within the date the specific cross‐section was collected (DiPrete and Grusky 1990).19 In MLM terms, the individual‐level observations, i, are the level‐1 units and are nested within level‐2 units of time, t. Once ARFIMA methods are applied at the aggregate level, we must next pay attention to this hierarchical structure.         "
"21","To fix problems of serial correlation at level‐1, we subtract the daily deterministic component from the level‐1 dependent variable:            "
"22","Note that this step removes the deterministic component from , so that  now consists of within‐month as well as non‐temporally autocorrelated between‐month variation. We then filter our xs through the month‐level effects:            "
"23","The logic is the same as that of Bafumi and Gelman (2006). By accounting for level‐1 and level‐2 effects, correct parameter estimates can be retrieved.20"
"24","A multilevel model now puts the double‐filtered data to work.21 The level‐1 equation—the within‐month model—can be written as            "
"25","The intercepts, , vary across months where . In other words, the intercept  represents the month‐averaged score of  purged of autocorrelation. It is simply . We can subsequently define these intercepts to be a stochastic function of aggregated individual‐level effects,  and aggregate‐level covariates, Zt:            "
"26","The error terms for Equations 5 and 6 are represented as  and  for level‐1 and level‐2 units, respectively.         "
"27","Combining the equations yields            "
"28","Our model is also well suited for the estimation of time‐varying parameters. As we show in our examples below, one can specify coefficients that will vary across time for certain independent variables, . If a level‐1 relationship might change in different contexts, a time‐varying coefficient, , can be specified. Thus, Equation 5 can be expanded to            "
"29","The steps can be summarized as follows: First, create monthly means for the level‐1 data of interest,  and . Second, find the proper noise models for them as well as for level‐2 series, Zt, that do not vary within months. Third, filter each through its noise model to create level‐2 series free of autocorrelation, , and . Fourth, remove month‐level deterministic components from the level‐1 data. Fifth, estimate an MLM in two levels using the double‐filtered data.         "
"30","Our model offers several advantages. We use the most reliable techniques available—ARFIMA models—to filter out level‐2 autocorrelation. Addtionally, by taking the deviations of i from level‐2 values, we fix problems of serial correlation at level‐1. In addition, we are able to include level‐2 variables that do not vary within time‐points as covariates as well as investigate interesting time‐varying effects. In the next section, we use Monte Carlo analyses to compare the statistical consequences of our approach to several alternatives.         "
"31","We expect that if the dynamic component is ignored, estimates will be adversely affected, more so as time dependence at level‐2 grows. Even if a lag at level‐2 is modeled, bias will still be present to the extent that the lag insufficiently accounts for autocorrelated errors. Moreover, if errors are clustered—and thereby not independent—the standard errors will be incorrect."
"32","We simulate data meant to mimic the properties of RCS data.22 We generated 11,000 data sets, with each consisting of 300 waves and a sample size of 100 per wave.23 Aggregate values of the independent variable, , were created along with  values for within‐month variation.24 Level‐1 observations, , were generated as a function of individual‐level effects,  (specified to have a slope coefficient of 0.5), aggregate‐level effects,  (specified to have a slope coefficient of 0.3), and random error. Next, series for  and  were calculated so that there were 1,000 data sets for each value of fractional integration between 0 and 1 in increments of 0.1.25"
"33","We tested the statistical properties of eight estimation strategies for each data set. We start by presenting “naïve models” where we fail to separate out the aggregate‐ and individual‐level effects. We do this using (1) OLS (labeled here OLS‐Naïve) as well as (2) a multilevel model (MLM‐Naïve) where intercepts vary across months. We next report the results of six additional strategies that could be used: (3) OLS pooling all data but separating aggregate‐ and individual‐level effects (OLS), (4) OLS specifying aggregate‐ and individual‐level effects and including a month‐level lagged dependent variable (OLS‐LDV), and (5) OLS accounting for nonstationarity by fractionally differencing the aggregate‐level monthly means (ARFIMA‐OLS). We also estimated three additional types of MLMs: (6) an MLM separating aggregate‐ and individual‐level effects and allowing intercepts to vary across time (MLM), (7) a version of (6) that adds a month‐level lag (MLM‐LDV), and (8) fractionally differencing the aggregate series and allowing intercepts to vary across time (ARFIMA‐MLM).26"
"34","The naïve models are clearly insufficient.27 For OLS, the estimates fall between the true slopes with low levels of d. But as d rises, so does the spread of the estimates, and the average estimated coefficient is biased toward zero. In other words, as the dependent variable becomes less stationary, slopes become more biased and less efficient.               "
"35","The MLM‐Naïve method properly retrieves the individual‐level effect, but two problems are present: (1) it ineffectively models aggregate‐level processes, since they are inseparable from individual‐level effects (see also Bafumi and Gelman 2006; Bartels 2009a); and (2) standard errors are incorrect. As d increases, the standard errors are biased downward, leading to incorrect inferences (see Table S3 in the supporting information).               "
"36","Beyond the naïve models, we need to confront the problems of modeling both individual‐ and aggregate‐level effects together as well as address the likelihood of nonstationarity at level‐2. The latter problem has been especially ignored by social scientists.28 Next, we explore the empirical consequences of six additional approaches: OLS, OLS‐LDV, ARFIMA‐OLS, MLM, MLM‐LDV, and ARFIMA‐MLM.               "
"37","What should we expect from each OLS approach? By pooling all observations and running an OLS regression (solution 3: OLS), we should retrieve incorrect parameter estimates for the aggregate effects of x (). Simply specifying an OLS model with a lagged aggregate dependent variable,  (OLS‐LDV), should produce unbiased and efficient estimates only if the lag accounts for aggregate‐level autocorrelation (Achen 2000). Since this will not occur in the presence of fractional integration, bias is expected. The approach of using an ARFIMA model for month‐level x () and month‐level y  and employing OLS will result in incorrect standard errors since OLS cannot effectively account for unobserved aggregate‐level variation.               "
"38","The MLM approaches should be an improvement, but the MLM assumption of independent level‐2 errors will be violated insofar as level‐2 units are correlated. Thus, an MLM without filters will produce biased and inefficient estimates as d increases. Similarly, an MLM with a lagged dependent variable, , will produce biased estimates and standard errors that are increasingly incorrect as d increases. We expect that the ARFIMA‐MLM model will prove to be the most reliable approach.               "
"39","Figures 1 and 2 display our estimates of bias and inefficiency for the various OLS and MLM models, respectively. Bias is the average of each parameter estimate divided by the true parameter value for each level of d. Thus, a value of 100 indicates a lack of bias. We calculate efficiency as , the degree of variation around the average estimate, where n is the number of data sets (1,000). Smaller values indicate greater efficiency. We display the results for four sets of estimates: bias and RMSE for each of the aggregate‐level effects (β for  for the OLS, OLS‐LDV, MLM, and MLM‐LDV models and β for  for the ARFIMA‐OLS and ARFIMA‐MLM models) and for the individual‐level effects (β for xit for the OLS, OLS‐LDV, MLM, and MLM‐LDV models and β for  for the ARFIMA‐OLS and ARFIMA‐MLM models).               "
"40","Bias and RMSE for OLS Coefficients"
"41","Note: For the OLS and OLS‐LDV models, this is the coefficient for . For the ARFIMA‐OLS models, it is the coefficient for . Lines in the bottom panels are all present but overlap.                           "
"42","Bias and RMSE for the Random Intercept Multilevel Models"
"43","Note: For the MLM and MLM‐LDV models, this is the coefficient for . For the ARFIMA‐OLS models, it is the coefficient for . Lines in the bottom panels are all present but overlap.                           "
"44","Figure 1 demonstrates that OLS and ARFIMA‐OLS perform reasonably well in terms of retrieving the correct slope for the between‐month effect of x on y. ARFIMA‐OLS is the most accurate, which is to be expected since it effectively controls for nonstationary month‐level effects. OLS‐LDV, however, is problematic; the estimated slopes are biased downward as d increases. The upper‐right quadrant similarly shows that ARFIMA‐OLS has no problems of inefficiency, whereas OLS and OLS‐LDV grow more inefficient as d increases.29 All three methods perform well in terms of retrieving correct individual‐level effects, evident in the fact that the lines can barely be discerned in the bottom‐left quadrant of Figure 1. So long as one subtracts the month‐level means from the observed data, correct within‐month parameter estimates can be retrieved. However, the efficiency of estimates is compromised in the case of the OLS models.               "
"45","Figure 2 presents the diagnostics for the MLM approaches. ARFIMA‐MLM estimates of β for prove to be the best in terms of being unbiased and efficient. In all, the results demonstrate the importance of accounting for nonstationarity.               "
"46","As one final check on the models, we measure the variation in the standard errors for these models at various levels of d. To this end, we present “optimism” in Table 1, which contrasts the estimated standard errors to sampling variation (see Shor et al. 2007). Following Beck and Katz (1995), . Values greater than 100 indicate that true sampling variation is greater than estimated variation and that standard errors are too small; values less than 100 indicate that standard errors are too large, since true sampling variation is smaller than estimated variation (Beck and Katz 1995). Thus, values above 100 increase Type 1 error rates, the critical inferential problem here.               "
"47","As Table 1 illustrates, the standard errors are much too small for all methods except the ARFIMA‐MLM model.30 At all levels of d, the standard errors are severely “overconfident” in the OLS models. That is, true sampling variability is much larger than estimated variability, leading to t‐statistics that are too large. For the OLS and OLS‐LDV estimates, this effect grows as d increases. This is also evident for the MLM and MLM‐LDV models. As the aggregate means are increasingly a function of past values, the standard errors are underestimated.               "
"48","The ARFIMA‐OLS models do well at various values of d but have optimism scores that are consistently high due to the model's inattention to aggregate‐level variation. Only after accounting for aggregate tendencies as well as individual‐level heterogeneity can one retrieve standard errors that reflect true sampling variability. The winner is the ARFIMA‐MLM model.               "
"49","In all, the simulations strongly support the need to consider and model the memory across time‐period clusters. With (OLS‐LDV and MLM‐LDV) or without (OLS and MLM) a lagged cluster mean, the models perform poorly when long memory is ignored. As d increases, these models produce biased and less precise parameter estimates and standard errors that are too small. On the other hand, ARFIMA‐MLM and ARFIMA‐OLS produce unbiased parameter estimates when fractional integration is modeled. Yet, ARFIMA‐OLS estimates of aggregate‐level standard errors will be too small, elevating the risk of Type I error. Thus, we advocate the ARFIMA‐MLM model when cross‐sections are related over time.               "
"50","As mentioned, having T > 50 is a good rule of thumb for reliable estimates of d and the use of ARFIMA methods. But even with much shorter data sets, an MLM approach can prove useful in overcoming worries of autocorrelation, for examining multiple levels of analysis at once, and for studying time‐varying relationships. Shorter RCS examples include the cumulative American National Election Study (see, e.g., Stoker and Jennings 2008), where an MLM might test the effects of context on electoral choice over 20 or so elections. One can apply stationarity tests to such data and generalize our ARFIMA approach to Box and Jenkins's (1976) original ARIMA framework. That is, simpler models are available when d is an integer—or when too few waves exist to properly estimate d as a real number. When diagnosed as stationary (d = 0), a series can be modeled as ; and, where d = 1, Equation 1 simplifies to a differenced version of : . Following that, the second filter can be applied to the individual‐level data. If the best model is simply (0, 0, 0)—that is, no autocorrelation exists in the aggregate—then the model reduces to mean‐centering of the level‐1 units (as suggested by Bafumi and Gelman 2006).            "
"51","In cases where T is very short (e.g., less than 10) and tests of stationarity are unreliable, one could simply use theory to decide whether differencing is appropriate at level‐2. For example, with 10 monthly waves and a dependent variable of partisanship, assuming long memory and differencing is a better choice than leaving the level‐2 data in level form. With longer T, however, it is best to begin with an ARFIMA noise model at level‐2. Next, we demonstrate the usefulness of the ARFIMA‐MLM approach in three separate examples."
"52","In the three examples that follow, we demonstrate the advantages of our method. First, in a comparison to another MLM approach to RCS data (Hopkins 2012), ours offers improved statistical accuracy and different aggregate‐level findings. Second, expanding a strictly time‐series approach (Lebo, McGlynn, and Koger 2007), our model allows a richer depth of theoretical development and empirical testing. And third, using the National Annenberg Election Study, our model's flexibility allows the study of a complex campaign environment not fully explored in the literature (e.g., Brady and Johnston 2006; Kenski, Hardy, and Hall Jamieson 2010).         "
"53","Our first of three examples presents our toughest test. Since others have suggested that RCS data are best dealt with in an MLM format, can we demonstrate ARFIMA‐MLM's value over other multilevel approaches? In “Whose Economy? Perceptions of National Economic Performance during Unequal Growth,” Hopkins (2012) uses a multilevel model to study over 215,000 respondents nested in 388 months of the Michigan Survey of Consumer Attitudes. These RCS data are a popular source for subjective evaluations of the economy (e.g., Bafumi 2010; Clarke et al. 2005; DeBoef and Kellstedt 2004; Krause 1997; MacKuen, Erikson, and Stimson 1992).            "
"54","Hopkins's MLM is justified to explain both individual‐level factors and their response to aggregate data: “To understand how economic conditions influence Americans’ economic perceptions, it is critical to observe attitudes under a range of economic conditions” (2012, 56). Indeed, the key conclusion of the article is at the aggregate level. When asking, “Whose economy matters?” for the formation of economic assessments, “the answer from the SCA is that Americans at all income levels weigh income growth at the low end in their responses” (2012, 68). This is based on regressing sociotropic assessments on aggregate income growth for five groups defined by income percentile."
"55","We replicate Hopkins's analyses and also try the MLM‐LDV and ARFIMA‐MLM approaches. As Hopkins does, we do this separately for rich, middle‐income, and poor respondents. Figure 3 displays the t‐test statistics for the key finding—the effect of 20th percentile income growth—in three models.31 Using ARFIMA‐MLM, the key effect essentially disappears; light gray bars show the results for Hopkins's method, and the black bars are for ARFIMA‐MLM (e.g., t = 1.41 for poor respondents).32"
"56","The Effect of Income Growth for 20th Percentile of Income Using Three Modeling Approaches"
"57","Note: Each bar represents the t‐statistics for the respective group's regression coefficient in three modeling approaches (darkest is ARFIMA‐MLM, medium is MLM, and lightest is MLM‐LDV).                        "
"58","What the plain MLM misses is that the dependent variable, aggregate sociotropic evaluations, is fractionally integrated (e.g., the d estimate is 0.81 for poor respondents). Ignoring between‐month dynamics leads to incorrect parameter estimates and an increased likelihood of Type I error.33 The t‐statistics associated with income growth at the 20th percentile are considerably greater when ARFIMA filtering is not used. As such, our method gives an inconclusive result as to whether lower‐strata income growth shapes national economic evaluations.34 Recall that the simulations found an optimism index much larger than 100 for d = 0.8 with a lagged value of  included. Thus, even with the wealth of data in Hopkins's analyses, neither the MLM nor the MLM‐LDV can deal with the data's inherent autocorrelation. The ARFIMA‐MLM model (optimism = 100) in such cases is up to the task. The best methods available to time‐series analysts need to be married to the MLM approach.            "
"59","In our second example, we show the gains of expanding a strictly macrolevel analysis to include individual‐level data: we can specify time‐varying effects for covariates and compare effects occurring at two levels. Lebo, McGlynn, and Koger (2007) use ARFIMA techniques to explain how yearly levels of Democratic and Republican Party unity interact closely with each other in congressional roll‐call votes from 1789 to 2000. In the Strategic Party Government model, the two parties balance their voting cohesion over time—when a party votes too cohesively, it risks pulling members away from the wishes of their constituents, but a lack of cohesion risks losing important votes and thus electoral support. The strong findings in the aggregate demonstrate this long‐term pattern over the course of American history but leave a great deal unanswered. For one, is this strategic behavior a consistently useful way of understanding party behavior on roll calls within congresses? Also, has the era of polarization affected the relationship between the parties in their roll‐call battles?            "
"60","The original data are RCS—observations are nested within years but do not repeat over multiple time‐points—and we investigate relationships using data at both the roll‐call and yearly levels. Beginning with the 29,734 final passage party‐line votes from the U.S. House of Representatives nested in 222 years, we calculated year‐level means for majority and minority party unity and estimated ARFIMA models for each in order to generate white noise series (i.e.,  and ).35 The second filter subtracts the year‐level means from individual observations of xi and yi, respectively minority and majority unity on roll‐call i. With our double filtering done, we add five more variables to the aggregate model (i.e., ): the percent of the House held by the majority and both median‐to‐median distance between the parties and standard deviations for the majority party in each of the first and second dimensions of DW‐NOMINATE scores (Poole and Rosenthal 1997).            "
"61","The findings of our ARFIMA‐MLM model (Table 2) tell a much more nuanced story than the original one. At level‐2, the original relationships all stand up: Majority Party Unity is closely related to Minority Party Unity, close enough that the error correction mechanism is significant. The size of the majority is also important—a larger majority party can afford to be less unified (see also Patty 2008).            "
"62","Looking at the effect of Minority Unity on final passage votes within years, we still see a very strong relationship (t = 23.15). Next, we estimate a random coefficient for Minority Party Unity's year‐by‐year effect at level‐1. The solid line in Figure 4 gives us the overall coefficient, whereas the jagged line gives us the yearly value (and Loess smoother) for , the effect of Minority Party Unity on Majority Party Unity for individual roll‐call votes. The relationship obviously varies a great deal over time, and pooling would hide this entirely.36 In particular, the strength of the relationship has been dropping for some time and is now essentially zero. In an increasingly polarized political environment, there is extremely little variation in unity across votes.37 As members of Congress appeal to increasingly polarized constituencies, the strategic adjustments parties make are primarily aggregate shifts from one year to the next, rather than within‐Congress variation across roll‐call votes.            "
"63","The Time‐Varying Effect of Minority Unity on Majority Unity, 1794–2006"
"64","Note: Coefficient and smoother for the roll‐call‐level effect of Minority Party Unity on Majority Party Unity.                        "
"65","Unlike the Hopkins (2012) example, here the original study used only aggregate data and accounted for autocorrelation in the data. However, it was limited in scope in that the level of analysis was strictly aggregate. The use of ARFIMA‐MLM opens up possibilities for studying the thousands of individual roll calls that compose the complete RCS data set. A similar approach could extend studies of key dependent variables in public opinion, Supreme Court decisions, or conflicts nested in time, for example.38"
"66","The 2008 presidential election campaign was a complex one. A full and flexible model of vote choice or candidate evaluation should be able to account for the effects of the unfolding economic crisis as well as time‐varying individual‐level factors such as sociodemographics and economic and political judgments. Kenski, Hardy, and Hall Jamieson (KHHJ 2010) study the election from multiple angles using the daily data of the National Annenberg Election Survey (NAES). In some dynamic analyses, they demonstrate how assessments of the parties, candidates, and issues changed—sometimes dramatically—over the course of the campaign (e.g., 2010, 4, 18, 19, 93, 156). Elsewhere, they pool together thousands of respondents interviewed over several weeks and run static analyses (e.g., 2010, 168, 271, 275, 299, 316). Aside from the statistical problems of pooling, KHHJ neglect to exploit the data's capability to simultaneously study how time‐varying individual‐level factors and dynamic campaign‐level processes influence political behaviors and judgment."
"67","In our final example, we use ARFIMA‐MLM to assess the roles played by the “fundamentals” and campaign‐specific characteristics while taking account of the (perhaps time‐varying) voter‐level factors that affected evaluations of Senators Obama and McCain. Our dependent variable measures positive versus negative evaluations of Barack Obama relative to John McCain, with 0 as a midpoint and higher values indicating pro‐Obama sentiment (M = 0.16, SD = 4.67).39 In the aggregate, the dependent variable is demonstrably nonstationary (d = 0.90), thus making our ARFIMA filtering especially necessary.40 On the right‐hand side, our covariates follow KHHJ's (2010, 299) specification and include four categories: fundamentals, sociodemographics, media, and opinions on central campaign messages.            "
"68","Our ARFIMA‐MLM model provides estimates for within‐day and between‐day effects in Table 3. The statistically significant findings of the original model are italicized. Several important characteristics emerge and demonstrate the difference in aggregate‐ and individual‐level inferences. For example, an optimistic assessment of the state of the national economy translates into more positive McCain evaluations (b = –0.11, SE = 0.05, p < .05), but as the electorate's opinion varied over the campaign, this did not affect candidate evaluation in the aggregate (b = 0.14, SE = 2.02, ns). The reverse is true for personal economic evaluations: ratings of respondents’ own financial situation have a nonsignificant impact on evaluations (b = –0.02, SE = 0.05, ns), whereas the aggregate movement of the variable led to more positive McCain assessments (b = –4.67, SE = 2.23, p < .05). Certainly, the effects of the economy and economic judgments on political evaluations are complex. Using RCS data to the fullest extent prevents us from oversimplifying the relationships present in the data.            "
"69","Lastly, in a differently specified model, we examine how economic evaluation and party identification shape candidate evaluation. We estimate one model predicting candidate evaluation in which the slopes are fixed across days, and a second model allowing the within‐day slopes, , to vary across days for economic evaluations and party identification.41 The time‐varying coefficients (plus a Loess smoother) are plotted in Figure 5, with the solid lines representing the constant coefficient we would get if we lose the flexibility of estimating . In line with the notion that campaigns activate and reinforce latent predispositions (Lazarsfeld, Berelson, and Gaudet 1944), we see these effects grow significantly over the five‐month period. In all, this example gives us a nice array of the advantages of the ARFIMA‐MLM approach: (1) individual‐ and aggregate‐level relationships are complex and can be studied simultaneously, (2) the time‐varying effects of covariates can be studied, and (3) dealing with level‐2 nonstationarity makes for trustworthy statistical inferences.            "
"70","Given what is known about the problems of time‐series data and our investigations here, time‐level clustering is an important issue to consider in RCS data sets. Most of the prior work using RCS data has been unsatisfactory—analyzing either within or between processes exclusively, not both simultaneously. We demonstrate that failing to account for dynamic effects that exist can lead to biased parameter estimates and incorrect standard errors. Our solution is a two‐step filtering process where means are retrieved, a level‐2 ARFIMA model is specified, and then level‐1 data are filtered through these estimates. Each of the other seven approaches we test encounters problems in one area or another, but our ARFIMA‐MLM always performs well, especially as memory in the aggregates gets longer."
"71","In addition, as demonstrated in our last two examples, we can include time‐varying coefficients for some covariates. To be sure, with so much data, the RCS design is a great resource for studying time‐varying relationships. Allowing the constants and coefficients to vary from one wave to the next while also measuring level‐2 factors means that the effects of level‐1 variables can be seen to rise and fall according to dynamic contextual factors. Even so, without a method such as double filtering, the inferences from such an exercise would be suspect."
"72","It is worth noting additional points of flexibility within our framework. Where T is lower, one can switch to models that do not involve estimating the fractional integration parameter, d. ARMA and ARIMA models are just particular types of ARFIMA models but are safer to estimate up until the point of about t = 50 (Dickinson and Lebo 2007). Estimating a (p, 0/1, q) model and then filtering values at level‐2 will allow the same implementation of our method one could get with longer T.         "
"73","Additionally, our ARFIMA‐MLM framework can certainly be extended to PCSTS designs, but with two notable caveats. First, the number of pseudo‐waves that can be compiled into an RCS design may be quite high, perhaps running into the hundreds of consecutive waves. With PCSTS, however, long data sets are rarer. Yearly data by country often top out at t = 65 for the postwar era. True panels of individual‐level data are unlikely to ever approach the t of an RCS design. One can only wish for something like the three‐ and four‐wave panels sometimes seen in the National Election Study or the British Election Study to be carried on at frequent intervals for decades. With a shorter t, the PCSTS analyst is best served by estimating a simpler noise model at the aggregate level, but the rest of our framework would still apply.         "
"74","Second, the PCSTS analyst has other methods available to control for autocorrelation, such as panel‐corrected standard errors (Beck and Katz 1995). Differencing or using a lagged dependent variable are two imperfect solutions, but they are still improvements on simply pooling the data or ignoring the sequence of the waves. Or, by differencing all the variables in a dynamic panel (Baltagi 2005), unit‐specific idiosyncrasies can drop out of a PCSTS model. Our double filtering method can be added to this list of solutions but, admittedly, has more competition in that particular toolbox.         "
"75","We encourage researchers to adopt the ARFIMA‐MLM model when analyzing RCS data or long panels and pseudo‐panels. By using multilevel models to study data nested in time, not only can researchers capture contemporaneous variation, but they can also directly model dynamic processes. Taken together, these results suggest that time‐level clustering is crucial to consider, and with greater attention to simultaneously modeling static and dynamic processes, this will provide a richer depiction of political and social phenomena."
