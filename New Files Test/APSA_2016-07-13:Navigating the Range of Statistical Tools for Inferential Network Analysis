"","x"
"1","Statistical methods usually require observational independence, or the assumption that actors do not influence each other's outcomes. While such an assumption may apply to randomized surveys or lab experiments, it is less believable in social and political systems where the interactions between actors are an integral part of the process of interest. The implications of interdependencies have been recognized in the cases of time‐series and spatial data, but political scientists are less familiar with the methods used to model network‐based interdependence. Crucially, the development of sophisticated statistical models for studying interdependence is not “merely” a statistical advance, but opens the door for new questions and theories to be posed and tested. Our goal is to introduce and compare three methods commonly used for statistical inference with network data, and to demonstrate the added value of specialized network models compared to a more traditional logit‐based estimation approach.         "
"2","Here, we focus on three techniques designed to model relational outcomes: the quadratic assignment procedure, exponential random graph models, and latent space network models. In other words, these are models that describe a network of political interactions, where the outcome of interest is a political relationship (e.g., wars, legislative collaboration). Such an understanding of networks—that is to say, one focused on explaining interdependence in relationships—is of interest in many corners of the discipline. For example, networks are formed by terror groups (Desmarais and Cranmer 2013; Perliger and Pedahzur 2011), mutual protection treaties (Maoz et al. 2006), and trade flows (Krempel and Plümper 2002). Institutional networks are relevant to diaspora politics (Miller and Ritter 2014), the impact of nongovernmental organizations on policy (Montoya 2008), and cooperation and conflict in legislatures (Tam Cho and Fowler 2010). Networks are also part of understanding political behavior, especially where we are interested in questions involving integration, cooperation, and conflict (Ahn et al. 2013; McCubbins, Paturi, and Weller 2009).         "
"3","The study of interdependence is part of the core mission of the social sciences, if not the core mission (Ward, Stovel, and Sacks 2011). It is impossible to describe political and social activity without describing how actors (e.g., individuals, groups, states) interact with, react to, adapt to, and influence one another in complex and dynamic ways. One can view interdependence as either a threat to the validity of statistical analysis or as fundamental to a theory of political processes. These two perspectives are radically different in substance, yet both imply the need to model interdependencies empirically. While we hold that the latter perspective is more compelling, political scientists have generally viewed interdependence as a threat.         "
"4","Most statistical techniques, including regression models, depend critically on the presence of conditional independence of observations for their validity. Consider a generalized linear model estimated by maximum likelihood (e.g., logit, Poisson, or Gaussian linear model). The joint likelihood—that is, the object to be maximized in order to recover the parameter estimates—is computed as            "
"5","Critically, this joint likelihood will only be valid if the observations are conditionally independent. If the conditional independence assumption is broken, by omission of a relevant covariate (as in the case of omitted variable bias) or by omission of relevant endogenous (network) dependencies, Equation 1 will not be a joint likelihood. To be clear, nothing will block the computation of a quantity,1 but that quantity is not the joint likelihood and has no known statistical properties. The bias introduced by violation of the independence assumption is arbitrary and can range from trivial to catastrophic, contingent on the extent of the unmodeled dependencies and the sensitivity of the model to them.         "
"6","Substantively, we must ask ourselves whether the observations under consideration influence each other through means not accounted for by the covariates. Cranmer, Desmarais, and Menninga (2012) illustrate the limits of the independence assumption in an analysis of World War II. The assumption of conditional independence in a traditional logit model of conflict implies that the British declaration of war on Germany in 1939 was entirely independent of all factors other than, say, regime type similarity and the ratio of relative capabilities between the two states. Such a claim is comical, as Britain's declaration of war was unambiguously a response to Germany's invasion of Poland, but standard regression models cannot account for such dependencies.         "
"7","Several recent studies have shown that when network dependencies are properly accounted for, long‐standing substantive findings may be called into question. Perhaps the most persuasive example of this involves the “democratic peace,” with the idea being that democracies tend not to fight one another (for canonical works, see Maoz and Abdolali 1989; Maoz and Russett 1993). The effect of the democratic peace has been largely established via logistic regression models, which assume conditional independence of observations. Recent works by Cranmer and Desmarais (2011) and Cranmer, Menninga, and Mucha (2015) have shown that joint democracy has no statistically discernible effect on conflict once network dependencies are accounted for, thus suggesting that network structure in which conflicts occur, rather than something about the nature of joint democracy, accounts for the paucity of conflicts between democratic states. What is more, failure to properly model network dependencies can result in the same sort of bias. Maoz et al. (2006) were interested in the network concept of structural equivalence, but they included this effect as a predictor in a logistic regression. A replication of this analysis by Cranmer and Desmarais (2011) using a temporal exponential random graph model uncovered substantial bias in the results, including a new lack of statistical significance for their operationalization of the democratic peace.         "
"8","While many network analysis techniques are descriptive in nature, we review and compare three methods commonly used for inferential network analysis: the quadratic assignment procedure, latent space network models, and exponential random graph models. We focus on cross‐sectional models where binary network ties (called edges) between actors (called vertices) are the outcome of interest (e.g., relational data where the outcome of interest is the presence or absence of an edge), although these models can often be extended to accommodate different network types (e.g., longitudinally observed networks and networks with weighted edges).         "
"9","The quadratic assignment procedure (QAP) is a nonparametric test for the significance of an association between two matrices with complex dependencies (Hubert and Schultz 1976). The QAP is not a statistical model but an add‐on for standard regression models that provides an unbiased test of association in spite of potential dependencies. While the original QAP (Mantel 1967) was designed for bivariate associations, Krackhardt (1987, 1988) introduced multiple regression QAP (MRQAP).            "
"10","Assuming that linear models for square matrices take the form               "
"11","The intuition behind the QAP is that permutations of N, where the order of the rows and columns is reshuffled simultaneously in the same way, preserve the dependency structure across observations within N, but remove the dependencies in the associations between N and X as well as N and Z. For example, if X denotes a matrix containing legislators' common memberships in committees and N denotes legislators' information‐exchange patterns, then a simultaneous permutation of the rows and columns of N (in the same way) would still preserve the information on who exchanges information with whom, but it is no longer possible to relate the common committee memberships of a specific actor pair  to the information exchange pattern of the same dyad. While there is likely an association between X and the original, nonpermuted N, there is not expected to be an association between X and the permuted N.            "
"12","The QAP uses this permutation procedure to create an empirical sampling distribution reflecting a new null hypothesis of no association that correctly takes into account the correlation between observations. The procedure repeats the permutations a large number of times and recomputes the association measure (e.g., the regression coefficient for X in the MRQAP case) at each iteration. The resulting distribution corresponds to the distribution under the null hypothesis that no association between X and N exists, but correctly takes into account the dependencies within N. Finally, the original coefficient for X is compared to the new distribution to see how many elements of the new distribution are less extreme than the coefficient, in order to obtain a dependency‐corrected (and thus bias‐corrected) p‐value for the coefficient. Essentially, this approach treats dependencies among the observations as a nuisance that needs to be corrected for rather than as an interesting feature that can be modeled. This makes the approach easier to learn and apply, but it offers no modeling choices if one is substantively interested in the structure of the dependencies.            "
"13","Martin (1999) generalizes the MRQAP to arbitrary outcome distributions (e.g., the common case of binary matrices) by applying the QAP test to coefficients from generalized linear models. However, subsequent research on the original QAP test finds that the p‐values produced by the original permutation approach are prone to bias under certain conditions (Dekker, Krackhardt, and Snijders 2007): (Combinations of) nonpivotal association tests (e.g., partial regression coefficients; see Kennedy and Cade 1996; Dekker, Krackhardt, and Snijders 2007), collinearity (the presence of a third variable Z in the model, which has a correlation both with N and the other predictor X; see Anderson and Robinson 2001), small numbers of vertices (Anderson and Robinson 2001), skewness of the edge weights in the outcome network (Dekker, Krackhardt, and Snijders 2007), and high levels of autocorrelation between observations of the same vertices (Dekker, Krackhardt, and Snijders 2007) lead to biased uncertainty measures. The current states of the art are Freedman–Lane semi‐partialing (Freedman and Lane 1983) and double semi‐partialing (Dekker, Krackhardt, and Snijders 2007), both of which resolve most of the problems mentioned above when the general linear model is used (Dekker, Krackhardt, and Snijders 2007). Freedman–Lane semi‐partialing and Dekker's double semi‐partialing are termed residual methods (as opposed to raw methods, which permute the raw N matrix) because they permute the residuals in order to partial out the effect of the third variable Z, which partly explains X and N. The Freedman–Lane semi‐partialing method first estimates the effect of Z on N alone (without including the model term of interest X), then permutes the residuals of that model to simulate a distribution of new N matrices, and finally recomputes the full model for all simulated N in order to obtain a sampling distribution, thereby partialing out the effect of Z from N. Dekker's double semi‐partialing is similar, but it partials out the effect of the third variable Z from the effect of the other predictors X, rather than the response matrix N. Dekker, Krackhardt, and Snijders (2007) show that these two approaches are largely unbiased under most circumstances in conjunction with a linear model; only high levels of collinearity between X and Z in combination with highly skewed edge values in the dependent network lead to an increased number of type I errors.            "
"14","It is yet unclear how this result would extend to binary network matrices with few edges (and thus a skewed distribution) when collinearity is present—despite numerous published applications to binary networks in conjunction with logistic regression (e.g., Ingram and Roberts 2000). Besides this caveat, standard errors cannot be recovered when the QAP approach is used; only the coefficient and its p‐value are available. Moreover, QAP‐corrected models are subject to the same limits as their uncorrected counterparts; for example, they are still sensitive to omitted variable bias.            "
"15","However, the MRQAP has some benefits that make it worth considering. Critically, it is the most accessible of the methods treated here. It is well implemented2 and easy to interpret, as MRQAP results can be interpreted like other regression. MRQAP can also produce reasonable results under conditions that might make it difficult to obtain estimates using other methods (e.g., a dense network of relationships). Finally, the MRQAP does not need a theoretical model to “deal with” network dependencies. While this reduces the likelihood that the results are contingent on a theoretically misspecified model, it also implies that there are no opportunities to learn about those dependencies.            "
"16","The exponential random graph model (ERGM) is a powerful model that has seen increased usage in recent years. First proposed by Wasserman and Pattison (1996), the ERGM is a direct operationalization of the joint probability density from which the networks are thought to be generated, with minimal modeling assumptions. For intuition, consider that the ERGM finds its parameters by maximizing the probability of the observed network over the networks with the same number of vertices that could have been observed. This is conditional on a set of network statistics that can include vertex‐ and edge‐level exogenous variables and endogenous dependencies; the results is a substantial increase in the scope of modelable dependencies without any independence assumptions. This produces a single, multivariate distribution from which the network of interest is most likely to be drawn.            "
"17","The ERGM is structured as follows:               "
"18","The ERGM requires two assumptions. First, one must assume that there is equal probability of observing any two networks with the same values of the statistics included in the specification. This is functionally the same as assuming that a model is completely and correctly specified. Consider two networks that are identical with respect to the values of the statistics computed in , for example, the number of closed triads. If one of these networks is more likely than the other, something not included in the model is causing that imbalance. By implication, the model would be incompletely or incorrectly specified. The second assumption is that the observed network exhibits the average value of those statistics over the networks that could have been observed. This assumption is necessary to identify the parameters, but it is not different in practice from the assumption made in regression models that the average relationships in a data set are representative of the population (Cranmer and Desmarais 2011).            "
"19","All effects in the ERGM are included as statistics computed on the network in the vector . Exogenous covariates, both at the edge and vertex levels, are included in the ERGM through statistics computed as               "
"20","The ERGM is also designed to accommodate a great variety of endogenous dependencies, also as statistics in the  vector. However, since the endogenous effects will capture different forms of interdependence, they must be specified differently, and no single formula exists for articulating them as in Equation 4. Consider two examples of endogenous dependencies includable in the ERGM. First, consider reciprocity. This effect is endogenous because it occurs only within the outcome network (e.g., i considers j a friend, and j considers i a friend as well). Reciprocity is illustrated in the left panel of Figure 1 and may be written as               "
"21","Examples of Canonical ERGM Statistics"
"22","Note: The left panel indicates reciprocity in a directed network, and the right panel illustrates transitivity in a directed network.                        "
"23","ERGMs are generally estimated in one of two ways. Markov chain Monte Carlo maximum likelihood estimation (MCMC‐MLE) is the preferred method for estimating cross‐sectional (single network at a single time period) ERGMs (Geyer and Thompson 1992; Hunter and Handcock 2006; Snijders 2002; Snijders et al. 2006; van Duijn, Gile, and Handcock 2009). An alternative method of estimation is called maximum pseudolikelihood estimation (MPLE) (Hyvarinen 2006; Strauss and Ikeda 1990). MPLE is substantially less computationally difficult than MCMC‐MLE, but it has the problem that confidence measures are downwardly biased. This problem is correctable when using this technique on longitudinally observed networks (Desmarais and Cranmer 2012b). For details on both of these estimation routines and a direct comparison of the two, see Desmarais and Cranmer (2012b).            "
"24","Yet the ERGM is not without its disadvantages. First, we mentioned that the ability to model specific forms of interdependence is an advantage of the ERGM, and it is, but it is also a disadvantage. Because an ERGM without any endogenous dependencies in its specification reduces to a standard logistic regression model, the researcher must model the endogenous dependencies and must also specify them in a complete and correct manner (otherwise, the model will be misspecified). This places an increased burden on the researcher, who may be primarily interested in the role of an exogenous covariate, to accurately model the generative structure of the network. Researchers wishing to avoid this additional modeling duty will find techniques like the QAP and latent space model (LSM) more appealing than the ERGM because such models condition out dependencies without requiring the researcher to specify them. A second disadvantage of the ERGM is that it can be prone to numerical instability in the estimation process, even when there is a theoretically intuitive specification. In particular, this problem makes the effective estimation of model specifications that fit the data very poorly difficult to achieve. This is perhaps a blessing in disguise, as it means that poor models cannot be fit (Cranmer and Desmarais 2011), but making sure that one's model fits the data reasonably well can be challenging, especially for networks in which the density of connections is either very sparse or very dense.            "
"25","Latent space models were introduced by Hoff, Raftery, and Handcock (2002) and operationalize dependence between observations through the notion of a k‐dimensional “social space.” The idea is that relations are transitive: If vertices i and j are tied and vertices j and k are tied, i and k are also likely to be tied directly. The positions of vertices in the latent space retain this transitivity in terms of the distances between vertices: Vertex pairs with a large (Euclidean or other) distance between each other in the latent space have large path distances in the observed network while proximate vertex pairs in the latent space have small path distances in the observed network. Latent space models are generalized linear models on dyadic observations that control for these dependence structures by conditioning on the distance between the vertices' latent space positions.            "
"26","In these models, the observations  in network matrix N are conditionally independent given dyadic covariates , their parameters , and the positions  and  in the latent social space (for details on this exposition, see Hoff, Raftery, and Handcock 2002). Thus, the probability of observing the network is the product of the individual probabilities for each dyadic observation, given the covariates, their estimates, and the latent space positions of i and j:               "
"27","As in the ERGM and QAP context, the  covariates can include relational information (e.g., do i and j have the same attribute value?) or information on any of the two vertices involved (e.g., an attribute of i irrespective of the attribute value of j or an attribute of j irrespective of the attribute value of i). There can be multiple covariates, and all covariates are saved in matrices conforming to the dimensions  of the outcome network matrix N. This means that the practical data preparation needed to run a latent space model is identical to that needed to run an ERGM.            "
"28","The dyadic tie probabilities  can be expressed as log odds, which results in a logistic regression equation. The probability of each pair of vertices  being tied is a function of the distance between i and j in the social space. This distance is most often the Euclidean distance between the positions  and  over the k dimensions of the latent space:               "
"29","The goal of the estimation process in a latent space model is primarily to find a distance matrix D that satisfies the triangle inequality and thus maps the dependence structure in the network matrix to a matrix of distances where adjacent vertices in the network have smaller distances than nonadjacent vertices, controlling for the covariates. This is done by maximization of               "
"30","For illustration, suppose the network of international militarized interstate disputes is to be modeled. First, one must define the relational conflict matrix (N) and the covariate matrices X (e.g., trade, contiguity). Second, one computes a matrix of, for example, the path distances in the conflict network. Third, one recovers each country's coordinates in a low‐dimensional space from the path distance matrix by multidimensional scaling (for a primer, see Jacoby and Armstrong 2014; Rabinowitz 1975). Fourth, one uses these rough positions as starting values for nonlinear maximization of the log likelihood in conjunction with a Euclidean model of space (as defined in Equation 8). This yields estimates for the latent positions Z controlling for the covariates X. The covariate estimates can be interpreted substantively because network dependence has been controlled for via the estimated latent positions, and the positions can be used for description of the network, similar to descriptive uses of techniques like multidimensional scaling (Jacoby and Armstrong 2014; Rabinowitz 1975) when applied to network data.            "
"31","Compared to the QAP and ERGM, latent space models have a number of advantages and disadvantages. Often cited as an advantage is the fact that latent space models can be applied to outcome networks with binary or valued data of any distributional shape. Yet the same is true for the QAP, and a recent ERGM advance in the form of the generalized ERGM (GERGM) renders the same true for ERGMs (Desmarais and Cranmer 2012a).            "
"32","Latent space models relegate network dependencies to the latent social space and estimate them automatically. This makes latent space models easy to use. Yet the model is less flexible than an ERGM because substantive theory related to the dependencies cannot be tested. In this sense, latent space models have a similar strength (and weakness) as the QAP. Yet one is required to specify the distance measure and the number of dimensions of the latent social space, which puts latent space models between the QAP and ERGMs with regard to the theoretical elaboration and required user skills. The choice of the distance measure and the number of dimensions is rarely guided by theory; it is a trade‐off between model fit and parsimony because more dimensions generally accommodate the dependencies better, but each additional dimension introduces nonlinearly more parameters. For illustration, a latent space model typically has an intercept, several coefficients for covariates, and n parameters per dimension of the latent space. This reveals a potential disadvantage of latent space models vis‐à‐vis the QAP and ERGM: They explain the same network structure but need many more parameters and are therefore less parsimonious. This is especially problematic in situations where few vertices are present because there may be ultimately more parameters than observations.            "
"33","Finally, interpretation of the coefficients for exogenous covariates may not always be straightforward: If the latent positions or distances are correlated with exogenous covariates, the coefficients may in fact indicate the opposite direction of the actual effect in the presence of the confounding positions or distances. While endogenous model terms may be correlated with exogenous effects in ERGMs as well, these problems are easier to detect and fix in ERGMs.            "
"34","Weighing these advantages and disadvantages, latent space models are an attractive choice if there are no or few isolates, if parsimony is not of primary importance, if there are enough observations, if the interdependencies are not theoretically interesting, and/or in situations where it makes sense to visualize and interpret the latent positions (e.g., political ideology). There are better models available when the dependencies are substantively interesting or when many isolates or few observations exist."
"35","To illustrate the strengths and weaknesses of the three models, we apply each to a simple, cross‐sectional policy network on relations between 34 political actors in the Swiss climate change mitigation network (Ingold 2008). We reanalyze her data set with a logit model and the network methods described above. Our results show that all three network models outperform the standard logit estimates on multiple criteria, with the ERGM performing best overall.4"
"36","The directed outcome network reflects collaboration among four government agencies, five political parties, six scientific/research organizations, 11 organized interest groups (private‐sector and business associations), and seven environmental nongovernmental organizations (NGOs; Ingold 2008; Ingold and Fischer 2014).         "
"37","We follow Ingold and Fischer (2014) with respect to model specification. See the supporting information for a detailed theoretical discussion. We break down our exogenous specification by theoretical mechanism.               "
"38","Transaction Costs. The literature suggests other mechanisms for collaboration between actors, such as transaction costs associated with acquiring contacts (Leifeld and Schneider 2012).                        "
"39","Political Influence. Actor i tends to collaborate with j if i deems j influential in the policy process because this is instrumental for achieving policy objectives (Ingold and Leifeld 2016).                        "
"40","Functional Actor Role Requirements.                        "
"41","The QAP and latent space models do not require or allow explicit specification of endogenous dependencies; only the ERGM does that. The downsides of this flexibility are that a theoretical understanding of the endogenous part of the data‐generating process is required, and the analyst must be familiar with the different ways that dependencies can be modeled. In the ERGM reported below, we include the following endogenous model terms.               "
"42","Before comparing the substantive results of the four models, it is important to assess their respective fit. One of the many advantages of network analysis is that it is generally a simple matter to check the overall fit and performance of a model. Goodness‐of‐fit assessment, prior to consideration of the substantive results, is important. After all, if the model fits the data poorly, one would not expect the results to have any bearing on the data‐generating process, and so they would be of no interest to the analyst. We consider that a model should pass at least a basic test of its fit to the data prior to the analyst investing the time to understand that model's substantive results."
"43","Figures 2, 3, 4, and 5 plot several characteristics of model‐based simulations against the observed collaboration network for each of the models. This is an easy way to determine whether a model captures network dependencies. For example, simulated networks should have the same distribution of path distances or roughly the same numbers of shared partners per dyad as the observed networks. Failing to capture the dependencies is a form of omitted variable bias. The diagrams show distributions of several typical endogenous network properties that can serve as benchmark criteria for comparing the observed network with 100 simulated networks based on the model and the covariates. If the model captures the dependencies well, the black line, which represents the values of the observed network, should pass approximately through the medians of the boxplots, which represent the distributions of the network statistics in the 100 simulated networks.            "
"44","The estimated ERGM (Figure 5) is relatively accurate with regard to these auxiliary statistics, with very few exceptions: Indegrees of 4, 5, and 16 and outdegrees of 6 are slightly underrepresented in the ERGM, but adjusting the model to accommodate these oddities would likely mean overfitting the data. The logistic regression (Figure 2), in contrast, displays much stronger deviations between simulations and the observed network. The edge‐wise shared partner, indegree, and outdegree distributions of the simulations and dyads with zero shared partners are not in line with the observed network. The QAP model (Figure 3) and the latent space model (Figure 4) fare between the logit and the ERGM in terms of endogenous fit: In the latent space model, indegree and outdegree, and edge‐wise shared partners are more accurate than in the logit model, but not as accurate as in the ERGM; zero dyad‐wise shared partners fit well (in contrast to the logit model), but shortest paths of length two and infinity are slightly off the mark. The QAP model has similar fit to the logit model; only in‐stars fit somewhat better. The conclusion from this goodness‐of‐fit assessment is that the ERGM is largely unbiased because its endogenous network dependencies are correctly specified, whereas latent space, QAP, and logit models are less accurate, in descending order of quality.            "
"45","ERGMs may yield a better fit, but does the difference between models affect the substantive results? Table 1 shows the estimated coefficients for all four models. While many results are consistent across different models, there are several coefficients that differ in their magnitude, significance, and even direction across models. In other words, different modeling strategies sometimes result in different substantive conclusions, which renders model selection based on endogenous fit an important task.            "
"46","For example, the Business vs. NGO effect can be confirmed in all of the network models, whereas the logit model shows an insignificant effect due to unmodeled dependencies. This highlights our point that appropriate treatment of dependencies between observations can make a substantive difference.            "
"47","On the other hand, there are profound differences between the three network models. For example, the Alter = Government actor model term has a positive but insignificant effect in the QAP model, a positively significant effect in the ERGM, and a negative and insignificant effect in the latent space model. It is unclear in principle which model is “right.” Yet, given that the dependencies have been most accurately captured by the ERGM, the positively significant effect in the ERGM seems most trustworthy. It is also in line with the theoretical prediction.            "
"48","Another important observation is that the latent space model deviates in many cases from the results of the QAP and ERGM. This happens in cases where a model term is correlated with the latent position of a vertex. The latent space satisfies the triangle inequality: Vertices with a path distance of g are placed closer to each other than vertices with a path distance of , for any g. The consequence is that central actors are placed centrally in the latent space because they have smaller average path distances to any other vertex. Examples are the Swiss Agency for the Environment, Forests and Landscape (BUWAL) and the Swiss Petrol Union (EV), which are at the center of the latent space (see the actual latent space in the last panel of Figure 4) and also have large indegree, closeness, and betweenness centrality scores. Since many government actors are central players, the Euclidean distances pick up the government covariate as part of the dependencies. The consequence of this trivariate correlation between distance in the latent space (or centrality in the network), the link receiver (alter) being a government actor, and, link sender (ego) sending information to the receiver is that controlling for latent distances reverses the coefficient for the covariate (or makes it insignificant in other cases). This is why we see a negative coefficient for the receiver being a government actor, whereas the QAP and the ERGM show the opposite. Thus, while latent space models offer an easily interpretable visual representation of social distance (last panel of Figure 4), this may also cause interpretability issues when covariates are correlated with the latent space and the outcome network. Further, including random sender and receiver effects, as is possible in the standard model, does not rectify the problem. A potential remedy is to use specifications of the latent space that explicitly correct for the activity and prominence of vertices via a bilinear model (Hoff, Raftery, and Handcock 2002; Hoff and Ward 2004).            "
"49","The Bayesian information criterion (BIC) at the bottom of the table indicates that the ERGM offers the most preferable combination of model fit and number of parameters. The BIC can be compared across the different models because the same data are modeled. The QAP is slightly better than the logit, and the latent space model—even though it does a better job of capturing the dependencies than the logit and QAP—has the largest BIC value because of its many parameters."
"50","Overall, the models confirm most of the hypotheses. The analysis confirms earlier findings that preference (dis)similarity is rendered unimportant if transaction cost considerations in terms of joint forum participation (Leifeld and Schneider 2012) and structural properties of the network are properly taken into account. Moreover, we find that collaboration in policy networks is a function of conflicting actor roles, influence attribution, and functional requirements.            "
"51","Given the advantages and disadvantages of the models considered here, which model is preferable? Table 2 summarizes the characteristics of the models. Four criteria, in no particular order, should be considered: parsimony, interpretability, theoretical focus, and ease of use.            "
"52","With regard to parsimony, the latent space model is the least appealing because, in addition to the covariates, a separate parameter is estimated for each actor's latent position times the number of dimensions of the latent space. This is reflected in the relatively bad BIC value reported in Table 1. The QAP is the best choice with respect to parsimony because only the 12 (exogenous covariate) parameters are used. With QAP, the data can be described using relatively little external information. The ERGM has additional parameters for the endogenous dependencies, but it is still relatively parsimonious.            "
"53","With regard to interpretability, the QAP fares worst because dependencies cannot be inspected at all. Moreover, one must be careful about bias when the network to be modeled is sparse and some predictors are collinear; such cases may be common because many empirical networks have low density. The latent space model allows one to visualize the latent space and interpret it substantively, and the ERGM provides parameters for each endogenous part of the data‐generating process; both the latent space model and the ERGM allow simulation‐based goodness‐of‐fit assessment. This is also possible with the QAP, but due to the way the QAP corrects for dependencies rather than modeling them, simulations based on a QAP model do not include any complex dependencies that may be important for the data‐generating process. This is reflected by the relatively bad endogenous model fit in Figure 3. The case study also illustrates that interpretability of the latent space model can be limited in some situations where covariates have to be evaluated as predictors of edges independently of the latent space, at least when Euclidean distances are used or when a bilinear model without activity and popularity corrections is employed.            "
"54","With respect to theoretical focus, the ERGM is the only real option if a substantive theory about the types of dependencies shall be tested; neither the latent space model nor the QAP allow for hypothesis testing of different, nuanced dependency structures. On the other hand, if the goal is to test substantive theory about the exogenous covariates only and the dependencies are merely a nuisance, the QAP and the latent space model allow one to do this without having to worry about the structure of the dependencies. It would be presumptuous for us to suggest that one model is superior to the others. The choice must depend upon the aims of the research, the theory, and the nature of the data. To the extent that an analysis is mainly concerned with network interdependency as a threat to analysis or wants to get a “clean” test of the relationships between the outcome network and exogenous covariates, the QAP is often sufficient. If an analyst wants a model that can account for omitted variable bias, can be interpreted graphically, and can be used for more than simple dyadic connections, use of the latent space model would work well. Finally, the ERGM is potentially the most powerful model when network structure is of interest in addition to the covariates, but it is also the most difficult to apply from an end user's perspective."
