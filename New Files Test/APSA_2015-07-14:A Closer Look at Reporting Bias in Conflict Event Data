"","x"
"1","Due to their broad coverage and instant availability, media reports have become the most important data source for larger data collections on political violence. However, as is well known, media coverage of world events can be biased in many ways. Earl et al. (2004) distinguish between (i) the veracity problem or description bias, and (ii) the selection problem. Veracity concerns the way that events are portrayed in the media. For example, Davenport (2010) documents the variation in news reporting about the Black Panther party in the United States. What he calls the “Rashomon Effect” is the finding that different newspapers, depending on their partisan orientation, can portray the same event in completely different ways.         "
"2","The second problem, selection, is at the core of the present analysis. It refers to the fact that media outlets choose to report on certain events, but omit others. Similar to the “Rashomon Effect,” this can be due to demand: The audience of the respective outlet may be interested in some events but not others. Another reason for selective reporting is supply: Many events will not be reported because information about them never reaches a reporter (e.g., due to the event happening at a remote location). Both effects have been examined in the literature. One of the first systematic treatments is by Galtung and Holmboe Ruge (1965) and establishes variation in the coverage of international crises across newspapers in Norway. Since then, a number of scientific analyses have followed in their wake, some of which specifically deal with the reporting of violence. Davenport and Ball (2002) find considerable differences across sources; among other findings, the article identifies underreporting of violence by newspapers in rural areas as compared to human rights reports and interviews. Underreporting is also a concern in the study by Drakos and Gofas (2006), who analyze the coverage of terrorist incidents across regime types. They conclude that there is evidence for underreporting of terrorism, in particular, in countries with low levels of press freedom, which can partly explain why democracies have higher observed rates of terrorism.         "
"3","How does reporting bias (or selective reporting) play out in quantitative analyses, in which violence is usually the dependent variable? If media reporting is selective in the sense that it fails to identify some events that really occurred, the error would be such that some of our violent units would incorrectly be coded as peaceful. Thus, selective reporting does not cause selection bias (where we fail to observe certain units), but rather leads to measurement error in the dependent variable (violent units show up as peaceful). As is commonly known, as long as the measurement error is uncorrelated with the independent variables, measurement error in the dependent variable is not particularly problematic in a standard regression framework other than increasing the uncertainty around the estimates we obtain (see, e.g., Wooldridge 2006). Therefore, in the worst case, we may fail to see a statistical effect where in reality there is one. Things become more complicated when measurement error in the dependent variable is systematic, that is, correlated with one (or more) of the independent variables. In this case, coefficients for these variables are biased depending on the direction and magnitude of the correlation, something we will explore in detail below.         "
"4","Given that reporting bias may be an issue when working with media‐based conflict data, what can we do? Essentially, there are four possibilities. The first one is to do nothing, and assume that the bias is either uncorrelated with the main explanatory variables, or if not, at least not strong enough to change the conclusions. The majority of work using media‐based event data for subnational analyses falls under this category. For example, Raleigh and Hegre (2009) study the impact of population concentration on violence, implicitly assuming that the reporting of violence is uncorrelated with population density. Similarly, the frequently reported strong effect of accessibility (measured as distance to roads, see, e.g., O'Loughlin et al. 2012) could be partly due to reporting being more likely in areas with high road density. A more recent example—which we will take a closer look at below—is Pierskalla and Hollenbach (2013) and their analysis of cellphone coverage and violence. Although the authors do discuss the problem of reporting bias, they assume that the design of their study (e.g., using a dichotomous measure of violence rather than a count variable) makes pernicious effects of reporting bias unlikely.         "
"5","A second possibility is to fix deficiencies in the data using capture‐recapture methods developed in biology (MacKenzie et al. 2002). This approach was developed to estimate the size of an animal population across some area, where perfect counts are impossible to do. Therefore, different samples are conducted, where each time an animal is captured, it is marked in order to recognize it during a later sample. From the overlapping captures across the different sampling rounds it is possible to estimate the joint detection rate, and thus, the total population in a particular area. The approach bears some resemblance to multiple systems estimation, which has been used to compute overall casualty rates from multiple incomplete sources (see, e.g., Ball et al. 2003). More recently, Hendrix and Salehyan (2015) apply capture‐recapture estimation to estimating the number of protest events coded from different news sources, each of which has particular substantive and geographic interests that may bias reporting. This approach, however, may not be suitable for problems where all sources are similarly prone to a particular bias: If, for example, all news reporting is similarly affected by the availability of some communication technology, capture‐recapture estimation across different news outlets cannot fix this.         "
"6","The third possibility is to assess the susceptibility to bias using sensitivity analysis. Here, the approach is to specify what factors could possibly affect over‐ or underreporting, and how severe this effect could be. Using Monte Carlo experiments, it is then possible to gauge whether—given the assumptions about reporting—coefficients are based in ways that would fundamentally alter a study's conclusion (Gallop and Weschle 2014).         "
"7","A fourth possibility is to correct for potential bias statistically. For binary models that are usually the preferred approach in conflict research, this can be done in different ways. Hausman, Abrevaya, and Scott‐Morton (1998) develop a maximum‐likelihood estimator that can correct for misclassification, that is, the fact that some cases that are coded as peaceful should really be coded as “conflict.” Hug (2010) demonstrates that this model works well when applied to macro‐studies of civil war and other phenomena. A related technique for dealing with selectively reported conflict events is through bivariate probit/logit models with partial observability, which can be used to represent both the violence‐generating and the reporting process (Poirier 1980). Although appealing at first, these statistical techniques have not been widely applied due to different issues. For example, the estimators presented have mostly been tested on artificial data, where the data generating process was defined to be suitable for the respective estimator, and there have been no tests on a real case of selectively reported data where the true values are known. Also, the estimators are oftentimes difficult to apply in practice; as Hug (2010) and Meng and Schmidt (1985) note, identification is a key issue, and strong assumptions are necessary to make it possible.         "
"8","In sum, for micro‐level analysts of violence, there is currently little that can help them tackle the potential problem of reporting bias in their studies. Most importantly, almost all solutions that are offered are based on assumptions that are difficult to defend: We can either assume away the problem and claim that it does not exist; we can conduct sensitivity analyses that assume a certain direction and magnitude of selective reporting without any evidence, or we can plug in a statistical estimator that requires strong statistical assumptions in order to fix the problem. Also, the few studies that present feasible approaches mostly exploit variation in coverage between states, as for example Drakos and Gofas (2006) on the underreporting of terrorism across regimes. This, however, is obviously of little use for scholars studying subnational variation in violence. Hence, there is a strong need to make progress along three fronts. First, there is the need to build up intuition of the problem for subnational analyses—scholars need to be aware that reporting bias does not only apply to variation between countries, but also within. Second, we need quantitative assessments of bias. How much of the violence do we actually “see” in the international media, and how is that selection determined by local factors? We simply do not know. Third, if scholars are afraid that reporting bias may be affecting their results, they need a simple diagnostic procedure to find out. In this article, we aim to address these three issues.         "
"9","The effects of measurement error from selectively reported violence are best illustrated using a simple experiment using artificial data. In doing so, the experiment uses the standard design most quantitative studies on political violence rely on. The original data on violence usually comes in the form of individual events. In order to test which local factors— for example, cellphone coverage—affect the risk of violence, the events are typically aggregated at the level of geographic units, such as cells (Pierskalla and Hollenbach 2013) or administrative districts (Weidmann and Ward 2010). With violence being a rare event, the event count is usually collapsed to a dichotomous variable, such that each unit either experiences violence or not. The remainder of this article analyzes the effects of selective reporting for this setup.         "
"10","Monte Carlo experiments are an ideal way to explore the implications of selective reporting. With both the violence‐generating and the reporting process known, we can show how different reporting mechanisms affect the estimates when naively running a simple model on the reported subset of the total violence. The description here summarizes briefly the setup and the results from such an experiment; all details can be found in the online appendix (Part A). For the experiment, a dataset of 5,000 cases is created that contains two variables: a binary dependent variable y (corresponding to violence 0/1), and a single continuous independent variable x1. With x1 randomly drawn from [0,1] and the model coefficients β0 and β1 set to ‐1 and 0.5, respectively, y is drawn according to a standard logistic function. In the experiment, we are interested in how well a standard logit model can recover the true effect of our independent variable x1 on y under different experimental conditions that vary the quantity and the determinants of reporting. In the experiment, this means that only a fraction of violence is correctly observed; in all the other cases, y is incorrectly observed as having value 0.         "
"11","The baseline condition of the experiment (see Appendix A.1) implements random reporting, that is, the observed instances of violence are selected randomly. Here, the experiment shows that relative marginal effects—the relative change in the predicted probability  for a unit increase in x1—are not biased, even though they are estimated with higher uncertainty the less violence we observe. In other words, if media sources randomly miss a number of conflict events, this would be unproblematic for our analysis. However, this is different if we let our independent variable x1 affect whether we observe violence, as it could happen, for example, if cellphone coverage affects the probability of reporting. Specifically, in the experiment we let x1 affect reporting such that the probability of reporting is proportional to .3 In this specification, c controls the impact that x1 has on reporting—for , these weights are constant and reporting is independent of x1; for , the probability of reporting is linear in x1. Appendix A.2 shows an illustration of how different values of c affect the probability of reporting.         "
"12","Figure 1 shows the relative marginal effects we estimate in a logit model for different fractions of observed violence and different values of c. For example, if reporting is linear in x1 (right panel), observing 70% of violence would lead us to estimate a roughly five‐fold increase in the predicted probability of conflict, whereas the true increase is only 40% (dotted line). Observing more violence (going from left to right in each panel) decreases bias. The bias also decreases as reporting becomes more weakly related to x1 (center and left panels). Additional results in Appendix A.3 show that this can even induce an effect where in reality there is none. In the next section, we will see if and how selective reporting affects quantitative results in a real case.         "
"13","The obvious drawback of the experiments presented in the previous section is that they are entirely based on artificial data; while this helps illustrate how selective reporting can affect our results, it is unclear if and how this applies to a real case. In the following section, the article conducts an empirical analysis of selective reporting and bias in a real case. It does so by focusing on a recent article by Pierskalla and Hollenbach (2013) that analyzes the relationship of cellphone technology on insurgent violence. Pierskalla and Hollenbach (2013) find that cellphone coverage at the subnational level across Africa is associated with a higher likelihood of violence, which they attribute to increased capabilities for insurgent collective action introduced by communication technology. However, as the above experiment has shown, the positive effect in the analysis could be driven by the use of media‐based conflict event data: If cellphone coverage makes the reporting of violence more likely, this can lead to measurement error in the dependent variable that is correlated with the main independent variable.         "
"14","At this point, however, this is just a suspicion. In order to find out whether there is some truth to it, we now need to test (i) whether reporting is really affected by cellphone coverage, as has been suggested, and (ii) if so, whether the statistical relationship between cellphone coverage and violence is indeed driven by this. For this analysis, we need two kinds of data. First, a “complete” dataset of violence on the ground, and second, a dataset of media‐reported events on the same conflict. This allows us to see which events make it into the international media, and therefore, into current media‐based event datasets, but also whether the effect of cellphones on violence changes depending on whether we use the full dataset or the media‐reported one."
"15","“Complete,” uncensored data on violence on the ground are very difficult to get. However, the case of Afghanistan provides us with an opportunity to obtain information on violence that comes close to the universe of cases. The data we use for this exercise is taken from the “Significant Activities” (SIGACTS) military database. SIGACTS were routinely collected by U.S. and Coalition forces in recent conflicts, such as Afghanistan or Iraq, in order to log activities of interest to the military. The data were collected for analysis and evaluation in the military, but have already been used in academic work (see, e.g., Berman, Shapiro, and Felter 2011; or Condra and Shapiro 2012). They include violent confrontations such as insurgent‐initiated attacks (“enemy action”), but also violent action initiated by friendly forces (U.S. or other members of ISAF, and Afghan security organizations). Following Pierskalla and Hollenbach's reasoning, however, we focus on insurgent‐initiated violence, and therefore, restrict the analysis to insurgent‐initiated lethal attacks.         "
"16","In order to find out which of the SIGACTS events made it into the international media, the SIGACTS were linked with an event dataset based on media reports, a preliminary version of the “Geo‐referenced Event Dataset” (GED) for Afghanistan (Sundberg and Melander 2013). The GED does not normally rely only on media reports; instead, it also draws on other sources such as NGO reports. However, the preliminary Afghanistan coding is entirely based on global media sources such as AP, AFP, and Reuters, and thus, serves as valid approximation of the violence we can observe through media sources. The official version of the Afghanistan GED that will soon be released includes more sources and improved geo‐referencing, and may therefore be able to alleviate some of the biases discussed here. For each GED event, its match in the SIGACTS dataset was identified. Matching was a tedious process; oftentimes, due to slight differences between real and reported events, coordinates such as district and date fail to identify the correct match, which means there is almost no way to automate it. Therefore, matching was done manually. Starting with the event's province and the date it occurred, the SIGACTS database was inspected for events that correspond to the given province and date. If this failed to produce a match, the search window was enlarged step by step to previous/future days, and adjacent provinces. As the results in Weidmann (2014b) show, this procedure even identified matches if the correct and the reported location are hundreds of kilometers away.         "
"17","A match was established based on the type of event, the involved actors and military units, the approximate number and type of casualties, and the description of the event. For the latter, the matching relied on the event narratives available in the news report and the SIGACTS dataset. The analysis was limited to one year (2008) with relatively constant military presence in Afghanistan (Livingston and O'Hanlon 2012), in order to avoid major fluctuations in military reporting introduced by variations in troop numbers. Weidmann (2014b) shows an example of matched event. It is important to note that the SIGACTS do not provide a complete picture of all the violence. Out of 286 events in the preliminary GED that could not be matched with SIGACTS (of 720 events in total), only 8 were about insurgent activity—the 278 remaining ones cover raids and air missions initiated by U.S. and Coalition forces. The finding that only about 2% (8/362) of all insurgent‐initiated events could not be matched suggests that the available SIGACTS version has almost perfect coverage of violent action initiated by insurgent groups (which is what we need for our analysis) and can serve as an approximation of the “universe” of violence. At the same time, it has large gaps when it comes to covering U.S. or ISAF‐initiated actions. In total, out of the 1,244 insurgent‐initiated events in 2008 that involved at least one casualty, only 354 (28.5 %) eventually made it into the international news. Thus, with media‐based data for Afghanistan, we capture less than a third of the violence that actually occurs on the ground.         "
"18","Equipped with a dataset of matched events, we are now ready to proceed to a first analysis of the determinants of reporting in the international news. In particular, we are interested in the extent to which reporting is driven by cellphone coverage. In order to test this, we run a series of logit models on the set of 1,244 lethal SIGACTS events. The dependent variable is coded as 1 if an event was reported in the international news, and 0 otherwise. Our main independent variable, cellphone coverage, is derived from the Mobile Coverage Explorer (MCE) dataset released by Collins Bartholomew (2014). MCE represents mobile coverage globally using electronic maps in GIS format, based on provider information. From MCE, we use a complete coding for Afghanistan that is closest to the year of our analysis. Using the spatial coordinates of an event, we determine whether the event happened at a location with coverage, in which case the variable takes the value 1. MCE is the same dataset that was used in Pierskalla and Hollenbach (2013) to derive their main independent variable.            "
"19","The analysis controls for two other types of factors that likely determine reporting. First, the severity of an event and the type of casualties is likely to affect whether it is considered newsworthy. We therefore include two control variables: (i) the (logged) absolute number of casualties, and (ii) the proportion of Coalition (“friendly”) fatalities of an event. Both variables are included in the SIGACTS dataset.4 Second, remoteness of the event location may have a strong impact on whether information about is relayed further. Adequately controlling for remoteness is important in order to avoid a spurious effect of cellphone coverage, since coverage is less likely to exist in remote areas. Therefore, we use three alternative indicators of remoteness: the distance of the event to the nearest town (5,000 people or higher), the distance to the nearest city (25,000 people or above), and the population density at the event location derived from the LandScan population dataset for 2008 (Oak Ridge National Laboratory 2008). All values of these independent variables were logged. Appendix B reports the correlations between the predictor variables, indicating that none of them achieves levels that are problematic for the regression. Results of the regression analysis are reported in Table 1.5"
"20","The results in Table 1 confirm our expectations. Cellphone coverage has a positive coefficient on reporting throughout. This effect is significant across all models. Controlling for remoteness and event severity, the predicted probability of reporting in Model 1 increases from 0.216 for events without cellphone coverage to 0.275 for events in covered areas (an increase by about 30%).6 Also, not surprisingly, we see that event severity is a strong predictor of reporting throughout all models: An event with three casualties has a probability of reporting that is roughly twice as high as for an event with only one casualty. Similarly, the type of casualties matters: In Model 1, comparing an event where half of the casualties were Coalition soldiers to one where all of them were, the reporting probability increases by around 60%. The three indicators of event remoteness also have the expected effects: Increasing distance to towns or cities decreases the likelihood of reporting, while higher population density makes reporting more likely. For Model 1, the reporting probability decreases by roughly 40% when going from the 5th to the 95th percentile of the distance variable.            "
"21","The “Area under the Curve” (AUC) statistics provides an estimate of the predictive power of the models (Ward, Greenhill, and Bakke 2010). Within the range of 0.5 (no predictive power) to 1 (perfect predictions), the models achieve intermediate values, which suggest that they explain a fair amount of the variation in reporting. Importantly for our purpose, however, these results provide empirical support for our first suspicion, that the reporting of violence is partly correlated with cellphone coverage. Now that this has been confirmed, we need to establish if and to what extent selectively reported violence is driving a positive effect of cellphones in statistical models.            "
"22","Using the data on Afghanistan introduced above, we now conduct a simple analysis of the effect of cellphone coverage on violence. Our task in this section is to test whether a positive effect of cellphones exists, and if so, whether it is partly (or entirely) a result of selectively reported violence. We proceed in the standard fashion by aggregating the conflict event data to subnational geographic units. As Pierskalla and Hollenbach (2013) in their first set of models, we work with a cross section in a single year (2008) and a dichotomous dependent variable (conflict 0/1), which, however, is calculated based on different datasets (the preliminary GED, the SIGACTS, or a random subset of the latter). The main independent variable is cellphone coverage, measured as the share of the area in each district that has cellphone coverage, and is again computed from the Mobile Coverage Explorer dataset (Collins Bartholomew 2014).            "
"23","The analysis includes a standard set of control variables. The district population is computed by aggregating values from the LandScan spatial population dataset (Oak Ridge National Laboratory 2008). We measure remoteness by computing the district's distance to the nearest major city of 25,000 people or above. Finally, in order to control for spatial dependence in violence, we include a spatial lag of violence as the number of events in the neighboring districts.            "
"24","Model 4 in Table 2 replicates the basic setup of the main models in Pierskalla and Hollenbach (2013). It uses a dependent variable and a spatial lag, both computed from the preliminary GED dataset that is based on media‐reported violence. The estimates confirm the earlier results: Cellphone coverage seems to be positively and significantly related to violence. The controls also point in the expected directions: According to Model 4, violence is more likely in more populous areas, while remoteness increases the risk of conflict. The positive spatial lag confirms the frequent finding that violence is highly spatially correlated. Thus, based on Model 4, we would conclude that cellphones foster violence.            "
"25","This conclusion changes once we plug in more complete data on violence, the SIGACTS (Model 5). Estimating the same logit model on these data, we find no effect for cellphone coverage; the coefficient is close to zero. While from Model 4, we would conclude that cellphone coverage roughly doubles the probability of violence (an increase from 0.261 to 0.481), this effect is nonexistent in Model 5 (an increase from 0.572 to 0.583). One may object that with complete data on the incidence of violence, we no longer need to collapse the dependent variable into a binary one. Instead, a count model using event counts as the dependent variable could be more appropriate. In Model 6, we estimate a negative binomial model on the SIGACTS data, but again, no significant positive effect of cellphones on violence emerges."
"26","While the results in Models 4–6 give reason to worry, one may argue that the positive effect of cellphone coverage for the media‐based violence data is simply a chance result, due to the fact that the reported subset is only one realization of selective reporting. In other words, we need to find out whether this positive effect occurs systematically when taking into account the uncertainty around the selection of events that are actually reported. To this end, we devise a simple Monte Carlo simulation based on the complete violence data and the results from Model 1. The basic idea of this simulation is to propagate the uncertainty in the selection of reported events to the conflict model, by repeatedly drawing samples of “reported” events and estimating the conflict model on these. The simulation works as follows:               "
"27","Repeating the above procedure many times (in our case, 1,000), we obtain means and 95% confidence intervals for the conflict model coefficients, which are reported in the last two columns in Table 2. Model 7 reports the results for models estimated on selectively reported subset of all events. Again, cellphone coverage receives a positive effect that is significant at the 5% level. For comparison, we now repeat the simulation procedure, but rather than reporting being guided by the estimates from Model 1 above, we draw the subset of 30% reported events randomly. The effect of cellphone coverage remains positive, but becomes smaller and is no longer statistically significant. Therefore, it seems that the strong positive effect of cellphones is driven by violence that is selectively reported; a small but significant influence of cellphone coverage on reporting is sufficient to drive up its effect in the conflict model such that it reaches significance.9"
"28","The above results caution against the use of media‐based conflict event data for certain research questions as it can fundamentally alter the conclusion we draw. We have learned that in the Afghanistan case, reporting was significantly influenced by cellphone coverage, which is sufficient to induce a statistical relationship where there is no real effect. However, how can we tell whether a similar issue also applies to other analyses using media‐based data? This is the real question that many other users of this type of data face. Obviously, using a dataset that is less subject to reporting bias is not an option, simply because there is none available. This section discusses possible ways forward, using again the example of cellphones and violence as an example."
"29","A heuristic approach could be to use the insights gained in the empirical analysis above to assess the likelihood that reporting bias affects one's results. Applying this to the Pierskalla and Hollenbach (2013) study, we would have to estimate how reporting bias in Africa compares to the above Afghanistan results. As the Monte Carlo experiments have shown, both the proportion of the reported violence and the strength of its dependence on cellphone coverage critically affect the magnitude of reporting bias. For reporting bias in Africa to be much lower, we would have to assume that (i) the proportion of violence reported in the media is much higher, and/or (ii) the dependence of reporting on cellphone coverage is much lower than in Afghanistan. The first assumption is difficult to defend. Unlike Afghanistan, many conflicts in Africa have seen little international involvement and have been simmering at low levels of intensity for years. For example, the civil war in the Central African Republic has been ongoing since 2012, but rarely makes the international headlines. Existing research has established that U.S. troop presence has a strong effect of whether a country receives news coverage (Jones, Van Aelst, and Vliegenthart 2013), an effect that should be multiplied for Afghanistan given the multinational military presence in the country. In fact, when we compare the distribution of casualties in Afghanistan and Africa (see Appendix D), we observe that we have more information about small‐scale events for Afghanistan. While it does not fully confirm it, this is consistent with the assumption that media coverage tends to pick up more of the smaller events in Afghanistan than in African conflicts (again with the potential caveat that the Afghanistan coding is preliminary). Therefore, it is probably safe to assume that news coverage of many African conflicts is at least as poor as it is for Afghanistan, where only about a third of all violence is reported the media (see above). The second assumption may also be somewhat bold, but is more difficult to assess. We could reason that the expansion of cellphone coverage provided many areas with a fundamentally new quality of communication, since few landlines existed before. If this is so, reporting may be at least as dependent on cellphone coverage as it is in Afghanistan.         "
"30","This heuristic strategy based on speculation is clearly not satisfying, but we can do more. In the following, we implement a simple diagnostic procedure proposed briefly in Dafoe and Lyall (2015) that is able to tell whether biased reporting in media‐based event data could be a potential issue. Dafoe and Lyall (2015) propose to test observable implications that should hold if the effect of cellphones is completely or partly driven by selective reporting in conflict event data. The basic idea behind this procedure is that the dependence of reporting on cellphone coverage should be stronger the lower the severity of an incident. For example, a small event with one casualty is likely to go unreported due to difficulties in communication, but a major attack that leaves 15 people dead will be reported no matter whether cellphone coverage exists at the location of the attack. This means that if selective reporting affects our results, a positive effect of cellphone coverage should be weaker or even disappear if we analyze high‐fatality events as compared to low‐fatality ones, since the former will suffer less from reporting being driven by cellphone coverage. The advantage of the procedure presented here is that it can be implemented based solely using the media‐based data; the severity of an event is a variable that is included in most of these datasets.         "
"31","In order to carry out this test, the first step is to replicate the analysis by Pierskalla and Hollenbach, in particular, the generation of their dataset. They also rely on the GED events dataset introduced above for their coding of violence, but using the first official version (1.0) that covers only Africa. This, however, is a version that does not only rely on media sources, and should therefore be less susceptible to reporting bias. Their spatial unit of observation is given by the PRIO‐GRID dataset (Tollefsen, Strand, and Buhaug 2012), a GIS dataset that divides the globe into regular cells of 55 km by 55 km. The most important variables (cellphone coverage, violence, control variables such as population or distance to the capital) are coded at the level of these cells. The diagnostic test relies on the cross‐sectional models in Pierskalla and Hollenbach (2013), Table 1. The implemented data generation procedure resulted in an almost perfect replication of the results given in the original article.         "
"32","With the data processing routines in place, we are ready to conduct the diagnostic test described above. In particular, we implement this test by estimating the effect of cellphones on conflict using subsets of the media‐reported events in the GED.10 More precisely, we order the events by increasing severity, and then estimate the effect of cellphone coverage on violence in a sliding window of 50% of the events (N = 376), moving the window upward in steps of 10 events.11 We estimate regression models with the same specification as in Table 2, but using only the 376 events in the respective window.12 If reporting bias was indeed partly driving the results, the estimated effects should decrease as we base our estimation on events of increasing severity.         "
"33","The plots in Figure 2 confirm this. The top panel shows relative marginal effects and their 95% confidence intervals estimated for each of the windows. There is a clear trend downward as we move towards high‐severity effects (the average severity of the events in each window is shown in the bottom panel). For low‐severity events, we estimate a relative marginal effect of almost 3 (a three‐fold increase when comparing places without and with cellphone coverage). However, the estimated effect approaches 1 (which corresponds to no effect) for high‐severity events that should be the least likely to suffer from reporting bias. Overall, moving up from low‐ to high‐severity events, we estimate a positive and significant effect for about 75% of all windows tested. The average effect across all windows is between 1.5 and 2, which is comparable to what Pierskalla and Hollenbach (2013) report in Figure 2.13 Estimates from a linear probability model with country fixed effects indicate a similar trend (see Appendix C). However, the trend we observe does not necessarily indicate reporting bias; it is also consistent with heterogeneous treatment effects where insurgents benefit from cellphones for small events, but not large ones. If this were the case, however, a similar sliding window analysis should yield the same pattern for the SIGACTS data in Afghanistan, which are unlikely to suffer from similar reporting bias. Figure 3 shows the results, using again windows of 50% of the events (N = 622) and a stepsize of 15.         "
"34","Figure 3 indicates a very different pattern for the SIGACTS data. Estimates mostly oscillate around 1.5, but remain insignificant for almost all of the tested windows. Clearly, we do not observe a pattern where small‐scale events generate a positive and significant effect but large‐scale events do not, as the one in Figure 2 above. Therefore, this plot should be evidence against the suspicion that heterogeneous treatment effects of cellphone coverage drive the pattern observed for Africa. Still, it is important to emphasize again that the diagnostic test presented here tests an implication of reporting bias. While the failure to see the decline in effect sizes can give us some confidence that our results are less likely to be subject to reporting bias, observing the decline does not necessarily mean that they are, since other mechanisms could be responsible for it.         "
"35","Despite the strong reliance on data derived from international news reports, recent micro‐level analyses of political violence pay little attention to the problem of reporting bias. This is highly relevant for the emerging strand of literature focusing on the impact of ICT on violence and contentious action since these technologies are likely to affect reporting to a large extent. This article has shown that this neglect is unfortunate. If reporting is correlated with the independent variable, the estimated effects can be strongly biased, and even lead to erroneous conclusions. The article presents the first quantitative assessment of bias in a subnational analysis, taking the impact of cellphone coverage on insurgent violence as an example. Using a unique opportunity where we have reasonably complete information on events on the ground, the article was able to establish that (i) the reporting of insurgent violence in Afghanistan was crucially enhanced by cellphone coverage, and that (ii) the positive statistical effect of coverage on violence seems to be partly driven by the selective reporting of violence."
"36","The article further implements a simple diagnostic test that is able to detect whether one's results are potentially driven by reporting bias. The basic idea is to estimate models on different subsets of the event data, ordered by severity. Since reporting bias is less likely to affect events with high casualty rates, the estimated statistical effects should decrease as we move toward these events. Applied to the Pierskalla and Hollenbach (2013) analysis, this pattern emerges. The same analysis on the SIGACTS data in Afghanistan, however, fails to produce a similar pattern. Together, this suggests that their analysis may indeed suffer from reporting bias, which even a more comprehensive selection of sources (as in the GED Africa dataset) may not be able to fully eliminate. The approach presented is not limited to violence data; it could, for example, be implemented for protest event data, ordered by protest size. The advantage of this diagnostic procedure is its simplicity; if scholars find results corresponding to those presented above, they can still decide whether they are willing to apply one of the statistical estimators described above and accept the strong assumptions behind them.         "
"37","But nevertheless, it is important to emphasize once again the diagnostic nature of the approach presented here. While the results give some indication for reporting bias, they cannot conclusively refute the findings in Pierskalla and Hollenbach (2013)—without a dataset that is less susceptible to reporting bias, this is difficult to do. If scholars had access to such a dataset, it would not be necessary to rely on media‐based data at all. Therefore, the result should only encourage scholars to be cautious when it comes to the true effect of cellphones. We were able to show that the results for Afghanistan are partly driven by cellphone‐induced reporting bias, so that same problem is at least a realistic possibility for Africa, too. This general advice of caution certainly applies to other uses of media‐based conflict data. At the very minimum, scholars should think about whether and how their main independent variable(s) could affect reporting. In many cases, there will be no reason to worry. In others, reporting bias may be an issue. Whatever conclusion authors reach, the potential limitations of media‐reported data deserve explicit discussion in their scholarly work.         "
