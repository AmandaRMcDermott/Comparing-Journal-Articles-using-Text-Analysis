"","x"
"1","To measure political knowledge in surveys, researchers typically use a set of factual questions about politics. Previous research suggests that motivation should play a role in how people answer these questions as it affects how much political information people acquire (Delli Carpini and Keeter 1996; Luskin 1987). However, survey research and research on memory suggest that motivation of a different kind can also influence response quality.         "
"2","The pace of a survey interview is established in part by conversational norms (Schwarz 1996, chap. 5) and in part by the incentives of the interviewer (Blair and Burton 1987; Krosnick and Alwin 1987). Interviewers often have incentives (monetary and/or personal) to complete a certain number of interviews within a short period of time. Simultaneously, respondents often want to finish surveys quickly. Such dynamics can lead interviewers to move briskly from one question to the next and respondents to satisfice by offering answers without much thought.         "
"3","Moreover, many scholars treat political knowledge questions as if respondents necessarily exert sufficient effort to retrieve all relevant facts from memory. This treatment is manifest in claims that incorrect responses to knowledge questions constitute prima facie evidence that citizens are ignorant of the queried facts. The idea that satisficing during the survey interview contributes to bad performance is not considered. It should be.         "
"4","When asked to recall a fact in surveys, respondents draw upon a kind of memory known as “declarative memory” (see, e.g., National Research Council 1994). For declarative memory, there is a correspondence between the amount of effort one devotes to recalling facts and the range of facts recalled (Kandel, Schwartz, and Jessell 1995, 656–64; National Research Council 1994, 28–29). With minimal effort, a relatively small set of facts from declarative memory emerges. With greater effort, more facts can be recalled. Therefore, respondents may fail to answer a question correctly not because they lack the motivation to acquire the relevant information, but because they are not sufficiently motivated to think about the survey question. To the extent that existing political knowledge measures are based on limited draws from declarative memory, they are likely to be biased downward.         "
"5","An incentive for greater respondent effort may reduce this bias by encouraging respondents to base their answers on a more extensive search of declarative memory. One kind of incentive, commonly used in experimental economics, is a monetary incentive:"
"6","                        "
"7","“The presence and amount of financial incentive does seem to affect average performance in many tasks, particularly … where increased effort improves performance. Prototypical tasks of this sort are memory or recall tasks (in which paying attention helps) … which are so mundane that monetary reward induces persistent diligence when intrinsic motivation wanes.” (Camerer and Hogarth 1999, 8)               "
"8","In our first experiment, we use a monetary incentive to motivate more thorough searches of declarative memory. We offer randomly selected respondents a small monetary reward ($1) every time they answer a politically relevant knowledge question correctly. A control group answers the same questions under standard survey conditions—no payment for a correct answer. This experiment allows us to evaluate an important null hypothesis:"
"9","                        "
"10"," Null Hypothesis #1: Knowledge questions in conventional mass opinion surveys accurately assess whether or not respondents hold the relevant political facts in memory. Providing an incentive for correctly answering knowledge questions will not affect the likelihood of offering a correct answer.               "
"11","Regardless of the precise nature or magnitude of the incentive, rejecting our first null hypothesis would demonstrate that typical survey procedures do not elicit all that respondents know. In that case, we could conclude that people acquire and store more political information than previous research suggests."
"12","A second attribute of memory also affects how we should interpret political knowledge data. Cognitive psychologists distinguish fact‐based declarative memory from rule‐based “procedural memory.” Procedural memory is the long‐term memory of skills and procedures.1 It “accumulates slowly through repetition over many trials, is expressed primarily by improved performance, and cannot ordinarily be expressed in words” (Kandel, Schwartz, and Jessell 1995, 658). Knowing where and how to find things is an important form of this kind of memory. To figure out which candidate they prefer or how they feel about a new policy proposal, many people draw on procedural memories of how to gather information that might help their decision.         "
"13","There are circumstances when using procedural memory to find information is at least as important a performance criterion as the ability to recall facts instantaneously. In many aspects of life, people expand their capabilities by using file drawers or computers to organize large amounts of information in ways that permit quick retrieval when they need it. But most political surveys offer people no opportunity to draw on analogous sources of political knowledge, even though they can do so when making political decisions."
"14","Unlike declarative memory, procedural memory cannot be observed directly in a traditional survey. But we can observe its consequences if we adopt a different measurement approach. Whereas the quick pace and incentives of many surveys inhibit respondents from using procedural memories, our second experiment allows some of our respondents to utilize that resource. In it, we give (a randomly selected) half of our respondents only one minute to answer each question, whereas the other half can take 24 hours to respond. This variation changes a knowledge quiz into a knowledge hunt. Conceptually, it transforms a measure of quick recall into a measure of the ability to find the correct answers to political knowledge questions when given an opportunity to do so—a concept that we call political learning skills. This experimental design allows us to evaluate our second hypothesis:         "
"15","                        "
"16"," Null Hypothesis #2: The ability to answer factual survey questions in conventional opinion surveys correctly (quick recall) and the ability to find correct answers when given an opportunity (political learning skills) are not sufficiently different to require separate measurement. Even if giving respondents extra time increases the number of correct answers, the change will be uninteresting: it will be constant across respondents or simply amplify differences between strong and weak performers.               "
"17","In what follows, we present experimental evidence sufficient to reject both null hypotheses. On average, offering a small monetary incentive led to an 11% increase in the number of questions answered correctly. Offering extra time had an even larger effect. Simply offering people a little money for responding correctly or extra time to find the answers does not transform them into political encyclopedias, but it does affect how they answer knowledge questions. A substantial share of those who appear to be “know‐nothings” in existing research on political knowledge can answer questions correctly when given a small incentive or extra time to do so."
"18","To examine how thorough searches of declarative memory and the application of procedural memory affect respondents' ability to answer knowledge questions correctly, we experimentally manipulate two elements of the survey interview, the incentive for answering questions correctly and the time offered to complete knowledge questions. To accomplish this manipulation efficiently, we randomly assigned respondents to one of four experimental groups within a single representative survey (whose attributes we describe below). Each respondent was equally likely to be placed in one of the four groups depicted in Figure 1.         "
"19","                 The Experimental Design                      "
"20","We offered one randomly selected half of our sample a monetary reward, $1, for each correct answer. Our survey included 14 knowledge questions, so respondents could earn up to $14 for answering all questions correctly. We chose $1 per question because we assumed that the amount would be nontrivial for many respondents and because this amount allowed us to stay within our budget while generating a sufficient number of cases per cell for rigorous statistical evaluations (see Bassi, Morton, and Williams 2006, for a recent review of the consequences of incentive payments for respondent effort in political science experiments).         "
"21","In our Internet‐based survey, which respondents completed using a computer or a WebTV unit, the knowledge questions appeared after an initial battery that solicited the respondent's party identification, level of interest in politics, and prior turnout. Next, all respondents saw a common introduction:"
"22","                        "
"23","In the next part of this study, you will be asked 14 questions about politics, public policy, and economics. Many people don't know the answers to these questions, but it is helpful for us if you answer, even if you're not sure what the correct answer is. We encourage you to take a guess on every question. At the end of this study, you will see a summary of how many questions you answered correctly."
"24","Respondents in the pay conditions then received the following instructions:"
"25","                        "
"26","We will pay you for answering questions correctly. You will earn 1,000 bonus points ($1) for every correct answer you give. So, if you answer 3 of the 14 questions correctly, you will earn 3,000 bonus points ($3). If you answer 7 of the 14 questions correctly, you will earn 7,000 bonus points ($7). The more questions you answer correctly, the more you will earn.2"
"27","The second experimental factor is time. To measure respondents' political learning skills, we gave one randomly selected half of our sample 24 hours to answer all 14 knowledge questions. The other half had only one minute to answer each knowledge question. Respondents in the “one minute” condition were informed that"
"28","                        "
"29","You will have 1 minute to answer each question. After 1 minute, you will be automatically forwarded to the next question. If you finish answering a question before 1 minute is up, you may proceed to the next question by clicking on the ‘Next Question’ button."
"30","Each of the knowledge questions was programmed to be on‐screen for up to one minute. If respondents answered the question within that period or if one minute had expired, the screen changed to show the next question. In the “one minute” condition, respondents could not go back to a previous knowledge question after they had moved past it in the interview."
"31","Respondents in the “24 hour” condition were informed that"
"32","                        "
"33","You will have 24 hours to answer these questions from the time you see the first question. Once the 24 hours are up or whenever you decide that you are done, you will be forwarded to the next section and will not be able to return to the knowledge questions. However, before you reach the next section, you may go back to previous knowledge questions by clicking the ‘back’ button."
"34","Starting from the moment at which respondents saw the first knowledge question, they had 24 hours to complete the knowledge series. During this period, they could go back and forth between knowledge questions (but not to the initial questions about interest, turnout, and partisanship), change their answers, and interrupt and resume the survey as often as they liked. When respondents reached the end of the knowledge sequence, a screen informed them that they could modify their answers until their 24 hours were up or move to the next part of the survey (at which point they were “locked out” of this part of the survey and could not return to the knowledge questions).3"
"35","The dependent variable in our study comes from answers to the 14 knowledge questions. Some of these questions were open‐ended; others were multiple choice. To facilitate payment for open‐ended questions in the relevant experimental conditions, we specified in advance a range of answers (e.g., “within X percentage points of the true percentage”) that would earn compensation. Respondents were told the number of questions they answered correctly (and the rewards they had earned) at the very end of the interview. This sequence is necessary because we asked some posttreatment questions about the election and wanted to avoid the possibility of performance feedback contaminating responses to these final questions."
"36","We chose 12 of the 14 questions for their relevance to the 2004 presidential election (the exceptions are questions about the length of a Senate term and the number of Republicans in the Senate). This experiment was conducted in the three weeks prior to that election. All of the topics covered in these questions reflected active campaign themes. Some of these questions were about candidate policy positions. We asked about the candidates' positions on tax cuts, education, and the line‐item veto. Other questions were about political circumstances that were relevant to the presidential campaign, such as the Senate vote on the Iraq authorization and the 9/11 commission's findings about links between al‐Qaeda and Iraq. Another set of questions focused on economic factors referenced during the campaign. We asked about official government statistics pertaining to the percentage of Americans who were not covered by health insurance, living in poverty, and unemployed. We also tested their knowledge of the estate tax and the federal debt. In short, we asked challenging questions about matters relevant to the 2004 election. A complete list of questions and their wording is in Appendix Table 1.         "
"37","We followed recommendations by Mondak (2001; Mondak and Davis 2001) and Krosnick et al. (2002) to discourage “Don't Know” responses by not giving respondents explicit “Don't Know” options. While our respondents could hit the “next question” button without marking any answer, almost none of them did. Discouraging “Don't Know” responses reduces distortions because, in the absence of encouragement, respondents may vary in their propensity to guess—and hence offer more correct answers with positive probability—for reasons that are orthogonal to their true levels of knowledge (e.g., relating to personality, social status, and confidence). Mondak and Davis (2001) find that discouraging “Don't Know” responses increases the frequency of correct responses by 10–15%. Since we discouraged such responses in our treatment and control groups, the performance increases that we report are over and above those that come from encouraging guessing. Hence, their efforts and ours demonstrate distinct, but complementary, ways in which seemingly arbitrary features of survey interviews can lead respondents to underreport what they know.         "
"38","In our analysis, we use the number of correct responses as our dependent variable.4 This choice raises the question of how to determine the range of answers to open‐ended questions that we consider correct. The ranges we use are listed in Appendix Table 1. Running the analyses with different ranges of the same general magnitude yields similar treatment effects.5"
"39","Our experiment was embedded in a representative survey of U.S. residents conducted by Knowledge Networks between October 19 and November 1, 2004. Knowledge Networks interviews national probability samples over the Internet by providing a large panel, selected through Random Digit Dialing, with WebTV units and/or free Internet connections in exchange for taking surveys. The participants for this study constitute a randomly selected subset of the KN panel and approximate a random sample of the U.S. adult population. Our survey was assigned to 1,550 panelists of whom 1,220 (79%) completed it. Eighty percent of the respondents who completed the survey did so within four days of the fielding date."
"40","Knowledge Networks' survey methodology makes our study a conservative test of our hypotheses. The company informs its panelists by email when a new survey is waiting for them. Respondents can take the survey at a time of their own choosing. Hence, even respondents in our control group (“one minute, no pay”) are not literally caught during dinner or at other inopportune moments and asked to answer the knowledge questions on the spot—as can happen in the telephone‐based surveys from which many political knowledge measures are drawn. In fact, they even had the opportunity to pause the interview when they learned that they would be asked political knowledge questions. (However, they could not stop the relevant timers once they saw the first knowledge question.) Clearly, we do not capture the true inconvenience of a typical phone interview. Moreover, panelists receive compensation just for participating because Knowledge Networks pays for their WebTV unit and/or an Internet connection to their PC. To be sure, this compensation does not represent an incentive to answer thoughtfully on any particular question, but the conditions in our control group do not recreate the conditions of a typical phone interview perfectly. Therefore, respondents in the control group are likely more motivated and less inconvenienced than respondents in the telephone surveys from which many claims about political knowledge are derived. All else constant, these attributes should make our null hypotheses increasingly difficult to reject.6"
"41","We begin the analysis by testing our first null hypothesis—that a monetary incentive will not increase the number of political knowledge questions answered correctly (holding constant at 60 seconds the amount of time respondents have to answer each question). Table 1 summarizes the effect of the $1 incentive on the number of correct answers. As the top of the table shows, the incentive increased the average number of correct answers from 4.5 to 5.0. This 11% increase is statistically significant at p < .05.         "
"42","We thus reject our first null hypothesis: Since an incentive for correctly answering knowledge questions increases the number of correct answers, it follows that conventional mass opinion surveys underestimate how much political information respondents hold in memory. Simply paying respondents a small amount for answering questions correctly yields a significant increase in performance. This result suggests that standard survey practice does not provide sufficient incentives for respondents to thoroughly search their declarative memory.         "
"43","The distribution of knowledge in the population is also consequential. Table 1 presents the effect of the monetary incentive for various demographic and attitudinal subgroups. For several groups, the experimental effect was far larger than the average 11% increase. Among respondents who report being moderately interested in politics, the monetary incentive increased correct answers by 32%. Men, white Americans, and those between 35 and 59 years of age also improved their performance disproportionately in the “one minute with pay” condition.         "
"44","Such differences are relevant because they indicate if observed knowledge gaps in the population widen or narrow when respondents are induced to think harder about what they truly know. Table 1 suggests that gender and race differences in political knowledge are larger than commonly reported. For example, in the control condition, men responded correctly to about 0.8 more questions than women. When we offer compensation, the gender gap widens. Giving both genders an incentive to think harder increased this difference to 1.3 on average. This outcome suggests that under conventional survey conditions, women search their declarative memories more effectively than men. Racial differences in quick recall increase even more dramatically when an incentive is offered. In the control group, whites provide less than one more correct answer (about 0.8 items) than do nonwhites on average. The monetary incentive expands this gap to 2.3 items. The monetary incentive improved whites' performance very robustly, but had no significant effect on nonwhites.7"
"45","To examine if these group‐level differences are robust to the inclusion of common demographic variables, we estimate multivariate models of quick recall for the control and treatment conditions. The OLS estimates are shown in Table 2. If the coefficients for a particular attribute are significantly different in the “one minute with pay” condition than they are in the control condition, then we can conclude that our treatment changes the effect of this attribute on respondents' scores. (With only about 300 respondents in each condition, we consider differences with p‐values of less than .10 as sufficiently precise.)         "
"46","The results in Table 2 confirm what we observed above. The race and gender gaps are largely robust to the addition of other demographic variables. The monetary incentive more than doubles both the gender gap and the race gap in quick recall.         "
"47","Turning to the matter of political interest, the incentive has its greatest effect on moderately interested respondents. This finding suggests that traditional survey procedures fail to motivate moderately and (to a lesser extent) strongly interested citizens to try as hard as they can when answering political knowledge questions. Hence, past survey‐based studies have likely underestimated the effect of political interest on political knowledge."
"48","In sum, when respondents are encouraged to exert extra effort in answering knowledge questions, men, white Americans, and respondents with moderate political interest increase their performance disproportionately. In our survey, the performance difference between moderately interested white men and uninterested nonwhite women is little over one item using traditional survey procedures (in the control group). This difference surges to more than four items when thorough memory search is encouraged (in the treatment group). Results like this suggest that conventional survey measures not only underestimate political knowledge, but also underestimate inequalities in the distribution of what citizens really know among several key demographic groups."
"49","According to our second null hypothesis, providing survey respondents with extra time should simply reproduce results obtained from previous knowledge measures. Our alternative hypothesis is that extra time offers respondents an opportunity to apply procedural memories to the question task. We document the effect of extra time as it appeared in two distinct experimental treatments. In one treatment, randomly selected respondents were given extra time to answer questions and no compensation for answering correctly. Another randomly selected group received extra time and a monetary incentive. The “24 hours with pay” condition indicates best how well respondents can educate themselves about politics when they are at least modestly motivated to do so by us. The “24 hours, no pay” condition documents how well respondents do on the learning task without extrinsic motivation.         "
"50"," Table 3 expresses the effect of extra time by comparing mean performance in the three experimental conditions. The number of correct responses is significantly higher when respondents have 24 hours to complete the knowledge questions. Compared to the control group, average performance increases by 18% without a monetary incentive and by 24% with the incentive. A more detailed look at the data reveals that while 28% of the respondents in the control condition answers less than three questions correctly, that share drops to 15% in the “24 hours with pay” condition. Only 10% get more than eight items right in the control condition, compared to almost twice that (19%) in the “24 hours with pay” condition.         "
"51","We now examine how extra time affected the performance of various subgroups. Our goal is to evaluate the extent to which respondents—who may or may not draw deeply from their declarative memory when answering political knowledge questions in traditional surveys—would use their procedural memory in the pursuit of such tasks when given an opportunity."
"52"," Table 3 shows the experimental effects of extra time (relative to the “control” condition) for the same set of demographic groups as in Table 1. Extra time causes disproportionately large performance increases for less interested respondents, respondents without a college degree, older respondents, and white respondents. As these groups are not identical to those in the previous analysis, these findings indicate important differences between quick recall and political learning skills.8"
"53","We present multivariate OLS estimates of these relationships in Table 4. The table shows the same model for all four experimental conditions (repeating, for ease of comparison, the results from Table 2.) The most important contrast in Table 4 is between the “one minute with pay” condition, which represents our best measure of quick recall, and the “24 hours with pay” condition. If the absolute value of a coefficient is greater in the “24 hours with pay” condition than in the “one minute with pay” group, this is evidence that differences in political learning skills amplify differences in what we can expect important groups of citizens to know when they have opportunities to learn.         "
"54","Four factors in Table 4 are significantly different in the second and fourth columns (indicated by common superscript letters in those columns). Another difference (for gender effects) approaches statistical significance. For example, people aged 60 and older do slightly worse on the quick recall task than do people under 35. Given time to consult references, however, they answer between 1 and 1.5 more questions correctly. So, while young and old Americans are equally knowledgeable when drawing from declarative memory, seniors are far more likely to give us correct answers when given an opportunity to use their procedural memory. Providing more time amplifies the effect of age on the number of questions answered correctly.         "
"55","Another large difference between quick recall and political learning skills occurs for people who left college without a degree. When using declarative memory only, this segment of the population is barely more knowledgeable than those who did not go to college at all. However, when given more time to answer the question their performance parallels that of the college graduates. This result suggests that college attendees who left without a degree may not store as much political information in their declarative memory, but many have acquired skills relevant to answering political questions. To the extent that they use these skills as the time of an important political decision approaches, their political decisions may be better informed than conventional knowledge measures suggest."
"56","Extra time also narrows the gender gap observed earlier. Women in our survey did not carry as much political information in declarative memory as men, but when given an opportunity to employ procedural memory their scores increased more on average than those of men. Fully employed people, too, take advantage of the opportunity to learn and answer questions far more effectively than under standard survey conditions."
"57","The most remarkable difference between quick recall and political learning skills, however, concerns the role of political interest. Political interest has a very large impact on our measure of quick recall (see column 2).9 When respondents are motivated by the prospect of a small material reward for answering correctly, but have no opportunity to draw on their procedural memory, the most politically interested among them do better than those who are moderately interested, and the moderately interested, in turn, do better than the uninterested. These differences are far smaller when it comes to political learning skills (see column 4). Politically uninterested respondents exhibit considerable learning skills (by comparison to those who are more interested). The learning skills of politically very interested people are still significantly greater, but this difference is barely half as big as the equivalent difference for quick recall. The opportunity to learn in our survey attenuates interest‐based variations in performance observed under traditional survey conditions.         "
"58","We illustrate this difference graphically in Figure 2 by comparing quick recall and political learning skills for respondents with different levels of political interest. The figure plots the predicted number of correct answers in the “one minute with pay” and the “24 hours with pay” conditions for a married, white, female college graduate between 45 and 59 with mean income and full‐time employment. The solid line illustrates the relationship between her interest in politics and her ability to answer questions instantly. The broken line demonstrates her ability to answer correctly when she has more time. For quick recall, the difference between being uninterested and very interested in politics corresponds to an increase of more than two questions answered correctly. This is a very large effect by comparison to the effects of other variables on quick recall. For political learning skills, the story is different. When we turn a knowledge quiz into a knowledge hunt, the performance difference between our most and least interested respondents is much smaller, amounting to just over one item.         "
"59","                 Quick Recall and Political Learning Skills by Level of Political Interest                            Note: This figure plots the predicted number of correctly answered knowledge questions by levels of political interest in the “one minute with pay” condition (quick recall) and in the “24 hours with pay” condition (political learning skills). Predicted values are for a married, white, female college graduate between 45 and 59 with mean income and full‐time employment.                     "
"60","This finding illustrates the usefulness of distinguishing between quick recall (performance based on declarative memory only) and political learning skills (performance that permits a contribution by procedural memory). Many people who are intrinsically motivated to follow politics acquire political information regularly and regardless of whether a decision is impending. They are knowledgeable when we ask them fact‐based questions on surveys. Others who do not enjoy politics as much are less likely to carry such information in their declarative memories. When survey interviewers contact them without warning and the survey rushes along apace, these people do not perform well. But it would be a mistake to assume that such observations are sufficient to infer a general lack of capability at politically charged moments, such as elections. The fact that most uninterested Americans carry little political information in declarative memory need not imply that their decisions are made in an uninformed way. Some of them have significant political learning skills and can use them to make more informed decisions than traditional knowledge measures suggest."
"61","That said, political learning skills indicate only a potential for making more informed choices. To what extent individuals realize this potential is a separate question. It is also a question that is difficult to answer for a large sample of people in an electoral context. The reason is that traditional surveys do not necessarily occur at the time when respondents reach political decisions. As a result, respondents' ability to answer factual questions at the time of the interview may not be a good proxy for what they knew when they made political decisions or developed political opinions. Surveys underestimate political knowledge levels if respondents have either not yet acquired information they will use or already forgotten information already used in their decision making. The percentage of respondents who answer knowledge questions correctly increases as an election approaches (e.g., Johnston, Hagen, and Jamieson 2004), indicating that many people acquire political information in anticipation of their vote decision. But once respondents have reached a particular political decision, it may be cognitively inefficient for them to retain the facts on which they relied. As a result, early deciders may already have forgotten some of the information that affected their decision (Lodge, Steenbergen, and Braun 1995; Rahn, Aldrich, and Borgida 1994). As different people decide at different times, it becomes virtually impossible to interview all respondents when they make their decisions. Since people's knowledge when they are interviewed need not indicate how well informed their decision actually was (or will be), assessing their political learning skills can provide valuable evidence about the range of information on which they are likely to base their choice.         "
"62","In this study, we show that people store, and know how to find, more political information than previous research suggests. We contend that conventional political knowledge scales suffer from two problems. First, they confound respondents' recall of political information and their motivation to engage in a survey interview by measuring simultaneously whether or not respondents know the facts and are motivated to tell them to us. Second, these surveys do not assess respondents' skill at accessing political information that falls outside of declarative memory and is therefore not quickly accessible at the time of the survey.         "
"63","We address the first problem by extrinsically motivating respondents to search their memory for the correct answer. The monetary incentive increases performance, indicating that people store more political information than conventional survey practice picks up. Having demonstrated this point, we believe that alternative calibrations of the monetary incentive (e.g., one cent or ten dollars per correct answer) can clarify the motivational push necessary to get different kinds of respondents to report what they know or find what they cannot quickly recall. The effects of nonmonetary incentives also merit attention. We hope to pursue these inquiries in future research."
"64","The second problem led us to measure political learning skills directly. Existing survey‐based knowledge measures ignore procedural memory, even though people rely on it regularly. In politics, those who cannot instantly recall a particular fact often have opportunities to ask someone else or look up the answer. Traditional surveys, while having many virtues, prevent or inhibit exactly the kinds of search activities that are in fact strongly encouraged by people who want others to make informed decisions.         "
"65","Moreover, we find important differences between quick recall and political learning skills. The people who can instantly recall politically relevant facts on a survey are not the same as those who can find correct answers when given an opportunity to do so. Some less knowledgeable people are more skilled at finding political information when they have an opportunity to do so than common interpretations of traditionally measured political knowledge levels suggest. In cases where political decisions allow citizens to seek information before making a choice (i.e., voting in elections), political learning skills will affect decision quality. So when a person's political learning skills are high, poor performance on unannounced and rushed survey‐based pop quizzes is less indicative of low political ability. In making this argument, we do not mean to downplay the importance of immediately available political knowledge. There are situations when citizens must make quick decisions with no advance warning. Here, procedural memory is of little help. But in many other situations, including elections, the element of surprise is absent and people can collect relevant information. In those situations, political learning skills can contribute to decision quality."
"66","Our results provide a new and distinct reason for being skeptical when analysts use existing knowledge measures as the basis for sweeping generalizations about what citizens do not know. Seemingly arbitrary attributes of survey interviews affect the validity of survey‐based claims about citizens' political capabilities. In particular, existing political knowledge measures, when used as measures of political competence, likely underestimate the public's true abilities. Just because some respondents do not store a lot of political information, they do not necessarily have poor political learning skills."
