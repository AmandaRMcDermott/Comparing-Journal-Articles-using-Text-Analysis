"","x"
"1","In this section, we briefly describe the background of the motivating empirical example regarding the competing theories of trade policy preferences. An enduring theme in the international political economy literature is the explanation of preferences for free trade. In a seminal contribution, Hiscox (2002) analyzes legislative voting on trade bills in the United States by drawing on political economy interpretations of two canonical theories from the trade literature: the Stolper‐Samuelson (SS) and Ricardo‐Viner (RV) models of international trade. The two competing theories differ critically in the extent to which they emphasize factoral versus sectoral cleavages. The SS model suggests that cleavages on trade policy will be along factoral lines and predicts that the owners of factors which the United States is relatively abundant in (compared to the rest of the world) will favor trade liberalization.6 In contrast, the RV model suggests an alternative cleavage between supporters and opponents of free trade that runs along sectoral lines.7 These two models of support for trade policy figure centrally in this long tradition of international political economy research (e.g., Ladewig 2006; Rogowski 1989; Scheve and Slaughter 2001).         "
"2","A key observation made by Hiscox (2002) is that the applicability of these competing models depends on how specific factors of production are to particular industries. If capital is highly mobile in the national economy, meaning it can easily move across industries, then the SS model is likely to be supported because the winners and losers of trade will be found among owners of abundant and scarce forms of factors, respectively. On the other hand, if capital is more specific (i.e., less mobile), then cleavages should fall along sectoral lines since capital is unable to easily adjust across industries. Hence, Hiscox hypothesized that whether congressional voting on trade bills can be explained by the SS or RV model will depend on the degree of factor specificity in the U.S. economy.         "
"3","To empirically test this hypothesis, Hiscox collected the data on factor specificity in the U.S. economy over nearly two centuries. His measures varied considerably over time, suggesting that during some eras voting should be along factor lines (capital/land versus labor) and in other eras along sectoral lines (exporters versus importers). To leverage these changes over time, Hiscox estimated separate regressions for different eras in time. Using a conventional model selection procedure called the  test, Hiscox provides evidence that support for liberalization is best accounted for by the SS model during eras where specificity was low. In contrast, he finds that in periods where specificity was high, the RV is the preferred model.         "
"4","Although breaking up the votes into different eras constitutes one informal way to test the factor specificity argument, the continuous measure of the factor specificity variable created by Hiscox does not provide natural breakpoints which can then be used to group votes. Thus, any grouping might be criticized as arbitrary. As we demonstrate, finite mixture models offer a relatively straightforward and yet formal way to directly incorporate the factor specificity measure. In particular, mixture models use the level of factor specificity to predict whether the SS or RV model is appropriate for each trade bill or even each vote. Thus, in addition to the overall assessment of the two models, we are also able to identify the list of trade bills in which the voting pattern is consistent with each theory.         "
"5","In this section, we first briefly review the specification, estimation, and inference for finite mixture models in the context of empirical testing of competing theories. We then discuss a method to identify the observations that are statistically significantly consistent with each theory. We also propose several ways to measure the overall performance of each competing theory. Finally, we compare the proposed approach with the standard model selection procedures."
"6","Before describing the proposed methodology, we emphasize an important distinction between causal and predictive inferences. For causal inferences, ignoring relevant confounders may result in omitted variable bias.8 In contrast, the existence of omitted variables alone does not invalidate predictive inferences.9 Indeed, it is well known that for the purpose of predictive inferences, parsimonious models tend to outperform unnecessarily large models (see, e.g., Hastie, Tibshirani, and Friedman 2001). Thus, if the goal of researchers is to construct a theory with strong predictive power (as opposed to testing causal mechanisms; see Imai, Tingley, and Yamamoto 2011, for relevant methodological issues), parsimonious models that can capture systematic patterns in the data are preferred. While our method can be used for both purposes, the causal inference approach would require strong research designs that enable the identification of causal effects. Our empirical examples should be thought of as the instances of predictive inference. Nevertheless, whenever using mixture models, well‐specified theories play an essential role in model specification.         "
"7","Model Specification.  Consider a finite number of  different statistical models, each of which is implied by one of the competing theories explaining the same phenomena. Beyond the fact that it can handle more than two theories at the same time, the proposed method is applicable without modification regardless of whether these statistical models are nested or not.               "
"8","Finite mixture models are based on the assumption that each observation is generated either from one of the  statistical models or more generally from a weighted combination of multiple statistical models. This does not necessarily imply that researchers must identify all relevant theories. It is also possible that any observation, which is consistent with one of  theories under consideration, is also consistent with other theories that may or may not be included in the analysis. Rather, the goal of finite mixture models is to measure the relative explanatory power of the competing theories under consideration by examining how well a statistical model implied by any of the rival theories predicts each observation in the sample. For example, it is perhaps the case that the Stolper‐Samuelson and Ricardo‐Viner theories do not exhaust all possible theories for trade policies. And yet, it is of interest to investigate the relative performance of each theory explaining the variation in the voting behavior of legislators.               "
"9","Formally, let  denote a statistical model implied by theory  where  is the value of the outcome variable ,  is the value the vector of covariates  takes, and  is the vector of model parameters. In statistics, typical applications of finite mixture models involve the same distributional and functional‐form assumptions with identical covariates. Similar to random coefficient models, such an approach makes parametric models flexible by allowing different groups of observations to have different parameter values. However, these alternative statistical methods can neither provide a measure of overall support for each theory nor classify each observation to one of the competing theories. Furthermore, different theories usually require different sets of predictors. In fact, Hiscox (2002) employed logistic regression models with different sets of covariates for the Stolper‐Samuelson and Ricardo‐Viner theories. One may also wish to specify different statistical models for rival theories. For example, when analyzing the duration of cabinet dissolution, the underlying risk of cabinet dissolution may be constant (as in the exponential model) or increases over time (as in the Weibull model) (see King et al. 1990; Warwick and Easton 1992). Unlike random coefficient models and regressions with interaction terms, mixture models can handle these situations.               "
"10","Given this setup, we formalize the idea that each observation is generated from one of  statistical models, but we do not know a priori which model generates a specific observation. Specifically, we use the latent (unobserved) variable  to represent the theory with which observation  is consistent. Thus,  can take one of  values, i.e., , depending on which statistical model generates the th observation. The data‐generating process is given by,                  "
"11","Next, assuming the conditional independence across observations given the covariates and the latent variable, the model specified in equation (1) yields the following observed‐data likelihood function where the latent variable  has been integrated out,                  "
"12","The mixture model does not necessarily “assume” each observation is implied by one and only one of the rival theories under consideration. An alternative and more general interpretation of the above mixture model, as seen clearly from equation (2), is that each observation is implied by a weighted combination of the rival theories where the relative weights are represented by . The distinction between these two interpretations is not important when fitting the mixture model because the data cannot distinguish them, but as discussed later, it becomes crucial when using the statistical test we propose.               "
"13","As mentioned earlier, finite mixture models can be extended to determine the conditions under which a particular theory applies. In the example given earlier, the level of factor specificity in the national economy determined the relative applicability of the Stolper‐Samuelson and Ricardo‐Viner models. Such variables can be easily incorporated in the finite mixture modeling framework. This is done by directly modeling the probability that an observation is consistent with theory , i.e., , as a function of the observed theory‐predicting variables,  (note that  may overlap with ), in the following manner,                  "
"14","Estimation and Inference.  Estimation and inference can proceed by either a frequentist approach of maximizing the observed‐data likelihood function or a Bayesian approach of sampling from the posterior distribution after specifying prior distributions. To obtain the maximum likelihood estimates, the Expectation‐Maximization (EM) algorithm (Dempster, Laird, and Rubin 1997), an iterative numerical optimization algorithm consisting of the expectation (or E) step and the maximization (or M) step, can be applied to the following complete‐data log‐likelihood function, which is derived by assuming that  is observed,                  "
"15","After the E‐step, the M‐step maximizes the function defined in equation (5). This step can be achieved by separately maximizing the weighted log‐likelihood function for each model, i.e., , where the weight is given by . This is again intuitive because the weight for an observation is greater when fitting the statistical model with which this observation is consistent. The updated estimate of  can then be obtained by averaging  across all observations,                  "
"16","When  is modeled as a function of covariates as in equation (3), then maximizing the weighted log‐likelihood function (based on the multinomial logit regression, for example) will give the updated estimate of model parameters . While the use of a parametric model means that the relationship given in equation (7) no longer holds, the “averaging” of  is still used to estimate  because  is used as a weight when fitting the model.10"
"17","The E‐step and M‐step are repeated until convergence. The advantage of the EM algorithm is its numerical stability (each iteration increases the observed‐data likelihood function in equation (2)) and its relatively straightforward implementation (the M‐step can be implemented through successive fitting of standard statistical models with the weighted likelihood function). The disadvantage is that convergence can be slow and standard errors must be computed separately (bootstrap or the numerical estimation of the Hessian matrix can provide approximate standard errors for all parameters including ).               "
"18","Alternatively, Bayesian inference can be applied to finite mixture models. Here, the standard approach is to use the Markov chain Monte Carlo (MCMC) algorithm with data augmentation where the latent variable  is sampled along with model parameters. Bayesian inference requires the specification of prior distributions. For example, Dirichlet distribution is often used as the prior distribution for . Given the prior distribution, the MCMC algorithm takes the following general form,               "
"19","                  "
"20","Sample  given the current values of all parameters with the following probability,                           "
"21","Given , sample all parameters.                        "
"22","                  "
"23","Given the subset of the data with , update  using the standard MCMC algorithm for this particular model.                        "
"24","Update  using the standard MCMC algorithm. For example, if the Dirichlet distribution is used as the prior distribution, then we have,                           "
"25","Again, the advantage of this MCMC algorithm is its simple implementation and ability to produce uncertainty estimates of all parameters including . In particular, standard MCMC algorithms for each of the submodels can be used to sample parameters from their joint posterior distribution.               "
"26","Finally, note that fitting mixture models can be computationally difficult given that the likelihood often contains multiple modes, which may pose problems for the EM algorithm. The mixing of the standard MCMC algorithm can also be poor. Thus, it is important to check the convergence carefully and run multiple independent chains with overdispersed starting values (Gelman et al., 2004).               "
"27","Grouped Observations.  In some situations, multiple observations are grouped and researchers may wish to assume that all observations of one group arise from the same statistical model implied by either a particular theory or more generally from the same weighted combination of statistical models under investigation. For example, multiple observations may be collected over time for each individual in a study, and all observations from one individual are assumed to be consistent with one of the competing theories or a particular weighted combination of these theories. In the trade policy example discussed earlier, it may be reasonable to assume that all votes on a particular bill are generated by a single theory because they all share the same level of factor mobility.               "
"28","In such a situation, finite mixture models can be formulated as,                  "
"29","We note that when the number of groups is small, the parameter  in  may not be precisely estimated. On the other hand, this grouping approach may result in a greater posterior probability that each group is consistent with a particular theory because multiple observations for each group provide more information about the relative appropriateness of each theory. To see this formally, note that the E‐step of the EM algorithm is given by calculating the following conditional expectation,                  "
"30","Calculating Usual Quantities of Interest.  Along with  and , one may also be interested in calculating usual quantities of interest such as predicted and expected values under finite mixture models. To do this, given the specified values of the observed covariates, i.e.,  and  if the mixing probability  is modeled as a function of , estimate the quantities of interest under each of the competing models as well as  for each . Then, the weighted average of these estimates where the weights are given by the estimated values of  represents the quantity of interest under the mixture model. To account for the estimation uncertainty, standard methods such as bootstrap, Monte Carlo approximation (King et al., 2000), and Bayesian simulation can be applied to calculate confidence intervals and standard errors. Note that it is important to account for the estimation uncertainty associated with the mixing probability  as well as other model parameters.               "
"31","One advantage of the proposed mixture modeling approach is its ability to yield a list of observations for which researchers have sufficiently strong evidence that they are consistent with one of the competing theories. To do this, we focus on the posterior probability, , that observation  is consistent with theory . This parameter can be estimated as part of either the EM algorithm or the MCMC algorithm (given in equations (6) and (8), respectively, or equation (11) in the case of grouped observations). Our proposal is to apply a prespecified threshold  and call observation statistically significantly consistent with theory  if its corresponding probability is greater than this threshold, i.e., .            "
"32","How shall we choose an optimal value of ? A naive selection of the value of  will result in a list with many falsely classified observations due to the well‐known multiple testing problem of false positives. For example, suppose that we conduct  independent hypothesis tests with the conventional 5% significance level. Even when the null hypothesis is true in all cases, the probability that at least one null hypothesis is falsely rejected increases rapidly with ; it equals  when .            "
"33","Thus, we propose to construct the longest list possible while at the same time controlling for the expected proportion of incorrect classifications on the resulting list. This can be done by applying the key insight from the fast‐growing statistical literature on multiple testing. Specifically, for each theory , we choose the smallest value of  (so that we can include as many observations on the list as possible) while ensuring that the posterior expected value of false discovery rate on the resulting list does not exceed a certain threshold .12 The value of  needs to be selected by a researcher a priori. For example, we may choose . This strategy yields the following expression for the optimal value of  under the proposed criterion (see Genovese and Wasserman 2003; Newton et al., 2004; Storey 2003),               "
"34","Alternatively, we can obtain the optimal threshold applicable to all theories by controlling the expected false discovery rate across all lists, which yields the following thresholding formula for a single value of  that is applicable to all rival theories,               "
"35","We emphasize that this test makes sense only if researchers interpret the mixture model as generating each observation from one rival theory. Thus, researchers who believe that each observation is implied by a weighted combination of all rival theories may not employ this test. Even in this case, however,  still represents the degree to which observation  is consistent with theory . Regardless of which interpretation they adopt, researchers can also measure the overall performance of rival theories, the topic to which we now turn.            "
"36","Finally, we propose two ways to formally assess the overall performance of each theory. First, we can estimate the population proportion of observations consistent with each theory. For each theory , the estimate of  represents this proportion and either its maximum likelihood or Bayesian estimate is obtained as a result of the EM or MCMC algorithm. Alternatively,  can be interpreted as the average degree to which observations are consistent with one of the competing theories. When  is modeled as a function of observed covariates, one can use the expected sample proportion of observations consistent with each theory. This measure can be calculated as the average of  across all observations in the data, i.e., .            "
"37","In addition, if we assume that each observation is consistent with only one theory, we may use the number of observations that are identified as statistically significantly consistent with one of the competing theories as a measure of overall performance. The idea is to focus on the observations for which we have strong evidence rather than to construct a measure by including ambiguous cases. In particular, the overall performance of a competing theory can be measured with the sample proportion of observations statistically significantly consistent with the theory. This measure is attractive because the observations for which the value of  is neither close to zero or one may correspond to the cases explained by a theory other than those included in the mixture model.            "
"38","A straightforward way to estimate finite mixture models for comparative theory testing is to use flexmix (Grün and Leisch, 2008a), which is an add‐on package freely available for the statistical software R.13 The flexmix package uses the EM algorithm to obtain the maximum likelihood estimates for a wide range of mixtures of regression models. Along with the replication code and data for this article, we provide an example syntax below so that others can use it as a template.            "
"39","If researchers are capable of simple statistical programming, it is also possible to estimate finite mixture models that are not available in the existing software. We provide such examples in a later section (bivariate probit regression model) and the supporting materials (semiparametric logistic regression model). Such an extension is straightforward because we can rely upon the existing functionalities within the general framework of finite mixture models. Similarly, if researchers wish to implement Bayesian finite mixture models, they can take advantage of the existing MCMC algorithm implementation, including the ones available in the MCMCpack package (Martin, Qunn, and Park 2009), for fitting various models.            "
"40","Next, we briefly compare the proposed mixture modeling approach with some of the alternative methods often used by applied researchers. Perhaps the most common approach to empirical testing of competing theories is to construct a regression model that encompasses all relevant theories and then examine the magnitude and statistical significance of coefficients corresponding to each theory. Achen (2005) criticizes this wide‐spread approach as atheoretical and calls it a “garbage‐can regression” because no single theory can justify the model specification of such regressions with many explanatory variables. A number of other scholars share this concern (e.g., Braumoeller 2003; Clarke 2000, 2007b; Gordon and Smith 2004; Granato and Scioli 2004).            "
"41","Parsimony is also regarded by many social scientists as an important criterion for theory development. For example, Friedman states, “A hypothesis is important if it ‘explains’ much by little, that is, if it abstracts the common and crucial elements from the mass of complex and detailed circumstances surrounding the phenomena to be explained and permits valid predictions on the basis of them alone” (1966, 14). The mixture modeling approach is consistent with this view. Rather than fitting a regression model with many covariates which encompass all theories under consideration, it allows for empirical testing of several parsimonious statistical models, each of which is justified by a particular theory.            "
"42","Political scientists who have abandoned the “garbage‐can” approach have used various model selection methods to test competing theories. Popular methods include Bayesian information criteria, the Vuong test (Vuong 1989), the  test (Davidson and MacKinnon 1981), and the Clarke test (Clarke 2007a). These methods are useful because they enable the comparison of two non‐nested models (Clarke 2000). Indeed, some of the methods share the same motivation as the mixture modeling approach.            "
"43","In particular, the  test, which is used by Hiscox (2002) in the application described in an earlier section, is based on the following mixture setup with the mixing probability ,               "
"44","More generally, a fundamental difference between the mixture modeling approach and the standard model selection methods is that the latter hypothesize one theory applies to all observations, whereas the former allows for competing theories to coexist. In the presence of theoretical heterogeneity, standard model selection procedures may yield an ambiguous conclusion (appropriately so!). In contrast, the mixture modeling approach can quantify the degree of such heterogeneity and identify the conditions under which each theory applies, which facilitates further theoretical development. This difference is evident in the mixture setup of the  test given in equation (14) where the test is conducted with the null hypothesis of  against the alternative hypothesis , ignoring the possibility that  may take a value other than 0 and 1. Thus, unless one theory applies to the entire population, for the purpose of testing alternative theories, mixture modeling is more appropriate than standard model selection procedures.15"
"45","Another general problem of standard model selection procedures is the potential bias arising from the fact that the usual standard errors do not incorporate the uncertainty concerning model selection because they are calculated assuming that a particular model is correct. This means that since any model selection procedure yields false positives, the standard errors associated with the estimated parameters of the selected model are inaccurate and often too small (see, e.g., Freedman, 1983; Freedman, Navidi, and Peters, 1988). In contrast, mixture models take into account all the estimation uncertainty including the one concerning the applicability of each model to specific observations.            "
"46","Finite mixture models are similar to random coefficient models (also known as multilevel models), which are essentially a generalization of models with interaction terms (e.g., Beck and Katz 2007; Gelman and Hill 2007). For example, both methods can easily incorporate grouping of observations that naturally arise in substantive problems. This is difficult to do within the framework of standard model selection procedures. However, several notable differences exist. While random coefficient models account for theoretical heterogeneity within a single‐regression framework by varying coefficients across groups of observations, finite mixture models explicitly use a different regression model for each theory and yield both overall and observation‐specific measures of different theories’ explanatory power. Another difference is that whereas standard random coefficient models require, a priori, the specification of groups across which coefficients are allowed to vary, finite mixture models use the data to decide which group (or theory) each observation belongs to. Thus, we argue that for the purpose of empirical testing of competing theories, finite mixture models are more appropriate than random coefficient models.            "
"47","Finally, Bayesian model averaging offers an approach that is conceptually quite similar to finite mixture modeling (see Hoeting et al. 1999; Imai and King 2004). The idea is to build a final model by computing the weighted average of multiple models according to the Bayes factor of each model. Like mixture models, this method therefore accounts for model uncertainty and avoids the preliminary testing problem of standard model selection methods discussed above. Nevertheless, there are important differences. Aside from the fact that it is applicable only within the framework of Bayesian inference, Bayesian model averaging focuses on the overall assessment of competing theories and the improvement of prediction capability by combining multiple models. In contrast, finite mixture models allow researchers to explore the conditions under which each theory is applicable and identify a set of observations that are consistent with a specific theory. It is also much easier to group observations for each theory using the clustering formulation discussed earlier.            "
"48","In this section, we conduct simulation studies to explore the conditions under which the proposed method works (or does not work) well. We investigate cases with two and three competing theories and also compare the results with other common procedures. In general, we find, as expected, that more information in the data (e.g., larger sample size, continuous outcome instead of binary outcome) improves the performance of mixture models."
"49","We begin with a simple data‐generation process with two competing regression models, each of which consists of a different covariate and an intercept. These two covariates are sampled independently from a bivariate normal distribution with zero mean, unit variances, and correlation equal to . We use a binary logistic regression with one theory‐predicting variable, which is independently sampled from normal distribution with mean 10 and variance 2.16 Given this setup, we vary the logit coefficients so that the population proportion of observations consistent with Model 1 ranges from 0.1 to 0.9. Finally, two sets of outcome variables are generated. The continuous outcome variable is sampled from a linear regression with the standard normal variate error while the binary outcome is generated according to the logistic regression. The results are based on  Monte Carlo experiments.            "
"50","The four left plots in Figure 1 show the estimated proportion of observations consistent with Model 1  (the vertical axis) against their true values  (horizontal axis) for eight different simulation settings; sample size is set to either 1,000 (solid triangles with dashed lines) or 5,000 (solid circles with solid lines), and the model is either the linear regression for continuous outcome variables (first column) or binary logistic regression for dichotomous outcomes (second column), with (top row) or without (bottom row) theory‐predicting variables. The results show that for both continuous and binary outcomes, the mixture model approach recovers the true proportion of observations consistent with each theory. The model works somewhat better when the outcome is continuous and when the sample size is larger.            "
"51","                Estimated Population Proportion of Observations Consistent with Model 1 (four left plots) and Classification Success Rates (four right plots) in the Two‐Theory Mixture Model Simulation Study"
"52","Note: The results of eight simulations are reported in the figure; sample size is set to either 1,000 (solid triangles with dashed lines) or 5,000 (solid circles with solid lines), the model is either the linear regression for continuous outcome variables (first and third columns) or binary logistic regression for dichotomous outcomes (second and fourth columns), and with (top row) or without (bottom row) the theory‐predicting variable. In the four left plots, the horizontal axis represents the true proportion of observations consistent with Model 1, , while the vertical axis represents the estimated proportion, . Solid symbols indicate the average of estimates and vertical lines represent the range from 5 percentile to 95 percentile of the sampling distribution of . The expected false discovery rate is set to . The vertical axis represents the proportion of successful classification among the observations that are classified to either theory. Since this classification success rate equals 1 minus false discovery rate, we should expect the proposed procedure to give a classification success rate approximately  (indicated by blue solid horizontal line) when it is working appropriately. The eight plots together show that the proposed method performs better when the outcome variable is continuous and the sample size is larger.                        "
"53","We next examine the performance of the proposed classification method for identifying observations that are statistically significantly consistent with each theory. The right four plots in Figure 1 show the classification success rates of the proposed method. Each plot of the two right columns uses the same simulation setup as the corresponding plot in the two left columns. We set the false discovery rate to , which means that if the method is working appropriately, we would expect the classification success rates to be approximately .            "
"54","Again, the results show that the proposed method works best when the data are most informative. The best performance is obtained when the outcome variable is continuous with the large sample size. In the binary outcome case, the proposed method has larger classification error than its nominal rate, but the performance significantly improves when the sample size is larger. In addition, although not shown in the plots, the number of classified observations increases along with the amount of information. For example, simulations with a binary outcome and no theory‐predicting variable often had less than  of observations classified to either theory, whereas simulations with a continuous outcome and theory‐predicting variable regularly had greater than  of observations classified.            "
"55","Finally, we compare the proposed method of classification with the two alternative methods—the Bayesian information criteria and the Vuong test. Figure 1 of the supporting materials presents the proportion of times when Model 1 is viewed as a better model according to these methods. As expected, this proportion becomes larger as the number of observations consistent with Model 1 increases. However, this result is not comforting when all observations do not come from a single theory, in which case it is misleading to conclude from these methods one model completely dominates the other.            "
"56","Next, we examine the performance of the mixture model with three competing theories. The simulation setup is nearly identical to the above case with two competing theories. First, we sample three covariates (one for each theory) from a multivariate standard normal distribution with all pair‐wise correlations set to . Next, each observation is assigned to one of the models according to the predetermined proportions, which range from  to  for Model 1. This step is achieved by fixing coefficients to certain values in the multinomial logit model. As before, we consider two sample sizes ( and ), two outcome variable types (continuous and binary), and without a theory‐predicting variable.17"
"57","The results are based on  Monte Carlo simulations and are presented in Figure 2, whose plots are formatted in the manner identical to those in the upper row of Figure 1. The plots reveal a pattern similar to the results for the two‐theory simulation studies presented above. The proposed method performs best when the outcome variable is continuous and the sample size is large. The direct comparison between two‐theory and three‐theory simulations is difficult because the models are different, but the simulation results suggest that the observed pattern is similar between the two scenarios.            "
"58","                Estimated Population Proportion of Observations Consistent with Model 1 (two left plots) and Classification Success Rates (two right plots) without a Theory‐Predicting Variable in the Three‐Theory Mixture Model Simulation Study"
"59","Note: The format of these plots is identical to those given in the upper row of Figure 1. See its caption for details.                        "
"60","In this section, we apply the proposed mixture modeling approach to the motivating empirical example concerning competing theories of trade policy preferences introduced in an earlier section. We also test three competing theories of democratic peace by revisiting the work of Huth and Allee (2002).         "
"61","Background and Data.  The data set Hiscox (2002) collected spans over 150 years and contains the information about roll‐call voting regarding 26 and 29 trade bills in the U.S. House and Senate, respectively. Each bill is coded as either protectionist or protrade. The outcome variable, a vote, is coded 1 if a legislator votes on a particular bill against liberalization and 0 if the legislator votes for. The data set also contains covariates regarding the factoral and industrial makeup of each state, which operationalize the two competing theories. For the Stolper‐Samuelson (SS) theory, Hiscox codes the variable profit as state‐level measures of profits, the variable manufacture as employment in manufacturing, and the variable farm as agricultural production.18 For the Ricardo‐Viner (RV) model, Hiscox uses the measures of the export and import orientation of a state, export and import, respectively.19"
"62","Finally, the data contain the national‐level measure of factor specificity, which is the key theory‐predicting variable. Hiscox was unable to collect a single measure of specificity over the entire period. Instead, he uses various measures and shows that all trend closely together over time in terms of the coefficient of variation across industries (see Figures 1 and 2 of Hiscox 2002). To create a single measure of factor specificity for each year, we use the coefficient of variation based on one of the two following measures given their availability: the annual earnings in 20 industries and the annual earnings of productive workers measures.20 As can be seen from Figure 1 of Hiscox (2002), the resulting measure factor spans the entire period and tracks other measures well. Thus, this variable takes a greater value when factors are relatively specific (i.e., immobile) and a smaller value when factors are relatively nonspecific.               "
"63","Statistical Analysis.  We begin our analysis by estimating a mixture model of two logistic regression models, one with the three covariates corresponding to the SS model as main effects and the other with the three covariates corresponding to the RV model as main effects. Furthermore, instead of using fixed effects for each logistic regression as done in Hiscox (2002), we use a mixture model with clustering where all votes for a particular trade bill are assumed to be consistent with the same theory. This is a reasonable approach given that factor specificity is measured and operates at the level of national economy. Finally, we model the mixing probability (or the population proportion of observations consistent with the RV model), , using a logistic regression with factor specificity variable as the only covariate. Under the maintained hypothesis, we expect the coefficient for this variable to be positive since a greater level of factor specificity is more likely to yield support for the RV model. In sum, using our previous notation, we have the mixture model with the following components,                  "
"64","We estimate this model using the R package, flexmix, and here we provide a syntax to illustrate the simplicity of implementation in the hope that it may serve as a template for other researchers. First, the explanatory variables for each model and nesting structure must be specified. In this case, the models are completely non‐nested except that both models include the intercept. Thus, we specify two separate formulas in the following manner, model <‐ FLXMRglmfix(family = binomial,  nested = list(k = c(1, 1),formula = c(~  profit + manufacture  +farm,~  export + import))) where family specifies the logistic (default link) regression for binary outcome, k within the nested argument specifies two models being fitted, each of which has one component, and formula tells which variables belong to each model.               "
"65","Next, we specify the outcome variable vote, whether all votes for the same bill should be clustered, and the model for how specificity influences the mixture probabilities. In this step, we pass the model object produced in the first step to the function stepFlexmix(), which estimates the model using the EM algorithm with different random starting values to avoid local maxima. The syntax is as follows, result <‐ stepFlexmix(cbind(vote, 1 ‐ vote)~     1|bill, k = 2,model = model, concomitant = FLXPmultinom(factor),data = Hiscox, nrep = 20) where|bill represents a standard R syntax for clustering for each bill, k is the number of competing models, concomitant specifies the logistic regression model with factor as the sole covariate to model the mixing probabilities, data is the data frame, and nrep specifies the total number of EM algorithm runs with different starting values.               "
"66","Figure 3 plots the estimated population proportion of observations consistent with the RV model, , across the range of factor specificity variable. This serves as an overall measure of applicability of each theory. For both House and Senate, the point estimates are consistent with the hypothesis that a greater level of factor specificity makes it more likely for legislators’ votes to be explained by the RV model. The fact that the estimated probability ranges from 0.3 to 0.5 implies that factor specificity only partially explains the theoretical heterogeneity, and there may exist other important determinants of the applicability of each theory.               "
"67","                Estimated Probability of Votes for a Bill Being Consistent with the Ricardo‐Viner Model as a Function of Factor Specificity"
"68","Note: Solid line is the estimated probability with actual observations indicated by sold circles, and dashed lines represent  confidence intervals based on the Monte Carlo approximation. Although there is a considerable degree of uncertainty due to the small number of bills, the positive slopes in the House (the left panel) and Senate (the right panel) are consistent with the hypothesis that the Ricardo‐Viner model rather than the Stolper‐Samuelson model is supported when the level of factor specificity is high.                           "
"69","While the point estimates are consistent, the statistical insignificance of the factor specificity variable and the resulting wide confidence intervals in the figure suggest that the evidence for Hiscox’s hypothesis is rather weak. This does not necessarily refute his hypothesis because the statistical power is low (the data contain only 26 and 29 bills for the House and Senate, respectively). On the other hand, the fact that the model was able to classify many bills with high probabilities means that legislative voting on many of these bills can be explained well by either the RV or SS variables. In sum, our reanalysis suggests that these two trade models have high explanatory power, but a more precise test of Hiscox’s argument requires a larger data set with more bills.               "
"70","Next, we illustrate the method described earlier and identify the list of trade bills, which are statistically significantly consistent with each of the rival theories. Here, we assume that all votes for any given bill are consistent with the same theory. While this assumption is rather strong, it is similar to that of the original analysis where all votes in one period are hypothesized to be consistent with either the SS or RV model. In fact, we are able to classify all bills even when we set the (posterior) expected number of incorrect classification to be very small, e.g., . Table 1 of the supporting materials provides the resulting classification lists of trade bills for each model. Scholars may use these lists to examine whether quantitative evidence is in agreement with qualitative knowledge about each trade bill.21"
"71","Comparison with Other Methods.  Finally, the mixture modeling approach also yields estimated model parameters for each of the competing theories, i.e., , as well as the estimated coefficients on variables that are used to estimate mixing probabilities, i.e., . In Table 1, we report these estimates and compare them to the “garbage‐can” regression (the last four columns), which Achen (2005) and others (e.g., Clarke 2000; Gordon and Smith 2004) argue should be avoided. Here, the “garbage‐can” regression refers to the single logistic regression, which contains all five variables taken from both SS and RV models. Following Hiscox’s original analysis, we also include bill fixed effects in this model.               "
"72","The table shows that for the mixture modeling approach, all estimated coefficients of the two models have expected signs and are statistically significant. For example, the estimated coefficient for the farm variable is negative, implying that states with high levels of agricultural production are more likely to oppose protectionism as expected under the Stolper‐Samuelson model. In contrast, in the “garbage‐can” regression the coefficients are considerably smaller and their standard errors are larger (relative to the size of the coefficients). For example, the farm variable is not statistically significantly different from zero both in the House and Senate.22 This suggests the superior discrimination power of each variable in the mixture model despite the fact that the “garbage‐can” regression was fit to the entire data.               "
"73","The results based on the mixture model also improve upon those reported in the original article. For example, the application of the  test indicates that the SS model is selected for the period between 1945 and 1962. However, Hiscox found that the farm and manufacture variables in this model have opposite signs than what is predicted (2002, 603). In contrast, the results of the mixture model show no such inconsistency. Furthermore, when the SS model (with bill fixed effects) is fitted to the subset of votes classified to the RV model given in the second and fourth columns of Table 1 of the supporting materials, the estimated coefficient for the farm variable has a positive sign (statistically insignificant in the House and significant in the Senate), which is opposite to what the SS model predicts. On the other hand, when the model is fitted separately to the “correct” subset of the trade bills, then all estimated coefficients are statistically significant and have the expected sign in both the House and Senate. This provides evidence supporting the appropriateness of bill classifications as a whole.               "
"74","Next, we apply the proposed mixture modeling approach and test three competing theories of democratic peace. Specifically, we revisit the work of Huth and Allee (2002), who empirically test three competing models—the accountability model, the norms model, and the affinity model. Here, we focus on the military escalation stage where a defending state has already refused negotiations and each state in a conflict dyad must choose whether or not to escalate the dispute.            "
"75","Background and Data.  The original data set consists of 374 military confrontations between 1919 and 1995 in which a challenging state had initiated a conflict against a defender. Huth and Allee construct separate dichotomous dependent variables for a challenger and a defender, which equal 1 if a state chose high levels of military escalation and 0 for low or limited escalation. For each of the competing models, the authors estimate a bivariate probit model to allow for correlation between the challenger’s decision to escalate crisis and the corresponding decision of the defender.               "
"76","The accountability model argues that leaders are constrained in their ability to use force by domestic political institutions and threats from rivals. In particular, competitive elections can constrain leaders in a crisis. To operationalize this idea, Huth and Allee use measures of democracy levels for challenger and defender and interact them with various characteristics of disputes.23 The norms model emphasizes the role of beliefs held by political leaders about how to negotiate and deal with political conflict. Specifically, the argument is that norms about domestic bargaining transfer to the international level. They use a measure of how strong nonviolent norms are in the state and interact it with several characteristics of the dispute.24 Finally, the affinity model argues that conflict decision making is driven by shared interests and ideologies. As a measure of similarity, Huth and Allee use an indicator variable representing whether countries have the same regime type and another variable indicating if this similarity measure has changed in the last five years. In addition, the authors include a set of “realist” control variables in each of the three models (see Tables 9.4, 9.13, and 9.19).               "
"77","Statistical Analysis.  Based upon the lack of statistical significance of the key estimated coefficient and its wrong sign, Huth and Allee conclude that out of the three models the affinity model “produced the weakest results” (2002, 283) and that between the accountability and norms model the accountability model was superior (2002, 286). Here, we formally test the three competing theories using the proposed mixture modeling approach. We begin our analysis by fitting the mixture model consisting of Huth and Allee’s three bivariate probit models.25Figure 4 presents the smoothed histogram of estimated posterior probabilities that each observation is consistent with each competing theory, . We find that the affinity model receives the greatest support where slightly more than 80% of observations are estimated to be consistent with this model.26 However, the mixture model shows essentially no support for the accountability model, which contradicts the original finding. Interestingly, as shown in Table 2, the estimated coefficients for the other models—affinity and norms models—from the mixture model are quite similar, though the coefficients are estimated less precisely for mixture models. Note that the standard errors on nearly all of the variables are very large, making inferences about the signs of coefficients inappropriate. Here, the mixture model is choosing the most parsimonious model. This suggests that the realist control variables are explaining most of the variation in the outcome variable.               "
"78","                Smoothed Histograms of Estimated Probabilities That Each Observation Is Consistent with Each Competing Theory of Democratic Peace, "
"79","Note: Solid vertical lines represent the estimated overall probability that observations are consistent with each model, . The affinity model receives the greatest support. The estimated probability for the accountability model is essentially zero for all observations.                           "
"80","What are potential pitfalls of finite mixture models? In this section, we list several limitations of mixture modeling and discuss practical recommendations that help applied researchers avoid them (see also Section 4 of the supporting materials for empirical illustration via simulation). First, the proposed mixture modeling approach provides one way to assess the relative predictive performance of rival theories, but like any statistical method, the method in itself does not solve endogeneity and other fundamental problems of causal inference in observational studies. For example, one may estimate causal effects using a mixture model which consists of causal submodels. In such cases, the inclusion of relevant confounders in each of the submodels will be required in order to identify causal effects.         "
"81","Second, one should not test too many competing theories at once. Fitting a mixture model demands much more from the data than fitting each of the submodels separately. The fact that each submodel is identified does not necessarily imply a mixture of all submodels is identified. Even if a mixture model is identified, like any statistical modeling, overfitting can be a problem too. For example, including too many statistical models, especially the ones that are similar to each other and/or have many parameters, can lead to inefficient and sensitive inference in a small sample. The data may simply lack enough information to distinguish all models. For this reason, we recommend that researchers test only two or three competing theories with typical political science data sets. Overfitting can also be avoided by making sure that out‐of‐sample predictions of mixture models are as good as their in‐sample predictions."
"82","Third, while identification is still possible (see Grün and Leisch 2008b; Hennig 2000), high correlations across predictors may reduce the statistical power of mixture models.27 There may also be a bias toward the selection of a submodel with a greater number of predictors, especially when a more parsimonious model generates relatively few observations and/or correlations across predictors are high (see Section 4 of the supporting materials).28 We emphasize that definitive theoretical results about this question do not exist and indeed in one of our empirical applications, the most parsimonious model is selected. Unlike other model selection procedures such as the Bayesian information criteria, however, the mixture model does not explicitly penalize models with a large number of parameters. Therefore, when using the proposed approach, substantive theory (rather than statistical methods) must guide model specification.         "
"83","Finally, while mixture modeling allows one to model the conditions under which different theories are applicable, these conditions must be directly derived from the underlying assumptions of each rival theory. This is exactly the contribution made by Hiscox (2002), who realized that the relative applicability of Stolper‐Samuelson and Ricardo‐Viner depends on their assumption about factor mobility. Although the inclusion of theory‐predicting variables is appealing for both theoretical and statistical reasons, this does not mean that one can use any variables to predict the applicability of rival theories.29 Even if it avoids the “garbage‐can” regression, such an approach may be condemned as a “garbage‐truck” model!30"
"84","We have shown that finite mixture models can be used to effectively conduct empirical testing of competing theories. Given that the mixture modeling strategy outlined in this article can accommodate a wide range of statistical models, we believe that the applicability of the proposed methodology is potentially high. Although finite mixture models have a long history in statistics, their main use has been to make parametric models flexible so that they fit the data better. We have shown that this methodology can be used for the empirical testing of competing theories, which is a central goal of social science research."
"85","One important advantage of the proposed mixture modeling strategy is its ability to model the conditions under which different theories are applicable. Any theory rests upon certain assumptions, without which the theory is not applicable. However, much of empirical research takes for granted these assumptions when conducting theory testing. Evaluating the underlying assumptions is especially critical when testing rival theories because the applicability of each theory depends upon the appropriateness of different assumptions. With finite mixture models, researchers can now test a theory as a whole, including its assumptions, and explore the factors that determine when each rival theory is applicable. Given the ease of using finite mixture models, we believe that more scholars should collect variables like Hiscox’s factor mobility measure and then use them directly in their statistical analysis."
