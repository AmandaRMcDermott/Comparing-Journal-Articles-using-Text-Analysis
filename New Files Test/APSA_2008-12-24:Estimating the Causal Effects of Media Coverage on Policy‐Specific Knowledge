"","x"
"1","The mass media are thought to be the primary way people obtain information about politics (Graber 2002), but this simple fact belies decades of research on media effects and conflicting findings regarding the role of the media as an information provider. Mondak (1995a) sums up the situation in his landmark study, Nothing to Read:         "
"2","                        "
"3","Media so thoroughly dominate the dissemination of information [in the U.S.] that it seems almost a truism that media matter. Unfortunately, what seems perfectly obvious at face value does not always lend itself to ready empirical confirmation. If media truly are a nearly all‐pervasive force, then we are left with a variable that does not vary. Largely for precisely this reason, researchers have struggled to demonstrate the existence of media effects on political behavior. Methodological leverage on a question evaporates when there exists no contrast group, no persons who are not exposed to the variable of interest. Further, it is of little help that individuals voluntarily select differing levels of media exposure. (159)"
"4","In our view, four problems make it difficult for researchers to draw causal inferences about the effects of media coverage on knowledge: (1) many studies do not include measures of media content in the analysis; (2) they instead use proxy variables, such as self‐reported measures of media use; (3) analyses of observational data are prone to omitted variable bias; and (4) scholars do not place enough emphasis on examining the effects of naturalistic (i.e., “real world”) treatments. These problems are evident in varying degrees in past research on media effects. We briefly summarize that literature below."
"5","Across studies in political science and speech communications, the modal approach to estimating media effects involves the analysis of survey data alone (e.g., Bennett 1994; Brians and Wattenberg 1996; Eveland and Scheufele 2000; Zhao and Chaffee 1995). What that means, then, is that most published work on media effects does not include measures of media content. As Graber once observed, “Most researchers fail to ascertain, let alone content‐analyze, the media information that, they assume, their subjects encountered” (2004, 516).            "
"6","Instead, scholars typically rely on data that ask survey respondents about their media consumption habits (e.g., “How many days in the past week did you watch the news on TV?”). Such measures suffer from measurement error (Bartels 1993; Price and Zaller 1993; Prior 2006) and low reliability (Chaffee and Frank 1996). There also is the potential for reciprocal causation between media use and knowledge (e.g., Steger et al. 1988). Because this style of research relies almost exclusively on cross‐sectional data, omitted variable bias is a concern. Differences in knowledge between those high and low in media use may reflect preexisting differences between the two groups in terms of their level of discussion, interest in politics, or some unmeasured factor that happens to be correlated with both knowledge and media usage.1"
"7","Not surprisingly, evidence of media effects among these studies is mixed.2 Some authors report null or even negative effects, especially when comparing sources, such as print versus TV (e.g., Brians and Wattenberg 1996; Craig, Kane, and Gainous 2005; Eveland 2001, 2004; Mondak 1995b; Robinson and Davis 1990). While observational research on media effects has a variety of shortcomings, its greatest strength is that it examines whether people have been influenced by real‐world treatments (i.e., it captures people as they naturally encounter political information).            "
"8","Another common style of research is to examine learning in response to news stories in a laboratory setting (e.g., DeFleur et al. 1992; Neuman, Just, and Crigler 1992; Norris and Sanders 2003). Here, subjects are exposed to simulated “media coverage” in the form of faux news stories (read on a computer screen or viewed on TV, for example). As long as there is random assignment to treatment and control conditions, it is possible to estimate the causal impact of the stimuli (i.e., the only factor that differs across conditions is delivery of the treatment). Thus, omitted variables are not a concern—even in a between‐subjects design. The biggest drawback with this style of research is that it takes people out of their natural environment and places them in a sterile setting (e.g., Gaines, Kuklinski, and Quirk 2007). While the previous category of studies reports mixed evidence of learning, experimental work shows robust learning effects. This may be due to the artificial setting, media treatments that are too strong, or some combination of both factors.            "
"9","We identified a third and relatively small group of studies (Jerit, Barabas, and Bolsen 2006; Nicholson 2003; Price and Czilli 1996) that combine media content analysis and public opinion data. In other words, these studies examine people as they naturally encounter political information, and they include measures of media content as a variable in the analyses.3 Because these studies rely on comparisons across individuals, the potential for omitted variable bias remains. Unobserved individual‐level characteristics may influence knowledge, thereby limiting researchers' ability to attribute learning to media coverage. As Mondak observes, “No study can account for all possible determinants of information acquisition. If any potential control variable is not included in a statistical model, then the relationship between [media exposure] and news comprehension still may be spurious” (1995a, 77).            "
"10","As a result of these various threats to causal inference, past research has come to differing conclusions regarding the effect of media coverage on knowledge (see Druckman 2005b for discussion). This is most evident in studies comparing the effect of print versus television, but even when it comes to media influence more generally, considerable doubt remains regarding the magnitude of learning effects. More important, the discipline still lacks a good understanding of why news coverage has the effects that it does. The typical approach— including a media use term in the statistical analysis—does not tell us anything about how or why media coverage influences knowledge. It is not clear, for example, whether this measure captures the amount of media coverage or some other qualitative feature of the news. This problem is exacerbated by the possibility that different people interpret the media use question—and its response options—differently (e.g., Chang and Krosnick 2003; King et al. 2004).            "
"11","The approach we describe later improves upon past research by (1) including actual measures of media content in the analysis, (2) conducting intra‐individual (“within‐subjects”) comparisons to limit omitted variable bias, and (3) studying people as they ordinarily encounter political information. Our goal is to provide more solid empirical footing for a relationship that one scholar described as “the most intuitively appealing proposition in the social sciences” (Mondak 1995a, 159). We extend this body of work by showing how different aspects of media coverage (volume, breadth, and prominence) contribute to learning.            "
"12","Drawing upon past research, we expect policy‐specific knowledge to be influenced by media coverage in several predictable ways. To begin, scholars have long recognized the role that opportunity plays in the acquisition of political information (e.g., Luskin 1990). All other things equal,         "
"13","                        "
"14"," H1: we expect the public's level of policy‐relevant knowledge to be positively related to the volume of media coverage particular issues receive.                  "
"15","Multiple stories on the same topic may increase the accessibility of that information in memory (Fiske and Taylor 1991). Another possibility is that repetition allows people to mentally rehearse the contents of a news story, aiding in its retention (Tewksbury, Weaver, and Maddex 2001).         "
"16","In addition to the volume of coverage, characteristics of the news itself might affect learning (e.g., Neuman, Just, and Crigler 1992). There is, for example, some evidence that recognition is better for stories with a human interest angle or a domestic focus (Davis and Robinson 1986; Price and Czilli 1996). We extend work in this area by considering the breadth of media coverage. Assuming that people devote only a modest amount of attention to political affairs, the extent to which different media outlets are covering a news story should be related to knowledge. Thus, breadth matters because it increases the chances of exposure (e.g., a person who misses a story in one outlet might encounter it in another). Coverage in multiple outlets also helps reach a news audience that is increasingly segmented due to the proliferation of media outlets (e.g., Prior 2007). Therefore,         "
"17","                        "
"18"," H2: we expect policy‐specific knowledge to increase as the breadth of news coverage about a topic becomes greater.                  "
"19","Insofar as story location sends a signal about the relative significance of an event, it also should affect knowledge (Davis and Robinson 1986; Price and Czilli 1996). Citizens are more likely to encounter information that appears on the front page of the paper or early in a television news program and to judge those news items as more important and worthy of their attention (Graber 2004, 549). The tendency to “scan the headlines” or to tune into the beginning of a newscast means that people will know the most about topics receiving prominent coverage (e.g., Schudson 1998). All other things equal,         "
"20","                        "
"21"," H3: we expect a story's prominence to be positively related to the level of policy‐specific knowledge.                  "
"22","As we describe in more detail below, we depart from the typical approach in studies of the media. It is our contention that the media usage variable, while commonly used, is not the best way to establish media effects. In addition to the various methodological problems noted above, this measure remains one step removed from the concept of interest: news coverage. We contribute to the literature by measuring several dimensions of media coverage and incorporating this information in models that seek to explain the variation in policy‐specific knowledge."
"23","The inspiration for our empirical analyses comes from studies that examine learning in the natural world (e.g., Delli Carpini, Keeter, and Kennamer 1994; Mondak 1995a; Zukin and Snyder 1984). Unlike experiments that simulate the provision of information (e.g., asking subjects to read a news story in a lab), the treatment in our analyses is naturally occurring and completely realistic. In the remainder of this section, we describe the broad outlines of the project and detail the specific features of our two studies as we report the results.         "
"24","Using the iPoll database at the Roper Center for Public Opinion Research, we identified approximately two dozen surveys that tapped policy‐specific knowledge and that were administered by the same survey organization (Princeton Survey Research Associates [PSRA]). The closed‐ended questions on the PSRA surveys cover a range of domestic issues (gun control, health care, AIDS, Social Security, etc.); the crucial similarity is that they refer to news events that took place in the weeks leading up to each survey. Thus, knowing the correct answer to these questions depends almost exclusively on recent exposure to information in the media rather than learning that may have occurred years ago (see the appendix for a list of issues). Unlike surveys that ask people about events that have transpired over several years (e.g., Gilens 2001), the timely nature of the PSRA questions provides a tremendous amount of leverage for understanding how the mass media affect knowledge.            "
"25","We combine the knowledge batteries with content analysis of these same topics in the national news. More specifically, we analyze the full text transcripts of three national media outlets during the six weeks prior to the first day of each survey.4 Our three sources, the Associated Press, USA Today, and CBS Evening News, reflect major newswire, print, and broadcast media outlets. We do not claim that the people in our surveys were getting their news from these particular media outlets; we simply assert that these sources provide a representative view of information that was appearing in newspapers and on television around the country.5"
"26","Once we identified the relevant sample of news stories in each media outlet, we tallied the total number of articles mentioning the correct answer during the content analysis period.6 A simple story count captured the essence of what we sought to measure—namely, the amount of coverage devoted to a particular issue. In the first of two studies, we log this measure because some of the cases in our dataset received an extraordinarily high level of coverage (see Nicholson 2003 for a similar approach). Doing this has little effect on our results (i.e., our conclusions are similar regardless of whether we log the variable). However, logging does add another element of realism to our argument, for it implies that the effect of media coverage is nonlinear (i.e., that increases in media coverage will have diminishing effects; Neuman 1990).            "
"27","The breadth of coverage speaks to the ease with which people can learn about important political developments, assuming they devote only a modest amount of effort to the task. We operationalize this concept with a dichotomous indicator that is coded 1 if both newspapers and television covered a topic during the six‐week period and zero otherwise. Though somewhat crude, our measure of breadth tracks coverage in other outlets.7 Finally, we created two measures of story prominence—one represents the number of USA Today stories that contain the correct answer and appear on the front page, the other corresponds to the number of CBS stories that contain the correct answer and air before the first commercial (both of which were logged).            "
"28","Past researchers have found creative ways to study knowledge by leveraging real‐world variations in media coverage (e.g., Delli Carpini, Keeter, and Kennamer 1994; Mondak 1995a; Zukin and Snyder 1984). In these studies, some people are exposed to naturally occurring treatments and others are unexposed. Differences in knowledge between the treatment and control groups are attributed to varying levels of media coverage. In Mondak's case, for example, a newspaper strike in Pittsburgh set the stage for an examination of the effects of local newspaper coverage on knowledge in Pittsburgh (the treatment group) and neighboring Cleveland (the control group). Such between‐subjects designs, while powerful, are not foolproof. In the absence of random assignment one cannot be certain, even with sophisticated statistical techniques, that the two groups are comparable (or “balanced”). Thus, what appears as a media effect or a noneffect might be due to underlying differences in the treatment and control groups.         "
"29","In the first of two studies, we address this limitation by using a within‐survey/within‐subjects design (also see Jacoby 2000 or Sigelman 1980). In a nutshell, we will compare individual i's level of knowledge on topic j when there is media coverage with i's knowledge of that same topic without media coverage. More specifically, we draw upon 23 PSRA surveys, each of which asks respondents multiple questions about a single political event (e.g., a battery of questions about a bill pending in Congress or several items about a government report on Social Security). Thus, we have multiple observations for the same respondent at a single moment in time (i.e., within the same survey). Crucially, there also is variation in the amount of media coverage devoted to different aspects of the same news event.         "
"30","These within‐survey/within‐subjects comparisons hold all individual‐level factors, observed and unobserved, constant. Regardless of the individual‐level differences that explain variation in knowledge in the cross‐section (education, income, etc.), for any given individual in these surveys, differences in knowledge can be attributed to varying levels of media attention. Accordingly, we define the “treatment effect” of media coverage as the difference in knowledge across topics receiving varying levels of media coverage.8"
"31","We begin by documenting examples of treatment effects within four surveys on different issues. Panel A of Figure 1 shows knowledge patterns for health care proposals offered by President Bill Clinton in 1997. The first proposal—expanding long‐term care—did not receive any coverage in the sources we examined. Knowledge for this item was quite low. Only 14% of the 316 respondents in the sample gave the correct answer (with a 95% confidence interval of 10% to 18% in gray shading). A second question in the same survey asked whether Clinton proposed expanding coverage for people with low income. Again there was no coverage and again very few respondents (only 20%) provided the correct answer. Indeed, the confidence interval (15 to 25) indicates that responses to this question are not statistically different from those on the long‐term care question. These same respondents also were asked about two health care proposals that received media attention. For the third item (nine stories across all three sources), the percent who knew that Clinton proposed expanding coverage for the working poor was 71% (65 to 76). The most heavily covered part of Clinton's health care proposals concerned expanding coverage for children (18 news stories). Nearly 80% (75 to 85) of the respondents knew that Clinton wanted to expand coverage for children.9"
"32","                 Policy‐Specific Knowledge Across Varying Levels of Media Coverage                      "
"33","Since the same respondents were asked all four questions in the same survey, it is possible to subtract each person's response on one of the options receiving coverage (the “treatment” condition) from that same person's response without any coverage (the “control” condition). In Panel A of Figure 1, we compute four “treatment effects” comparing the cases with some coverage (9 or 18 stories) to either of the items that received no coverage. For example, comparing the first and fourth items, the treatment effect is 66 percentage points (.80 – .14 = .66, s.e. = .03). This is the largest effect we observe in our data.10 The other effects are sizeable but smaller at 57% (item 3 vs. item 1), 59% (item 4 vs. item 2), and 51% (item 3 vs. item 2).         "
"34","We present three other sets of questions appearing in separate cross‐sectional surveys, each of which uses the combined measure of media coverage. As with Panel A, knowledge varies positively with the number of news stories devoted to each aspect of the issue. In Panels B through D of Figure 1, levels of knowledge for the baseline items are low and statistically indistinguishable within each survey. The treatment items are higher and correspond in rough fashion to the size of the increase in the number of stories for presidential proposals on the subject of gun control (Panel B), a government report on medical errors (Panel C), and recommendations from a national commission on Social Security (Panel D).         "
"35","With the exception of Panel D, all the effects increase in magnitude along with media coverage. The two treated items in Panel D were covered in just a few stories (three and four each). In both cases knowledge levels are greater than the two items that did not receive any coverage, but there is an unexpectedly large increase in knowledge on the eligibility age item relative to the positive treatment effect for the private investment accounts proposal. The aberration may be due to the way we aggregated stories across the three media outlets. Such anomalies prove inconsequential later when we examine the effects separated by source."
"36","Before doing that, however, two patterns from Figure 1 are worth noting. First, in these data the relationship between knowledge and media coverage appears to be positive and nonlinear. That is to say, knowledge gains are largest as the level of media coverage changes from no coverage to some coverage (around nine stories). Once an issue receives some coverage, additional media attention (10 stories and beyond) does little to increase policy‐specific knowledge. The second notable pattern concerns variation in the baseline level of knowledge. In Figure 1 the baseline ranges from 13% to 40%, a difference we attribute to variation across the topics in our dataset (issue difficulty, question wording, etc.). We address these differences more systematically below.         "
"37","In a moment we will conduct a regression analysis of the treatment effects. First, however, we provide a bird's‐eye view of the range of these effects. In our dataset, there are 65 comparisons of no coverage versus some coverage. As shown in Figure 2, in 60 out of 65 cases (92%) the treatment effect is positive; in nearly all of those cases that effect is statistically distinguishable from zero. The most notable feature of Figure 2 is the range of treatment effects. Positive effects range from a gain of 1 percentage point (coverage of the human papilloma virus) to 66 (presidential health care proposals).         "
"38","                 Knowledge Treatment Effects                      "
"39"," Note: The figure displays cases in which the comparison is no coverage versus some coverage (N= 65).                     "
"40","We also identified another 48 cases where it is possible to conduct ordinal comparisons (some coverage versus more coverage).11 The statistics for treatment effects are similar. Thirty‐seven of the 48 (77%) are positive, and 32 of 48 (67%) are statistically significant. All together, 97 of 113 (86%) comparisons are positive (i.e., media effect > 0) and in 88 (78%) the confidence intervals of those treatment effects do not overlap zero. Across all comparisons the average size of the treatment effect is 15 percentage points (s.d. = 17).12 We also can determine if the size of the effect depends on when the news event took place. Treatment effects are smaller (p < .10; one‐tailed t‐test) for events that took place more than a month before the survey (this corresponds to the fourth quartile of a variable measuring the number of days between the event and the start of the survey). Effect sizes are indistinguishable across the other quartiles.         "
"41","Having shown that substantial treatment effects exist, we next sought to account for the varying size of those effects. Table 1 shows the results of an analysis in which we regress the treatment effect (i.e., knowledge gains) on the three media measures. We begin by looking at the effect of volume, which is operationalized as the difference in the number of stories across the treatment and control questions.         "
"42","The first series of models (Models 1–4) examines just those issues in which the comparison is between topics that received no coverage versus some coverage. The second series of models (Models 5–10) examine all of our cases (i.e., they include the zero vs. some coverage comparisons as well as all other ordinal comparisons). We capture differences across the issues in our dataset with fixed effects terms (i.e., dummies for each survey) that have been suppressed for presentation purposes."
"43","Focusing first on Model 1, the coefficient on our combined volume measure is positive and significant (coeff = 16.51; p < .01). Moving across the columns, we see that the same pattern appears when we include separate measures for newswire, print, and television news (Models 2–4). To put these findings in context, a two standard deviation change above and below the mean for either newspapers or television news results in a roughly 32 percentage point increase in policy‐specific knowledge (the magnitude of this effect diminishes once we control for breadth and prominence). The positive and significant finding for the combined volume measure also extends to all ordinal comparisons (Model 5). In this second set of models, medium‐specific differences once again appear to be inconsequential. Regardless of whether we examine the amount of coverage in newswire, print, or television sources, our measure of logged volume is positively and significantly related to the dependent variable (Models 6–8). Here the predicted effect of the same two standard deviation change in newspaper or television coverage is about 30 percentage points. Consistent with our first hypothesis, more media coverage leads to higher levels of policy‐specific knowledge (though the logged operationalization of Volume implies that this effect tapers off at very high levels of coverage).            "
"44","What about the influence of other features of news coverage? According to our second hypothesis, the breadth of media coverage should affect knowledge even after we control for differences in the number of news stories. To recap our earlier argument, breadth speaks to the number of distinct outlets that are covering a news story, and it bears directly on the opportunity for learning. Our third hypothesis predicts that knowledge will be higher for topics receiving more prominent coverage (measured either in terms of the number of front‐page stories in USA Today or the number of CBS Evening News reports aired before the first commercial). As Graber (2004) and others have argued, story location sends a cue to audience members about the significance of a news event. All other things equal, people should be more knowledgeable about events that receive more prominent coverage. We test both hypotheses in Models 9 and 10, where we regress the treatment effect on our combined measure of news stories in addition to our indicators for breadth and prominence.            "
"45","Model 9 shows that even when we control for variation in the amount of media coverage, the breadth of that coverage affects the public's awareness of recent news events. In fact, there is a nearly 11‐point gain in policy‐specific knowledge when a news event is covered by both television and print news (p < .01; Model 9). Reading down the column, we also see that issues covered more prominently in print news also are associated with higher levels of policy‐specific knowledge (p < .05). Specifically, a two standard deviation change above and below the mean on Newspaper prominence results in a 15‐point change in knowledge. The final column shows the same basic pattern for a model including Breadth and Television prominence (p < .01 for both; Model 10). A similar two standard deviation change in Television prominence results in a 14‐point change in policy‐specific knowledge.13"
"46","Taken together, our analyses indicate that where a story is placed matters just as much as the amount of coverage. More specifically, the treatment effect for an issue that receives a high level of coverage but has very few stories appearing on the front page is 19.5 points. Conversely, the treatment effect for an issue that receives a low level of coverage but whose stories are prominently placed (e.g., the front page) is 20.6 points.14 This suggests that one of the most effective ways to raise awareness of important political developments may not be to increase the number of stories, but to make sure that whatever the level of coverage, stories appear prominently in the news. Of course, the treatment effect for events that receive high coverage and have many stories on the front page is even larger (34.4 points). Our point is that sizeable gains in policy‐specific knowledge can be achieved by moving articles from the inside of the paper to the front or by placing a story before rather than after the first commercial break of the evening news. Naturally, such changes may mean less emphasis on other issues.            "
"47","In an effort to speak more directly to the differences between the events in our dataset, we included several issue‐specific indicators in addition to the terms in Models 9 and 10. These dummy variables take on a value of 1 if the issue was characterized as a health care topic, an issue that affected senior citizens, or a partisan issue. None of these indicators were related to knowledge (p‐values average .64 and range from .28 to .89). More importantly, even when we control for differences in issue type, our combined measure of volume and our indicators for breadth and prominence retain their same sign and level of significance.15"
"48","To this point, we have been examining aggregates (e.g., the percentage providing a correct answer to the control question vs. the treatment question). The previous analysis showed that variation in the size of aggregate knowledge gains could be explained by changes in the volume, breadth, and prominence of news coverage. Here we look at individual survey respondents and examine whether aspects of the media environment still matter after controlling for characteristics such as a person's education, income, and age. We take advantage of the fact that each respondent was asked multiple questions about the same topic and create an individual‐level measure of “learning.” This dependent variable is coded “1” if a person answered a question incorrectly in the control condition but gave a correct response in the treatment condition. Respondents are given a score of “−1” if they exhibited the opposite pattern (i.e., they answered the control question correctly, but answered the treatment question incorrectly). Those who answered both questions correctly or incorrectly are coded as “0.”16 This setup will allow us to analyze the effect of our three media variables and individual‐level characteristics in the context of a single model.17 The results of an ordered probit analysis appear in Table 2.            "
"49","The first three rows show the coefficients for volume (all sources combined), breadth, and prominence. Because we employ two measures of prominence (one for newspapers and another for television), we report estimates for models including each measure. Consistent with our three hypotheses, all three media measures are positively associated with knowledge gains (p < .01). This means that the likelihood of “learning” (i.e., changing from an incorrect answer on the control question to a correct response on the treatment item) increases along with the amount of media coverage as well as the breadth and prominence of that coverage. Much as one would expect, individual‐level factors such as a person's level of education, age, and following the topic also are positively related to learning (p < .01).18Table 2 also shows that some factors, such as a person's income and his or her race, are unrelated to knowledge gains across the treatment and control items in our dataset.19"
"50","The second and fourth columns in Table 2 show first differences, which represent the change in the predicted probability of being in the learning category for different values of the independent variables. Once again, for each independent variable, we examined a two standard deviation change around the mean, with the other variables set either to their mean or mode. Several interesting patterns emerge here. First, we see that the breadth and prominence of coverage are as least as important as the amount of coverage (in terms of the magnitude of effects), and in the television prominence model they are more important. Second, with only one exception, the impact of all three environmental factors is greater than that of common predictors such as a person's level of education and attentiveness. Thus, not only do our three media variables “matter” in the statistical sense, but also their substantive significance is on par with factors that have long been at the forefront of research on political knowledge.            "
"51","So far, Study 1 provided support for all three of our hypotheses, but several factors threaten our ability to make causal inferences. The first is spurious causation, or the possibility that a third factor is driving both knowledge and media coverage. The fact that some of the cases in our dataset involve identifiable groups (e.g., senior citizens, children) is potentially problematic in that regard. News events pertaining to groups might be more likely to pique the public interest and garner media coverage (e.g., Schneider and Ingram 1993). We explored this possibility by determining whether the treatment case in each of the 113 comparisons invoked an identifiable group, such as children, seniors, low‐income people, and so on. When we include this variable in the models from Tables 1 and 2, our key substantive findings remain unchanged. We explored other possibilities (e.g., whether the topic had been on the agenda before or concerned a public health topic) and found no evidence of spurious causation. While it may be difficult to rule out every alternative, we have eliminated some of the most obvious candidates for spurious causation. This topic and a related one (endogeneity) are given fuller treatment in the appendix. There we conduct an auxiliary set of analyses in which we predict media attention to the issues in our dataset (see Smith 2001 for a related approach). At least for the set of issues in this study (i.e., recent news events), our analysis is not plagued by endogeneity. The third, and final, threat to causal inference has to do with the slight variations in topic across the within‐survey/within‐subjects comparisons (see note 8). We address this issue with a second study.            "
"52","Our first study examined respondents' knowledge at a single point in time (e.g., the four gun control measures depicted in Panel B of Figure 1). To discern the effects of the news on learning, we compared items which received media coverage (e.g., gun safety locks and background checks at gun shows) to gun control policies that received little or no coverage (e.g., a gun buyback program and an agreement with the NRA). While everything about the survey respondents was held constant, these comparisons required an assumption about issue topic equivalence; namely, that topic j=j′. Here we do not need to make that assumption. We focus on several surveys in which different respondents were asked identically worded questions about the same topic at two time points.20 We begin by describing the aggregate‐level patterns across these three cases and then present the results from a statistical analysis.         "
"53","The first set of over time comparisons asks respondents about patients' rights legislation in Congress. From May 31 to June 3, 2001, a random sample of 1,001 respondents was asked, “From what you have seen or heard, is the debate over patients' rights legislation about … allowing patients to sue their health insurance plans, how much managed care plans can charge patients, or the privacy of medical records?” This question was repeated from August 2 to 5, 2001, to a different cross‐section of 1,005 people. In the weeks leading up to the first survey, the correct answer to this question appeared in seven stories across our three sources, and 24% of respondents provided the right answer. As a result of an agreement between politicians on the right to sue, media coverage of the issue increased substantially (to 72 stories) in the six weeks before the August survey. This time, when respondents were asked about the legislation, close to half (46%) gave the correct answer to the question, representing a 22 percentage point increase."
"54","Our second set of over time comparisons employs two surveys on the fiscal status of the Social Security and Medicare trust funds. Of critical concern is the relative financial strength of each trust fund, an important fact that could determine which program policymakers attempt to reform first. In the first survey on December 8–13, 1998, a random sample of 1,201 respondents was asked, “According to news reports, both Social Security and Medicare are facing financial problems in the future. If Congress doesn't take any action, which of these two programs is expected to be the first to not have enough money to cover all benefits—Social Security or Medicare?” This same question was repeated four months later on April 10–22, 1999, to a separate cross‐section of 1,200 respondents."
"55","The correct answer to this question is Medicare, but only 38% gave the correct response in the December survey. In the weeks leading up to this survey, we were unable to identify a single story providing the correct answer to the question in any of our three sources. Following reports from the Social Security and Medicare trustees in late March, there was a slight increase in media attention to the issue. In the six weeks prior to the April survey, the correct answer appeared twice in the AP, once in USA Today, and once on CBS Evening News. When asked in April about which program was in more financial trouble, a plurality (44%) correctly named Medicare—a 6 percentage point increase over the December survey.         "
"56","Our final case involves the passage of a Medicare prescription drug bill in 2004. Knowledge of the bill's passage increased 8 percentage points (from 23 to 31%) over a two‐month period in which media coverage of the issue increased nearly 50% (from 42 stories providing the correct answer before the first survey to 61 stories before the second survey).21"
"57"," Table 3 shows the results for a series of probit models. We look for evidence of media‐induced learning by examining the sign and significance of the T2 indicator (After news coverage). Consistent with Hypothesis 1, we observed learning in response to an increase in media coverage in all three cases (p < .01). The coefficients correspond to predicted knowledge gains of 20 points (patients' rights), 6 points (trust funds), and 9 points (prescription drugs). In the patients' rights case, both the T1 and T2 surveys included a question asking respondents if they had been following news coverage of this issue. In auxiliary analyses (not shown), the interaction between this term and After news coverage is positive and significant (p < .01), indicating that the largest knowledge gains occurred among the most attentive.22"
"58","Unfortunately, neither of the other two cases includes a “follows” question or measures of general political knowledge. However, they both address issues that were likely to be of interest to people over 65, so we examined the interaction between Age and After news coverage, on the assumption that this age group would be the most attentive to news stories about Social Security and Medicare. The results were mixed. For the trust funds analysis, this is in fact the case, as the coefficient on the interaction is positive and statistically significant (coeff. = .48; p < .10). However, in this model the T2 indicator is no longer significant, suggesting that learning on this issue was concentrated exclusively among people over 65 (a pattern that might be expected given the slight increase in media coverage between T1 and T2). In the case involving prescription drugs, After news coverage is significant while the interaction between that term and Age is not. This might reflect the fact that coverage of this issue already was high before the increase in news coverage (making it possible that everyone—even the less attentive—had been exposed to stories about the bill; see Price and Zaller 1993, 148).         "
"59","Nevertheless, in all three cases we obtain results that are consistent with the findings from Study 1. This represents additional confirmation of Hypothesis 1—obtained with a different design, different respondents, and different questions. Taken together, Studies 1 and 2 indicate that the volume of media coverage along with the breadth and prominence of that coverage independently contribute to changes in the level of knowledge among the American public."
"60","This study heeds researchers' calls to merge the strengths of survey and experimental designs (e.g., Mondak 1995b, 525). Because we combine 23 nationally representative surveys that cover a range of domestic issues, the treatment effects we document have a high degree of external validity. At the same time, our study retains many of the advantages of a natural experiment, i.e., we examine learning about actual political developments in the United States. Everything about the media coverage these events received, as well as the conditions under which people were exposed to it, is naturally occurring. Finally, the use of within‐survey/within‐subjects comparisons (Study 1) and over time analysis (Study 2) helps isolate the causal effects of media coverage.         "
"61","These advances do not mean our study is without limitations. Aside from concerns about the exogeneity of media coverage that we address in the appendix, we assume that our media sources are representative of the information environment. We also assume that the topics selected by the survey organization are representative (i.e., not biased in a way that might affect our results). Finally, and on a somewhat different note, we say little about the meaning citizens attribute to real‐world events (Gaines et al. 2007; Neuman, Just, and Crigler 1992). Nevertheless, this study takes advantage of a powerful way to assess the causal impact of media coverage on knowledge. And it does so in a way that makes it possible to explore which aspects of the news matter most.         "
"62","It also bears repeating that this study examines a theoretically important type of knowledge. Policy‐specific information may influence a person's evaluation of elected officials as well as the significance people attribute to particular social and political problems. Not only do these considerations influence vote choice, but they also color a person's disposition towards government (e.g., trust). Moreover, any given policy‐relevant fact may have multiple consequences. Perceptions of the crime rate, for example, may shape a person's views regarding sentencing guidelines, prison construction, and police conduct (Gilens 2001, 381).         "
"63","In the past, scholars have discounted the importance of general knowledge on the grounds that decision making shortcuts make it possible for citizens to vote and take positions on issues without having to recite facts about the U.S. political system (e.g., Lupia 1994; Popkin 1991). But low information rationality does not imply no information rationality (Delli Carpini and Keeter 1996, 52). Even the simple act of voting often requires some policy‐relevant information. Heuristics, such as partisan identification, can reduce information costs somewhat, but as Delli Carpini and Keeter explain, “citizens still need to have more specific information about public officials, political parties, and public policy … Without such information any shortcut to political decision making will lose its efficacy” (1996, 55). This study furthers our understanding of the features of media coverage that make it easier for people to acquire this kind of information. What would seem to be the most obvious determinant of media effects—the volume of coverage—is not the only or even the most important predictor of knowledge. The breadth of coverage and the prominence of a story are equally powerful predictors of knowledge and are more important than demographic characteristics or indicators of socioeconomic status.         "
