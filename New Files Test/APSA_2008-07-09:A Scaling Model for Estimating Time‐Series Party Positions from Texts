"","x"
"1","Party positions are unobservable and must therefore be treated as a latent variable in empirical work. Scholars face the challenge of measuring these underlying party positions and policy dimensions. Parties reveal their positions indirectly through a variety of activities. They publish manifestos prior to elections in which they state policy goals, they make political statements and speeches, and their members cast votes in parliaments (Benoit and Laver 2006b). Currently, there are three primary methods for estimating latent party positions. Hand coding and computer‐based analysis of manifestos assume that election manifestos contain precise information about party positions at a particular point in time. Expert surveys measure the positions not from primary sources, but indirectly through judgments of country specialists who rely on a variety of sources beyond manifestos to form an opinion.1"
"2","In an ideal world, regularly conducted expert surveys may provide the best means for estimating party positions. Experts are able to synthesize large quantities of information from various sources, including manifestos, speeches, voting patterns, and media reports (Benoit and Laver 2006b). Moreover, surveys may be able to examine when new issues arise and determine their relative importance (Castles and Mair 1984; Huber and Inglehart 1995). Experts are able to tell researchers what, in their opinion, are the salient dimensions, rather than leaving the researcher to guess or assign arbitrary weights. From a pragmatic standpoint, however, expert surveys are difficult and expensive to repeat over time and across countries, requiring continuous sources of funding to conduct new surveys at regular intervals. Often, they require multilingual research teams. If a researcher realizes that a survey failed to include a question, it is impossible to go back in time to retrieve that information. Frequently, surveys phrase questions differently, making the comparisons across surveys questionable. Moreover, it is difficult to know whether different experts across countries and over time understand and answer the questions in a similar manner. While surveys often come up short as pooled cross‐sectional time‐series data, they do provide researchers with a method for checking the validity of position estimates from other methods in addition to providing a snapshot of party positions at one point in time (Gabel and Huber 2000).            "
"3","Probably the most well‐known and widely used method for generating party positions is hand coding of party manifestos. The Comparative Manifestos Project (CMP; Budge, Robertson, and Hearl 1987; Budge et al. 2001) has greatly advanced the ability of scholars to conduct comparative research by providing estimates of party positions across countries and over time. The CMP group has created 56 issues, which fall into seven major categories. To generate party positions, the CMP group codes the number of quasi‐sentences which fall into each issue and then divide by the total number of quasi‐sentences in the manifesto to control for manifesto length. Thus, the score for each party for each issue is simply the percentage of total sentences which fall into this issue.            "
"4","To calculate party positions on a left‐right dimension from these data, scholars have employed several methods. Laver and Budge (1992) provide one of the more commonly used approaches. They identify several important issues as left‐wing issues and others as right‐wing issues. Then they simply sum the left‐wing scores and the right‐wing scores and subtract the right totals from the left totals. The problem is that not all 56 categories can be attributed to the left or to the right. Thus, even though two parties may discuss the left‐wing issues in an identical manner, if one party mentions neutral issues while the other does not, the positions of these parties will be coded differently.2"
"5","In addition, left and right issues may vary across countries and over time. This may create problems for constructing a valid left‐right scale. For example, in the United States, decentralization would probably be a right‐wing issue while in other countries it may be a neutral issue, or even a left‐wing issue. Moreover, it is not clear that all issues should be given the same weight in determining party positions, and weights may vary across countries and time. The fixed coding scheme of the CMP also means important new issues must be placed into existing categories (e.g., global terrorism after 9/11). Other categories may no longer be relevant (e.g., foreign special relations between West and East Germany after 1990)."
"6","There have been several attempts to fix the manifesto scheme. Gabel and Huber (2000), for example, suggest simply extracting the first principal component from the 56 issues, an approach they refer to as the vanilla method. Others have retained the seven main categories in the original dataset and then extracted principal components from each category (Klingemann 1995).            "
"7","The hand‐coding approach provides the only cross‐sectional time‐series database on party positions to date. It has the advantage that researchers know exactly what issues are included in the left‐right dimension because categories are defined. However, the coding scheme of left‐right positions itself is problematic and can lead to invalid positions. Moreover, because the manifestos have been coded only once, researchers do not know the uncertainty associated with this technique.3 Finally, such a project is costly and difficult to replicate.            "
"8","The most recent innovation in estimating party positions involves computer‐based content analysis of party manifestos. This method attempts to reduce both the costs and likelihood of human error associated with hand coding texts. Laver, Benoit, and Garry (2003) make great advances in computer‐based content analysis by suggesting the use of reference texts rather than hand‐coded dictionaries.4 Using this approach, researchers first identify reference texts known to represent the extremes of the political space (and possibly the center as well). This one‐dimensional space is anchored by assigning reference values to the reference texts, ideally obtained from previous expert surveys. Laver, Benoit, and Garry's computer program Wordscores then counts the number of times each word occurs in the reference texts and compares these counts to word counts from the texts being analyzed. The manifestos are placed on a continuum between the reference texts depending on how similar the word counts are to each reference text. This method clearly constitutes a breakthrough for quantitative content analysis of manifestos. It is easy to implement, and researchers can apply it in almost any setting.            "
"9","Nevertheless, there are several issues with the Wordscores technique, which our approach aims to address. First, the usefulness of the Wordscores approach hinges on the ability of the researcher to identify appropriate reference texts and reference values. Scholars or experts can reasonably disagree about the extremes of the political space. The choice of reference values becomes even more critical when positions are estimated for more than one dimension. To estimate multiple dimensions, Laver and his co‐authors propose that researchers use different reference values on the exact same references texts. This is problematic for two reasons. First, they suggest that it is feasible to generate specific policy dimension estimates from the entire manifesto, even though only some parts of the text deal with the issue under investigation. Second, if analysts have the same two extreme reference texts for all policy dimensions, then party placements hinge on the reference values attributed to the center parties alone. Exogenous measures of a single reference party position could therefore determine the Wordscores results.5"
"10","Second, Wordscores assigns all words the same weight in the estimation process. Thus, words that occur frequently in all texts and provide little political information, such as conjunctions and articles, pull the document scores towards the center of the space, making these scores incomparable with the original reference values assigned to the reference texts. To make these scores comparable, Laver, Benoit, and Garry (2003) rescale the raw scores by stretching the variance of document scores to equal the variance of the reference text scores. Martin and Vanberg (2008) point out, however, that the particular rescaling algorithm used by Laver, Benoit, and Garry (2003) does not place the transformed scores on the same metric as the reference texts. They offer a new rescaling technique which leads to different results from those produced by the original rescaling procedure. We avoid this problem entirely by estimating the importance of words for discriminating between party positions rather than treating all words equally.            "
"11","Finally, time‐series estimation is problematic using Wordscores. The Wordscores authors argue that their technique should not be used for time‐series analysis because the political lexicon is constantly in flux (Benoit and Laver 2006a, 133). Nevertheless, scholars seem willing to assume that political language is sufficiently stable to use this technique for time‐series estimation (Budge and Pennings 2006; Hug and Schulz 2007; McGuire and Vanberg 2005). The bigger issue for time‐series estimation using Wordscores is the proper identification of reference texts. This challenge has led researchers to adopt various approaches in order to apply Wordscores to time‐series data, all of which come with their own problems. Some analysts concatenate all manifestos over the entire time period in order to produce long reference texts (Budge and Pennings 2006), others run the algorithm twice using two different sets of reference texts from different time periods (Hug and Schulz 2007), and, lastly, some pick two reference texts from different time periods assuming that these constitute the extremes during the entire period (McGuire and Vanberg 2005).6 Time‐series party positions can be estimated with Wordscores if one is ready to make three assumptions. First, the political lexicon remains sufficiently stable over time, second, chosen reference texts include all relevant words over time, and third, the reference texts represent the most extreme positions during the time period. We propose an approach which does not rely on reference texts and therefore does not make the latter two assumptions.            "
"12","This article presents an easy‐to‐implement statistical scaling model to estimate time‐series policy positions from political texts. Like other manifesto‐based position estimates, this approach assumes that relative word usage of parties provides information about their placement in a policy space. The advantage of this new approach is threefold: its ability to produce time‐series estimates, the fact that it does not require the use of reference texts because it instead assumes an underlying statistical distribution of word counts, and, lastly, the ability to use all words in every document and to estimate the importance of each of these words."
"13","This approach draws on a long tradition of quantitative analysis of text. Authorship studies, for example, try to identify authors based on their literary styles. To do so, linguists attempt to uncover characteristics of a particular author by measuring and counting stylistic traits (Holmes 1985; Peng and Hengartner 2002). This technique has been prominently applied in political science to identify authorship of the unsigned Federalist Papers (Mosteller and Wallace 1964).         "
"14","The process by which words are generated in a text is highly complex, but to facilitate analysis, linguists commonly use a naïve Bayes assumption in applied work (Eyheramendy, Lewis, and Madigan 2003; Lewis 1998). A text is represented as a vector of word counts or occurrences. Individual words are assumed to be distributed at random. Put differently, the probability that each word occurs in a text is independent of the position of other words in the text. It has been pointed out that “while this assumption is clearly false in most real‐world tasks, naïve Bayes often performs classification very well” (McCallum and Nigam 1998, 1). Scholars then have tried to determine statistical distributions which most accurately approximate word usage. Commonly used distributions include the Poisson (Mosteller and Wallace 1964), the negative binomial (Mosteller and Wallace 1964) and other Poisson mixtures (Church and Gale 1995), as well as zero‐inflated (binomial) distributions (Jansche 2003). All of these distributions are heavily skewed, as is the case of word usage.         "
"15","Political scientists have started to make use of the naïve Bayes assumption and word frequency distributions to analyze political text. Monroe and Maeda (2004) use a Poisson word count distribution to extract multidimensional positions of U.S. legislators from their speeches. They find that the principal dimension of speech in the U.S. Congress is of a linguistic nature, with the second dimension yielding policy‐relevant results.         "
"16","We analyze word frequencies of party manifestos and assume the frequencies are generated by a Poisson process. This particular distribution is chosen because of its estimation simplicity: it only has one parameter, λ, which is both the mean and the variance. This assumption means that the number of times party i mentions word j in election year t is drawn from a Poisson distribution. This model specification is essentially a Poisson naïve Bayes model and has also been used by Monroe and Maeda (2004). We later apply other distributions to test the robustness of our findings to the distributional assumption. The functional form of the model is as follows:            "
"17","This model treats each election manifesto as a separate party position and all positions are estimated simultaneously. In other words, the position of party i's manifesto in election t‐1 does not constrain the position of party i's manifesto in election t. If a party maintains a similar position from one election to the next, it means the party has used words in similar relative frequencies over time. On the other hand, if the model indicates that a party moves away from its former position and closer to the position of a rival, it implies that the party's new word choice more closely resembles that of the rival's than of its former self. An alternate specification might assume that a party's position at time t is both a function of its word choice at time t and its position in previous elections. Such a specification might ensure smooth party movement over time, but the movement would both be a function of the word usage and the assumptions about the model's functional form. The current specification has the advantage that observed party movement is, in fact, due to changes in word frequencies and is not an artifact of the model.         "
"18","As specified, the model estimates positions on a single dimension. Using the entire manifesto text as data, we expect this dimension to correspond to a left‐right politics dimension, which we confirm by comparing the results to other estimates of left‐right positions. This expectation is justified if manifestos (or other documents being analyzed) are encyclopedic statements of the parties' positions.7 To obtain specific policy positions, we modify the text data to be analyzed. For example, we estimate economic positions by running the model on manifesto sections regarding economic policy only. This approach is in contrast to Monroe and Maeda (2004) and other factor analytic techniques, which interpret multidimensional scores ex post. It is also different from Laver, Benoit, and Garry (2003), who estimate different dimensions not by altering the text inputs but by changing the reference values assigned to reference texts.         "
"19","Unlike a standard Poisson regression model, the entire right‐hand side of the equation needs to be estimated. To do this, we use an expectation maximization (EM) algorithm. The EM algorithm is an iterative procedure to compute maximum likelihood estimates for latent variables (McLachlan and Krishnan 1997). The E step involves calculating the expectation of the latent variable as if it were observed. The M step then maximizes the log‐likelihood conditional on the expectation. The implementation of this algorithm entails several steps:            "
"20","                              "
"21","                   Calculate starting values.                      "
"22","We obtain starting values for word fixed effects (ψ) by calculating the logged mean count of each word. For the party fixed effects (α), we use the logged ratio of the mean word count of each party‐election manifesto relative to the first party election in our dataset. We set the starting values relative to the first party‐election because this party fixed effect is set to zero during the estimation in order to identify the model. To obtain starting values for word weights (β) and party positions (ω) from the word frequencies, we first subtract the starting values for the word and party fixed effects from the logged word frequencies. We then use the left‐ and right‐singular vectors from a singular value decomposition of this matrix as starting values for ω and β."
"23","                   Estimate party parameters.                      "
"24","We estimate party parameters (ω and α) conditional on our expectation for the word parameters. In the first iteration, our expectation of those word parameters equals their starting values calculated in step 1. We maximize the following log‐likelihood for each party‐election it:                        "
"25","                   Estimate word parameters.                      "
"26","We estimate word parameters (ψ and β) conditional on our expectation for the party parameters, which we obtain in step 2. For each word j, we maximize the log‐likelihood:8"
"27","                   Calculate log‐likelihood.                      "
"28","The log‐likelihood of our model is the sum of the individual word log‐likelihoods from step 3, which are themselves calculated conditional upon the party log‐likelihoods from step 2:                         "
"29","                   Repeat steps 2–4 until convergence.                      "
"30","Using the new expectations for the word parameters, we reestimate party parameters (step 2). Then, using those expectations, we reestimate word parameters (step 3). This process is repeated until an acceptable level of convergence, measured as the difference in the log‐likelihood from step 4 between the current and the previous iteration, is reached."
"31","We obtain confidence intervals for the estimates using a parametric bootstrap. We first estimate all parameters by running the EM algorithm described above. From these ML estimates, we calculate λijt for each cell in the dataset. We then generate 500 new datasets, each time taking random draws from a Poisson distribution with parameter λijt for each cell in the word count matrix. Finally, using the ML estimates as starting values, we rerun the algorithm on each of these datasets and estimate 500 new party positions. We use the 0.025 and the 0.975 quantiles of the simulated party positions as an approximate 95% confidence interval.9 Our method for estimating party positions is one of few which allows researchers to measure the uncertainty associated with the estimation.10"
"32","The parametric bootstrap has the desirable property that the confidence intervals shrink as the number of words increases, something which should be true of confidence intervals of estimates from text analysis (Benoit, Laver, and Mikhaylov 2007; Laver, Benoit, and Garry 2003). We have tested this with a Monte Carlo simulation (Appendix B). First, true parameter values for the party positions were fixed, and the remaining parameter values were drawn from random distributions. Second, simulated word frequencies were generated by taking random draws from a Poisson distribution using the true parameter values to calculate λijt. Finally, the simulation generated confidence intervals from 100 bootstraps. We repeated this procedure, each time increasing the number of unique words being used in the estimation, starting with 25 words and ending with 10,000 words. Because we only increase the number of unique words in this procedure while holding party positions fixed, only the error surrounding these estimates should vary. The simulation demonstrates that the average confidence interval for party positions decreases substantially as texts get longer. The average 95% confidence interval is almost six times larger for 25 unique words than for 500 unique words, and the interval is still 2.5 times larger for 500 words compared with 5,000 unique words. The reason for this decrease is that the model treats each unique word as an independent observation. More words mean more data for estimating party positions, and hence smaller confidence intervals.            "
"33","We have tested several alternatives to this method for producing confidence intervals, but believe the parametric bootstrap provides a good compromise between all of these approaches. The first alternative to our method would involve a nonparametric bootstrap. This approach would sample words from each text with replacement to generate new manifestos. In simulations, we have found this problematic for text data. The simulated manifesto data do not correspond on average to actual manifesto word counts. Infrequent words in the manifesto rarely appear in the simulated data, leading to confidence intervals that do not encompass the ML position estimate. As a second alternative, after obtaining the ML estimates, one could numerically calculate a Hessian matrix, take the negative inverse of this matrix to obtain a variance/co‐variance matrix for the entire parameter space, and take draws from a multivariate normal distribution to obtain simulated parameter values. However, given the number of parameters typically being estimated in our model, computational obstacles make it impossible to calculate such a large variance/covariance matrix. Third, rather than using a Poisson model, one could revert to a negative binomial model with an overdispersion parameter. Because we use a parametric bootstrap, the confidence intervals we generate are sensitive to our distributional assumptions. Wrong distributional assumptions will generate poor simulated data and lead to invalid estimates of uncertainty. King notes, for example, that the Poisson model will produce biased standard errors in the presence of over‐ or underdispersion (King 1998, 128). Simulations reveal, however, that confidence intervals produced using the negative binomial model only increase slightly compared with the Poisson model, while the computational effort to generate them vastly increases. This leaves us with the Poisson model using a parametric bootstrap as the most feasible method to obtain confidence intervals.            "
"34","To implement the routine, we have written a computer program Wordfish for the R statistical language.11 As input, the program requires a word frequencies matrix.12 The code then takes the word frequency dataset, generates starting values, and runs the algorithm. It outputs the party positions along with the word weights and party and word fixed‐effects. In addition, the program can generate confidence intervals from a parametric bootstrap.13"
"35","Like all statistical models, Wordfish makes several assumptions which researchers should keep in mind when using the method. To estimate positions over time, the model assumes—like users of Wordscores do—that word meanings remain stable. An alternative estimation strategy would hold only a subset of word weights fixed, while allowing the remaining words to have different weights in different time periods. Such an approach would naturally come at the cost of making the model more time consuming to estimate. In addition, it would require subjective judgments on the part of the researcher as to which word parameters to allow to vary and which ones to hold fixed. Researchers would have to state a priori which words' meanings have changed over time and which have not. Because of the inherent difficulty of this task, we opt to assume that all word parameters are fixed over time. Moreover, it is not possible to allow all word parameters to vary across time because the model would be unidentified. To identify the model, we would have to hold party positions fixed, and, given we are interested in party movement over time, this would make little sense. However, we do believe that our approach has an advantage in estimating time‐series positions because it uses words from all documents. If the political lexicon changes through words entering and exiting the political dialogue, rather than through words changing meaning, our method does take these changes into account when estimating positions.            "
"36","With regard to dimensionality, Wordfish assumes the principle dimension extracted from texts captures the political content of those texts. In other words, if researchers want estimates of party positions regarding foreign policy, they should run the program on documents containing information about foreign policy only. Such a decision is nontrivial. It means that a researcher must carefully read the manifesto to be able to divide it into issue areas, or policy dimensions. Naturally, this requires the knowledge of the document language. Different researchers may make different decisions about which parts of the manifesto refer to economic policy. This leads to an additional source of error which we do not take into account here. If the researcher is not concerned about specific dimensions and is confident the texts under investigation represent the totality of the authors' policy positions, he or she can confidently extract a left‐right dimension.            "
"37","Therefore, when analyzing more than one dimension, we recommend that researchers first define the dimensions ex ante and, second, use only documents that contain information relevant to that dimension. Defining the dimension includes being transparent about what information is being used. For example, a researcher might define a foreign policy dimension as including texts on security, defense, and the United Nations. Others might disagree with this definition and develop a different one. However, only documents which deal with the dimension and issue of interest should be compared. In practice, parties divide manifestos into issue areas themselves to make them more readable and accessible to party members and the electorate. This facilitates the task of defining policy dimensions. In addition, Wordfish gives researchers the ability to analyze the degree to which the estimates capture the dimension under investigation by estimating the word‐discrimination parameters. For example, words related to foreign policy should presumably receive a great deal of weight when examining foreign policy texts. If they do not, the researcher may want to consider reexamining the source documents.            "
"38","We apply this new technique to estimate the positions of German parties in the postreunification era (1990–2005).14 The estimation requires three steps: defining policy dimensions, generating the word frequency dataset, and running the algorithm. We perform two analyses: a left‐right dimensional analysis using the entire manifesto of each party in each election, and a multidimensional analysis using particular sections of each manifesto (economic, societal, and foreign policies).         "
"39","Our first analysis uses the entire manifesto text, and we expect our results to capture a basic left‐right dimension of German politics. In the second analysis, we calculate positions for individual dimensions of interest. Here, we concentrate our analysis on economic, societal, and foreign policies.15 Each manifesto text is thus divided into three separate files. We then run our algorithm on each dimension separately and retrieve three positions for each party.16"
"40","We follow a scheme applied to German manifestos by König, Blume, and Luig (2003) to divide up the manifestos into policy‐specific sections.17 The economic dimension captures socioeconomic policies including taxes, revenues, and spending. The foreign dimension covers international political and economic affairs as well as relations with the European Union. Finally, the societal dimension includes diverse areas such as law and order, gender equality, higher education, immigration, housing, and sport. Once the dimensions are defined and the manifesto texts are compiled, we generate a word frequency dataset. The rows of this matrix correspond to a party manifesto from a particular election and the columns to all unique words mentioned in the texts. This means that we have 25 rows (five parties, five elections) and several thousand columns depending on the number of unique words for each dimension. While it is possible to estimate positions using the entire party‐word matrix, we remove words that parties use infrequently and thus contain little information about their placement. We include a word in the estimation if it was mentioned at least once on average by each party during the period between 1990 and 2005. This has three practical advantages. First, it speeds up the estimation process by eliminating the “long tail” in our dataset. Second, it ensures that our estimation results do not hinge on these infrequently mentioned words. Lastly, it eliminates the possibility that spelling mistakes or other minor and infrequent errors affect our estimates.18"
"41"," Figure 1a plots the party position estimates (ω) for the main left‐right dimension.19 The estimates reflect several important changes in the party system over time. Since reunification, the former East German communist Party of Democratic Socialism (PDS) has occupied the left end of the political spectrum. The Greens start out on the left in 1990, but move slightly towards the political center up until the most recent election in 2005. This movement reflects the transformation of the Greens from an environmentalist fringe party in the 1980s to a mainstream governing party by 1998. Most importantly, our estimates pick up the significant right shift of the Social Democratic Party (SPD) throughout the 1990s. This matches conventional wisdom that Chancellor Gerhard Schröder moved the traditional left‐wing socialist party to the political center to recapture government in same way that Tony Blair moved the British Labour Party to the center with his “Third Way.” In addition, we see a left shift by both the SPD and the PDS in 2005. This may be explained by a split in the SPD. The left wing of the SPD, led by former party leader Oskar Lafontaine, was upset by the party's rightward movement under Schröder and split off to form a new party together with the PDS, Die Linke. The SPD needed to move left to placate their base and to avoid losing even more party members to Die Linke. Finally, the liberal Free Democrats (FDP) and the conservative Christian Democrats (CDU‐CSU) are further to the right and remain relatively stable over time. The FDP tends to be slightly to the right of the CDU‐CSU up until 2005, when it moves to the center. The confidence intervals, reported in the appendix, reveal that we can distinguish between parties in all elections except between the Greens and PDS in 1990 and between the CDU‐CSU and FDP in 2005. We also find a statistically significant time trend for all parties. Nevertheless, there are several instances in which we cannot statistically distinguish between a party's position and its position in the previous election.            "
"42","                 Estimated Party Positions in Germany, 1990–2005                         "
"43"," Figures 1b through 1d plot our party estimates for the economic, societal, and foreign dimensions. On the economic dimension, our analysis confirms that the liberal FDP is clearly the most conservative party, demanding lower taxes and less public spending. This is reflected by the large gap between this party and the CDU‐CSU. The two largest German parties (SPD and CDU‐CSU) are closest to each other in 2002 and 2005. Following the 2005 election, the two parties formed a grand coalition government. In general, all party positions remain relatively stable over time on this dimension.            "
"44","The societal dimension captures a wide range of policies, including immigration, education, and environment. The most significant finding for this dimension is that all parties except the Greens move to the left in 2005. In the context of German electoral politics, this was the year when the SPD chancellor decided to hold early elections because some of his own party members had switched over to the PDS. The FDP is still to the right of all parties. This party is often thought to be located between the SPD and the CDU‐CSU on social policies. However, the dimension includes more than just social policies, making it difficult to compare this dimension to other estimates of social policy positions.            "
"45","On foreign policy, a similar ranking of the parties emerges. The Greens, which emerged from an antiwar, pro‐environmental social movement, and the PDS are located closely to each other during the first half of the 1990s. Once the Greens enter government in 1998, their policy positions shifts slightly towards the center. The SPD makes its most significant ideological shift throughout the 1990s, when it moves from a leftist position towards a centrist position on foreign policy. Again, this change is likely to be associated with the SPD taking over government responsibility in 1998. The CDU‐CSU and the FDP have similar positions. In 1990 and 2005, the FDP is more centrist and located between the two major parties.            "
"46","A comparison of the size of the confidence intervals reveals that positions estimated from fewer words have larger intervals. For example, the average confidence interval for the economic policy dimension (4,714 words) is 54% larger than the average confidence interval for the left‐right dimension (8,995 words). These results confirm the Monte Carlo simulation that more words reduce the uncertainty surrounding the estimates."
"47","To further confirm our findings, we check the validity of our results both internally and externally. For internal validiation, we examine the word parameters. We expect to find a particular pattern in the results. Frequent words (e.g., conjunctions, articles, prepositions, etc.) should not discriminate between party manifestos because they do not contain any political meaning. Therefore, they should have large fixed effects associated with weights close to zero. In contrast, as words are mentioned more infrequently, they are more likely to be part of politically relevant language and discriminate between the parties. These words should therefore have smaller fixed effects associated with either positive or negative weights, depending on whether the words place parties on the left or on the right."
"48"," Figure 2 plots the estimated word fixed effects against the word weights. The scatterplot confirms our expectations and takes the shape of an “Eiffel Tower of words.” Words with a high fixed effect have zero weight, but words with low fixed effects have either negative or positive weight. The graph also highlights some words as examples. Most importantly, words with large weights have a politically relevant connotation. Manifestos on the left mention words like “fascism,”“professional ban,”“male violence,”“emancipation,” and “pornography” more often than the ones placed on the right. The largest weight on the left is for the word “BRD,” the abbreviation for Federal Republic of Germany, a word that is used primarily by one party, the PDS. While this may appear rather trivial, in the German political context of reunification it is, in fact, an interesting result. It is well known that the official doctrine of the former communist party of East Germany (SED), the predecessor to the PDS, was to refer to West Germany in its abbreviated form in order to demonstrate its rejection of West Germany's claim for sole right of representation. However, the official position of West German governments was to use the full constitutional name (Stevenson 2002, 50). This pattern seems to continue after reunification. On the right, parties use words such as “income taxation,”“nonwage labor costs,” and “education vouchers” more often. The highest weight on this side is for the word “general welfare payments,” related to a long‐standing proposal by the liberal democratic FDP to bundle up all welfare payments and pay them out to eligible citizens in one lump‐sum payment.            "
"49","                 Word Weights vs. Word Fixed Effects. Left‐Right Dimension, Germany 1990–2005 (Translations given in text)                         "
"50","Finally, words with large fixed effects do not have discriminating value. The plotted words “entry into force,”“protects,”“safe,”“they/she,”“the,” and finally the word “and” with the largest fixed effect do not contain much politically relevant information. Their associated weight is close to zero."
"51"," Table 1 completes the word analysis for all dimensions and reports the top 10 words placing parties on the left and the right. For instance, in addition to the words shown already in the figure, parties on the left use “womens' movement” and “stratosphere” much more often, whereas parties on the right talk more about “business location” and “mobility.”            "
"52","On the economic dimension, words such as “workers' participation,”“quota,”“mobility,” and “negotiated wages” matter most. All of these are words associated with economic and labor policy. Likewise, on the societal dimension we find references to “process of reunification,”“university graduates,”“sexuality,” and “climate catastrophe.” With words as diverse as these, the results reinforce our belief that this is a category capturing societal politics broadly defined. Lastly, words such as “unilateral,”“NGOs,”“weapons production,” and “armies” all clearly refer to the foreign and defense policy domain. In addition, right parties often refer to the European defense and security policy (EDSP), the European police agency (Europol) and to the EU budget. In sum, the fact that the weights are largest for words carrying political meaning demonstrates that our model is capturing the policy space."
"53","Next, we cross‐validate our results with existing methods (hand coding of manifestos, expert surveys, and Wordscores). First, we compare our results with the Comparative Manifestos Project left‐right scale and three policy scales for Germany, 1990–98 (Budge et al. 2001). The CMP data constitute the only comparable time‐series dataset. The three policy scales are market economy (MARKECO), welfare state (WELFARE), and international peace (INTPEACE). We assume that these correspond to our economic, societal, and foreign dimensions. Second, we use expert survey estimates from Benoit and Laver (2006b) on a left‐right dimension and on a taxes versus spending dimension for 2002–2003. Finally, we compare our estimates to Wordscores estimates on an economic and social dimension from Laver, Benoit, and Garry (2003) for 1990 and 1994 and from Proksch and Slapin (2006) for 2005.            "
"54"," Table 2 presents the correlations between our and other position estimates. The correlations between our Poisson scaling model and the other three methods is high, suggesting that the techniques provide similar placement of parties in the political space. Unlike what Monroe and Maeda (2004) find in U.S. Congressional speeches, this indicates that the dimension we estimate is political and not solely linguistic. Almost all coefficients range between 0.8 and .98. Only our broad societal category corresponds less well to social and welfare categories of the other measures.            "
"55","As an additional cross‐validation, Figure 3 directly compares our left‐right dimension with the Comparative Manifestos Project left‐right scale for the years 1990–98. The CMP data suggest major changes in the party system that are inconsistent with standard accounts of German politics. First, it locates the conservative CDU‐CSU closer to the Greens than to any other party in 1990, including its governing partner the FDP. Second, it suggests that the social‐democratic SPD shoots from being next to the former communists to the position of the free‐market Free Democrats, crossing the position of the Green party. It is inconceivable that a major centrist party in an established multiparty system would make such a jump. Moreover, expert survey data do not find that the SPD is to the left of the Greens in 1990 (Huber and Inglehart 1995). In contrast, our method provides less extreme party movements in the 1990s, eliminating the unlikely crossovers suggested by the CMP data. We find that the SPD makes a more modest move relative to the other parties, remaining in the center of the space throughout the period. Our estimates furthermore match the rankings of the parties from the Huber and Inglehart expert survey data. In general, our findings for the German party system correspond well with other methods for estimating party positions. When used as time‐series data, our estimates substantially improve upon previous estimates by providing smoother party movements than those found in the CMP data.            "
"56","                 Comparison of Left‐Right Positions in Germany, 1990–98                         "
"57","While the analysis of the word weights, together with the method's high correlation to other estimates of party positions, indicate that we are capturing a primary left‐right dimension in German politics, questions may remain about how robust this technique is to the texts we chose and the model specification we use. Here we demonstrate that our technique is robust to the selection of texts and our assumption about the underlying statistical distribution of word counts."
"58","The model specification means that adding or subtracting elections or parties may affect the positions of all remaining parties. To test the extent to which our results hinge on the elections and parties we include in the analysis, we rerun the results dropping single manifestos (one party in one election year), an entire party, and an entire election year. In all cases we get results which correlate very highly with our original estimates of party positions. Our lowest correlation with the original party positions estimates occurs when we drop an entire party, the FDP. When we do this, our results correlate with the remaining original estimates at 0.94. When we drop the entire election year 2005, the remaining party positions correlate with the original positions at 0.99. Likewise, we correlate very highly (r = 0.99) with our original results when we drop individual manifestos (the CDU‐CSU's 1990 manifesto and the FDP's 2005 manifesto). This would suggest that even if researchers are unable to obtain all party manifestos, they can still use our method and have a high degree of confidence in their results."
"59","In addition, we examine how well our results hold when we alter our assumption about the underlying statistical distribution of word counts. Although, from the standpoint of estimation, the Poisson distribution has the nice feature that its mean equals its variance, this assumption is unlikely to hold for word‐count data (Jansche 2003; Mosteller and Wallace 1964). Therefore, we also estimate our model using a negative binomial distribution with a separate overdispersion parameter for each manifesto.20 The additional parameters vastly increase the computation time, in particular when running the parametric bootstrap. The results again correlate very highly with our original party position estimates using the Poisson distribution (r= 0.97). The only major difference between the negative binomial estimation and the Poisson estimation is that using the negative binomial estimation we find that the liberal FDP is located just to the left of the CDU, in between the CDU and the SPD, while the FDP was the most right‐wing party all years except 2005 using the Poisson distribution.         "
"60","Finally, we estimate our results using the simplest distribution possible, a log‐normal distribution. Here, we simply regress party and word parameters on logged word counts. This also gives us virtually identical results, correlating with our Poisson estimates at 0.94. Moreover, using the log‐normal, we get the same party ordering that we had in the Poisson model. In both the log‐normal and negative binomial models, all the party trends remain the same, with SPD and the Greens moving to the center of the political space as they enter government."
"61","Comparative politics research requires accurate time‐series estimates of party positions. Surprisingly, there is currently no easy‐to‐implement method that provides valid time‐series positions along with measures of their uncertainty. We have presented a methodology which aims to fill this gap. We assume an underlying word frequency distribution in political text and use an EM algorithm to estimate party parameters (positions and fixed effects) as well as word parameters (weights and fixed effects). Our approach adds to existing methods by providing a computer‐based text analysis program, Wordfish, which does not require the use of reference texts. Like the Comparative Manifestos Project, our method can create rich time‐series data, but does not require teams of potentially error‐prone hand coders to do so. At the same time, like Wordscores, we provide easy‐to‐implement computer code which researchers can apply to virtually any set of political texts. Our method only requires party manifestos of those parties whose positions are to be estimated.         "
"62","We have demonstrated that our approach produces estimates of party positions which correspond well with positions from other estimation techniques. We are able to accurately portray the German party system in the 1990–2005 postreunification era. Our estimated positions correlate highly with other methods. However, our approach is much less cost and time intensive, it is easily replicable, and it produces a more accurate time series with uncertainty estimates. In addition, the results for word parameters suggest that the technique captures a political, rather than linguistic, dimension."
"63","Nevertheless, when deciding how to estimate party positions, researchers should carefully assess our set of assumptions compared with those of other computer content analysis programs. First, our method does require analysts to assume word meanings do not vary over time; however, if words enter and exit the political lexicon our approach will still capture their relative importance. Other computer content analysis methods, such as the Bayesian approach taken by Monroe and Maeda (2004), make the same assumptions about word meanings as we do, but are significantly more complicated to implement. Wordscores requires the additional assumptions that all words of interest are contained in the reference texts specified, and these texts represent the extremes over time. Second, researchers must decide whether they prefer to set dimensions ex ante or interpret them ex post. If researchers prefer the latter, only the Bayesian approach of Monroe and Maeda (2004) is currently able to extract more than one dimension from texts. Wordscores requires researchers to identify new reference values and then to assume that both their reference texts and documents of interest contain sufficient information about their dimension of interest to produce meaningful results. We suggest analysts use only documents specifically pertaining to their dimension of interest. This requires that analysts carefully select the texts they are using as data and be familiar with their content. However, we hope that all researchers using computer content analysis do this regardless of the methodology they employ. Lastly, researchers may want to consider speed and ease of use when selecting a methodology. When estimating positions at a single point in time for which it is fairly easy to assess the political extremes and identify appropriate reference texts, Wordscores provides the fastest and easiest method for obtaining valid and replicable positions. In situations such as estimating a time series, for which identifying appropriate references texts is difficult or perhaps impossible, our technique provides researchers a straightforward and relatively fast technique for avoiding many of the assumptions necessary when using Wordscores. Finally, researchers may prefer our technique even when estimating positions at a single point in time because we are able to estimate the importance of words for discriminating between texts, thus avoiding the rescaling problem inherent in Wordscores.         "
"64","There are certain limitations associated with our current model specification and the algorithm which open up a research agenda and which should be addressed in future work. First, although our results appear relatively robust to dropping texts (e.g., removing a party, an election, and an individual manifesto), our algorithm is sensitive to the overall number of texts used. Because each word parameter is estimated using all manifestos, the data must include a sufficient number of manifestos to avoid a small‐N problem in the estimates. Second, we have shown that our results seem robust to different distributional assumptions. Nevertheless, future work should examine in more detail the consequences of choosing one distribution over another, paying specific attention to the consequences for uncertainty estimates. Third, our current model assumes that the political lexicon remains similar over time. This is because word parameters are estimated in a time‐insensitive manner in order to identify the statistical model allowing all party positions to move. Future versions of the model could relax this assumption for longer time periods and allow weights for a subset of words to vary over time, although researchers would have to make judgments about which words to allow to vary. Finally, we have opted to extract a single dimension over time and our results suggest that it is policy relevant. It would also be possible to rewrite the model to extract more than one dimension. However, we believe that in comparative politics research scholars may prefer policy dimensions whose meaning is set ex ante rather than interpreted ex post."
"65","This set of questions opens up exciting new avenues for research on party positions and ideology estimated from political texts, which is reflected by an increasing number of studies that combine quantitative linguistic analysis with the study of political ideology. Our method takes this approach to examine party ideology over time. The results provide new insights into the German postreunification party system and its political lexicon."
"66","                B1                              [                                       Simulation: Text Length and Uncertainty Estimates ]            "
