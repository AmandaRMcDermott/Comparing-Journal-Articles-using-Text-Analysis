"","x"
"1","The first condition we explore is the idea that the aggregate data need be “informative” concerning the underlying microlevel data. As Robinson (1950) has shown, there is no clear or direct relationship between data that are observed at different levels of aggregation. Indeed, this conundrum has puzzled scholars for decades (Gehlke and Biehl 1934). Nonetheless, some aggregate data are more informative about the microdata than others. In this section, we focus on what it means for aggregate data to be “informative,” and what consequences arise when data are not very informative, but one proceeds with estimation just the same. We then discuss how aggregation from the individual level can introduce troublesome biases. Lastly, we describe the role of microtheories in the analysis of macrodata. Without loss of generality, our discussion is couched in a framework where the macrolevel data are election districts and the microlevel counterparts are the individual‐level voting data.         "
"2","One way to gauge the level of information contained in aggregate data is to consider all of the deterministic information contained therein. Consider a simple problem in split‐ticket voting like the one shown in Table 1. Data for each district can be summarized by such a table. The available data include the values T, the Democratic proportion of the House vote, (1 −T), the Republican proportion of the House vote, Xb, the Democratic proportion of the Presidential vote, and Xw= 1 −Xb, the Republican proportion of the Presidential vote.1 What we do not have, but may be interested in estimating, are the proportions of split‐ticket votes: βw, the proportion of all those who voted for the Republican presidential candidate who also voted for the Democratic House candidate; and 1 −βb, the proportion of those who voted for the Democratic presidential candidate who also voted for the Republican House candidate.2 Since vote shares necessarily fall between 0 and 1, the unknown parameters βb and βw fully characterize the table. The extent to which these parameters are further bounded within the unit square is the deterministic aspect of an aggregate data set.            "
"3","Suppose that the district shown in Table 1 had 100 voters, and that the Democratic vote totals for president and House were 60 and 30, respectively. In that case, there cannot have been more than 30 voters who supported both Democrats, and βb cannot exceed . Likewise, βw has an upper bound of , while both parameters have a lower bound of 0. In this way, combinations of marginal totals may exclude some values for each parameter, for each district. Note that in distributing the 30 Democratic House votes between the Democratic and Republican presidential voters, we simultaneously determine both βb and βw, since the parameters are dependent. If βb= 0.5, then βw is necessarily 0, and so on.3"
"4","By plotting all logically possible pairs of parameter values, one can succinctly summarize the deterministic information for each observation. Since , when one plots the possible values of βw on the y‐axis and the values of βb on the x‐axis, the result is a line with intercept  and slope . This line has been termed a “tomography line,” and there is one for each observation.4 Both parameters are bounded within the [0, 1] interval, but those lines that do not extend across the entire unit square are further bounded, and one may be more successful when estimating the true parameter values for those observations. For estimation problems that can be simplified to 2 × 2 tables, then, a “tomography plot” succinctly displays the scope of the problem.            "
"5","In addition to taking account of the deterministic bounds, one might incorporate some kind of assumption about how districts are related in order to arrive at estimates of plausible mean parameter values for a set of districts, or, sometimes, of parameters for each district. There are thus two diagnostic uses for tomography plots. First, they show all available deterministic information in a problem, and thereby reveal, in an informal sense, how constrained are the parameters, and thus how easy or hard the estimation problem will be. Second, one may examine these plots to assess whether an assumption that the (βb, βw) pairs were drawn from a distribution with a known form seems reasonable for the data at hand. The simplest distributional assumption is the case where all the βwis are equal and all the βbis are equal. In this case, it is easy to determine the values of the common βw and βb. Consider the very simple case with two observations or districts. The subscripts indicate the district.               "
"6","Testing a hypothesis about the distribution from which data were drawn is fairly straightforward when the data are directly observed. In this case, however, we have only a range of possible values for each observation, as mapped out by the lines. A single tomography plot is consistent with many different individual‐level data sets, so many different joint distributions will be consistent with any given set of tomography lines. In that sense, the information gleaned from tomography plots is never more than suggestive and does not allow one to make definitive claims about whether particular distributional assumptions obtain. To say that a tomography plot is “informative” is merely to report that one or two conditions are met. If most of the tomography lines seem to intersect in a region, then it is more likely (but not certain) that the actual individual‐level data are clustered there. In turn, this area marks a plausible location for the mode of the joint distribution of βs. Second, if there are relatively narrow bounds on one or both parameters, one can further limit the possible parameters of this distribution. At best, though, one can conclude that the data are consistent with a unimodal distribution, when there is an area of intersection. On the other hand, if no area of intersection is evident and the bounds are wide, the implication is that the TBVN distributional assumption is not reasonable.6 Whether the distributional assumption seems to hold or not, meanwhile, is important not only for the purposes of estimating means, but also because, at the estimation stage, the computation of the standard errors is based on the distributional assumption. So whether the standard errors are correct or incorrect also depends on whether the distributional assumption is correct or incorrect. This logic is summarized in Table 2.            "
"7","King contends that an “informative” tomography plot can reasonably be assumed to have been generated by a truncated bivariate normal distribution. That is, he would attribute a higher probability that the output from data analysis is summarized by Cell 1 rather than by Cell 2. Similarly, if a tomography plot is uninformative, he claims the data are less likely to have been generated from a TBVN, and the situation is more likely to be summarized by Cell 4 than by Cell 3. There is no particular reason to believe that the diagonal cells in Table 2 are more likely than the off‐diagonal cells. King's contention here amounts to an a priori assumption. Indeed, it would be very hard to make a formal probabilistic argument about this link. Our examples that follow should produce more intuition on what is and is not revealed by a tomography plot. At best, a researcher hopes that the tomography plot will be informative: if it is not, the resulting standard errors may be too large to be useful, or simply incorrect (see King 1997, ch. 16).            "
"8","One's assessment of whether the distributional assumption is correct thus depends on the nature of the tomography plot, though, of course, this assessment is never definitive. Moreover, deciding whether a tomography plot is informative is something of an art, no one has devised a concrete measure for “informativeness” or any formal test for accepting or rejecting the TBVN distributional assumption (or any other distributional assumption) on the basis of the plot."
"9","Consider Figure 1. By the reasoning just discussed, this plot is informative. First, while the bounds on βb span the entire permissible [0, 1] range, the bounds on βw are more narrow, and thus limit the range of possible true values. Second, there is a general area of intersection of tomography lines. If these lines are related (as implied by the distributional assumption in the EI model) then the true points on each line should fall within the area where the lines generally intersect. In this plot, the area of “general intersection” clearly falls at approximately (βb, βw) = (0.65, 0.20). While this point may not represent the true values for βb and βw for all districts, if we have no other information, these values seem to be reasonable first guesses. Of course, not all tomography plots are as seemingly informative. Sometimes the bounds will not be very informative at all, and, in addition, the tomography lines will not display any sort of commonality. Obviously, in such cases, it is far riskier to force the distributional assumption on the data.            "
"10"," Informative Tomography Plot                         "
"11","Contrast the tomography plot in Figure 1 with the tomography plot on the left in Figure 2, where we have very little deterministic information about the underlying data. No “general area of intersection” seems to be present, and the bounds on both parameters are very wide. If one insists on proceeding with EI, it will impose a truncated bivariate normal distribution and then produce estimates for the β parameters (overall means and one per district) accordingly. The mode of the assumed TBVN, indicated by the small square in the plot on the right in Figure 2, is the estimate for the mean βb and βw values for these data. However, “if the ultimate conditional distributions are not reasonably close approximations to the truth, incorrect inferences may result” (King 1997, 185). Here, the tomography plot has not given us a good indication that the distributional assumption is correct—quite the contrary.7"
"12"," Uninformative Tomography Plot                         "
"13","Moreover, it is perhaps even more important to acknowledge that cross‐level inference is always tenuous. In particular, a plot may appear to be informative even if the underlying data generation process is not at all well approximated by a TBVN. On the other hand, a plot may not appear to be informative even though the true parameters describing behavior do conform well to a TBVN distribution. A variety of possible situations are illustrated in Figure 3. In each plot, the true (βb, βw) pairs for each district are indicated by a point on the tomography line. In the first plot, the parameters are drawn from a TBVN distribution, and the plot is properly informative. In this case, a researcher would likely proceed properly based on this diagnostic. In the second plot, the parameters are not drawn from a TBVN distribution, but the tomography lines nonetheless appear to suggest a mode (i.e., it appears to be an “informative” plot).8 Here, a researcher who proceeded with confidence would be grossly misled, and the analysis would suffer accordingly. In the third plot, the parameters are drawn from a TBVN distribution, but it has relatively large standard deviations on both the βb and βw parameters, and the resulting tomography plot does not appear at all “informative.” On the basis of such a tomography plot, there is no reason to favor the truncated normal distribution as the underlying distribution, even though, in this instance, it happens to be correct. In short, inspecting tomography plots is worthwhile because they illustrate bounds, but researchers must understand that they are not definitive with respect to distributional assumptions.            "
"14"," A Panel of Tomography Plots                         "
"15","A second condition that helps to surmount the huge barriers to making ecological inferences is having data that aggregate without bias. It is usually possible to obtain reasonable estimates of individual‐level parameters given only aggregated data if the aggregated data set contains no aggregation bias. The assumption of no aggregation bias holds if the parameters (βb and βw) are not correlated with the regressors, i.e., the X variable. In this application, that would mean that levels of Democratic President‐Republican Representative and Republican President‐Democratic Representative voting are not correlated with levels of support for the Presidential candidates. In fact, if no aggregation bias exists in the data, simple OLS will provide reasonable, unbiased, and consistent estimates of the overall means of the β parameters (Goodman 1953). EI should perform likewise. So in the very special case wherein data exhibit no aggregation bias, there is no reason to favor EI over OLS.9 Meanwhile, when data are affected by aggregation bias, neither model is trustworthy.10"
"16"," Figure 4 uses tomography plots to illustrate how aggregation bias causes difficulties for ecological inference. All panels show nine districts, each having 100 voters, with βb and βw representing the proportions voting straight Democratic and voting for the Republican presidential candidate and the Democratic House candidate, respectively. In each scenario, these values are known, but we consider how the analyst not knowing them would proceed.         "
"17"," Aggregation Bias and Tomography Plots                      "
"18","If the true voting patterns are represented by the left panel, the tomography plot is clearly misleading. The lines intersect around (0.7, 0.3), but this is not a good estimate for the β pairs, whose actual mean is (0.5, 0.5). The source of the error is aggregation bias: both β parameters are highly positively correlated with X, so that as one moves up and right on the plot, the slopes of the tomography lines (which are entirely determined by X) decrease, causing the misleading region of intersection in the lines.         "
"19","Of course, the difficulty in a real data‐analysis situation, in which one does not know the true β values, is that there are so many possible scatters of (βb, βw) pairs for a given set of tomography lines. In this artificially simple problem, wherein very few districts each have few voters, there are about 2 × 1014 different possible joint distributions of βb and βw arising from the known vote totals.11 Knowing only the aggregates and their matching tomography lines, one cannot distinguish between the situations portrayed in the left, center, and right panels. It is thus very difficult to glean any information about aggregation bias directly from a tomography plot, except in the artificial situation wherein the true β values are known. Furthermore, the contrast between the middle and right panels (analogs to the left and middle panels of Figure 3) demonstrates that even very low aggregation bias does not guarantee that an informative tomography plot will not be misleading. In both cases, the correlations between both β parameters and X are less than 0.10 in absolute value, but only in the center case is the tomography plot correctly informative. In the right panel, the mean of βb is 0.48, and the mean of βw is 0.52 notwithstanding the intersection of lines in the vicinity of (0.7, 0.3). Assuming a TBVN centered there will, of course, result in faulty estimates.         "
"20","To get a better sense for the degree to which aggregation bias foils ecological inference, consider next the results from a Monte Carlo simulation displayed in Figure 5. Here, data were constructed to exhibit aggregation bias but to be consistent with the distributional and spatial autocorrelation assumptions of the EI model.12 In this simulation, 250 data sets were generated exactly according to the description in King (1997, 161). The data were drawn from a TBVN having parameters βb=βw= 0.5, σb= 0.4, σw= 0.1, and ρ= 0.2.13 The true values, βb=βw= 0.5, are marked in the plots by a vertical line. For each simulation, we have drawn a bar centered on the point estimate for the parameter and model in question, extending one estimated standard error to each side. The error bars in Figure 5 clearly indicate that, even accounting for the standard errors, the estimates are inaccurate.14 The sense of precision is overstated more by EI than OLS. On average, EI's estimates for βb are 25 S.E.s from the true value, and its estimates of βw are −14.7 S.E.s from the true value. In the OLS model, meanwhile, βb is 18.8 S.E.s from the true value while βw is −11.4 S.E.s from the true value, on average. Obviously, the standard errors are erroneously estimated and suggest more precision than actually exists. Hence, even if the data are consistent with the other assumptions, if the parameters are correlated with the regressors, neither OLS nor EI will yield accurate results. Neither model is robust against aggregation bias.         "
"21"," Consequence of Aggregation Bias                      "
"22","The question of whether we are able to make reasonable ecological inferences turns on the issue of aggregation bias (Cho 1998). Though King claims that his method is “robust” to violations of the aggregation bias assumption, the evidence strongly suggests otherwise. King's claim originates in (and holds only for) an unorthodox definition of “robustness.” He contends that EI is robust because it will never produce estimates of the β parameters which are outside the [0, 1] bounds. But estimates constrained to respect bounds need not be close to the truth, or even within a few standard errors of the actual values. Indeed, King's model does not produce unbiased or consistent estimates in the traditional statistical sense of those words when aggregation bias is present. When regressors are correlated with parameters, the estimates from EI are not equal to their respective population parameters, in expectation, and the discrepancy between the estimates and the true values does not converge in distribution to zero as the sample of data points becomes large (Cho 1998). Moreover, there is also evidence that the estimation of the standard errors is inaccurate as well. A fundamental issue for the split‐ticket voting analyst, then, is whether aggregation bias exists in the election‐returns data set. If rates of ticket splitting vary systematically according to how competitive the district was in the presidential race, then estimates from the King model (without covariates) will be untrustworthy.15"
"23","There is, no doubt, considerable variation in the precission of theory informing data analyses in the social sciences. Works aiming to test exact predictions from fully specified formal models are surely in the minority, and purely inductive exercises in which the authors cast about for relationships among a large number of variables that merely seem likely to be connected are not rare. We hesitate to take a strong position on the strict necessity of strong theory in all instances. However, in the case of ecological inference, we begin with the knowledge that aggregation can easily obscure data generating processes and microlevel mechanisms. Robinson's seminal work on ecological inference (1950) broached the highly memorable example of literacy and nonnativity. Upon discovering that states and regions with more foreign‐born residents are, on average, more literate, one could infer that immigrants to America tended to be highly fluent in English. Even a casual acquaintance with American history, however, would suggest an alternative logic: immigrants tended to congregate in areas whose native‐born populations were comparatively educated and literate. With the aggregate‐level finding of a positive correlation in hand, one could work backwards to those rival accounts (and others), and not be in a position to choose one over the other on purely statistical grounds. With additional aggregated data, one could test their plausibility. But in the absence of additional data, it is prior knowledge and the credibility of the rival microlevel accounts that direct us to favor one account over the other. Our point, then, is less the purist's stance that theory must always precede empirical analysis than the common‐sense argument that ecological inferences ought to be accompanied by an explicit microlevel theory."
"24","To anticipate our arguments about voting, an analysis of ticket‐splitting, or of transition probabilities from an earlier election to a later election, ought to be sensitive to the wealth of knowledge accrued about voting behavior and candidate strategy. Since it will always be the case that many alternative microlevel data generating processes could produce the same pattern of aggregate results, unambiguous ecological inferences are rare indeed, and strong claims of adjudication between rival models need to be very explicit about microlevel mechanisms. Furthermore, tests have to be constructed around the micrologic, with due attention to both (or all) rivals."
"25","It is rarely ever simple to move from a microtheory to a macrolevel analysis, or from macro data analysis to a microlevel inference. Achen and Shively (1995) provide numerous examples where intuition goes wrong, and they demonstrate mathematically how relationships vanish or reverse in the process of aggregation. They describe proper macrolevel specification as “a subject with no simple relation to microlevel setups where our theories and intuitions apply” and assert that “macromodels will in general confound conventional statistical procedures” (1995, 95). Clearly, the upshot is that models that provide a good fit to the aggregate data may not provide an accurate portrayal of the underlying individual‐level behavior. Indeed, this is the ecological fallacy—that what appears to be the case among macrounits may be vastly misleading with regard to the microunits. Explicit attention to microlevel theories, then, is important and should inform any analysis of aggregated data precisely because “reverse engineering” proves so vexing.         "
"26","We now turn from this more theoretical discussion about the conditions under which ecological inference can be reasonable to an application of ecological inference techniques to split‐ticket voting. In particular, we examine Burden and Kimball's (1998) analysis of ticket splitting at the Congressional district level. Their analysis of estimates based on King's (1997) EI method leads them to conclude that, contrary to previous findings (e.g., Alesina and Rosenthal 1995; Fiorina 1996), “voters are not intentionally splitting their tickets to produce divided government and moderate politics” (Burden and Kimball 1998, 533). Instead, they claim, ticket splitting is primarily the result of lopsided congressional campaigns in which well‐funded, high‐quality incumbents tend to run against unknown, underfunded challengers.         "
"27","Our examination will show that there are two main reasons to doubt the generality and veracity of their conclusions. First, the EI model is not well‐suited to these data. Neither of the previously discussed necessary conditions is met: these data are neither informative nor immune to aggregation bias. Second, their test is not informed by serious consideration of microlevel theories. There are, as well, a number of less fundamental difficulties with their analysis including an oversimplification of the full ticket splitting problem, an imperfect data set,16 and use of a buggy EI software program.17"
"28","Our tactic is to revisit Burden and Kimball's analysis, highlighting the critical decisions at each stage, and focusing on how an ideal treatment of the split‐ticket voting problem would differ. Our primary goal is to demonstrate that deriving insight into why individuals split their ballots by examining only aggregate data is far more difficult than their article implies. Because our main point is to focus on the extreme difficulty in deriving estimates of individual behavior using only aggregate data, we do not provide a revised, competing analysis of district‐level split‐ticket voting estimates. Instead, we precisely identify the major barriers to such an analysis and explicate what remains to be done before accurate estimates can be produced. Accordingly, this article endeavors to delineate the conditions under which the EI model is an appropriate analytical tool."
"29","Burden and Kimball do not tackle American voting in all its complexity, but, rather, examine only two ticket‐splitting scenarios. First, they analyze Presidential and House votes and then, separately, Presidential and Senate votes, ignoring House‐Senate splits and all other races on the ballot. Thus, they consider not the very high dimensional problem of all varieties of ticket‐splitting on complete ballots, but only two sets of two‐way tables. Since EI is designed for 2 × 2 problems (i.e., it assumes dichotomous categorical variables), they further simplify the analysis by discarding all votes not cast for one of the two major parties and by assuming (falsely, as they recognize) that there are no ballots featuring choices in congressional contests but not choices in the presidential contest.18 They thereby reduce the size of the table describing each House district to 2 × 3. Table 3 shows the full House‐President case for Vermont, with each table entry, vij, representing a count of votes cast for presidential candidate i and House candidate j. Table 4 shows the simplified version analyzed in two steps by Burden and Kimball.19"
"30","In Table 1, we assumed a simple problem in which all voters had made choices for both Representative and President. Table 4, by contrast, acknowledges abstention from U.S. House voting. Burden and Kimball's Table A–1 is a general form of our Table 4 (except that it transforms the vote frequencies into row proportions). To submit data in this form to EI, Burden and Kimball summed across the first two columns to create another 2 × 2, with House‐Vote and No‐House‐Vote for columns, and then estimated the Democratic and Republican presidential vote shares for only those voters who did not abstain in the House election. Treating these estimated quantities as known then reduces the 2 × 3 problem to a 2 × 2 table where the cell entries are now the quantities of interest, rates of straight and split‐ticket voting.            "
"31","Burden and Kimball's analysis of split‐ticket voting in 1988 thus proceeded through multiple stages: (1) they estimated abstention (with EI); (2) they estimated ticket‐splitting rates, conditional on the abstention estimates (again with EI); and, (3) they modeled these district‐level estimates of split‐ticket voting as a function of candidate, institutional, and constituency traits (with OLS). Specification issues arise at every stage of the analysis, and, naturally, the accuracy and validity of each stage depend strongly on the accuracy and validity of the preceding stages. Since the errors from each of the stages compound, the final results are highly prone to indeterminacy. Burden and Kimball make no attempt to incorporate uncertainty from any previous stage of their analysis into the proceeding stages. Instead, at each estimation stage, they begin with a “clean slate,” assuming that estimation from previous stages is without error. Even if the estimation at each stage were valid, their multistage estimation procedure poses serious problems.20"
"32","Each stage of their estimation beginning with the conceptualization of the problem, however, has difficulties. We do not examine the last stage of their estimation closely, but note that Herron and Shotts (2003, 2004) and McCue (2001) have scrutinized the validity of using point estimates generated by EI as dependent variables in a second‐stage linear regression, the exact process by which Burden and Kimball arrived at their final estimation. The analysis by Herron and Shotts shows that this process may yield inconsistent and attenuated estimates, but worse, these estimates may suffer from sign reversal and augmentation bias. Clearly, these problems seriously affect the ability to make valid or accurate inferences. Remarkably, Herron and Shotts arrived at these conclusions while assuming that all of the assumptions of EI hold.            "
"33","We heed the admonitions of this analysis, but focus on the earlier stages of the Burden and Kimball analysis. Indeed, even if the final stage of their analysis were valid, the preceding stages cast overwhelming doubt in and of themselves. Every stage of their analysis is plagued with problems. For conciseness, hereafter, we focus primarily on their stage‐two analysis, wherein they produce estimates of district‐level split‐ticket voting rates. Although the first‐stage analysis is less substantively interesting, the problems with EI at the second stage clearly apply to the first‐stage analysis as well.            "
"34","Given the complexity of the American ballot, a full analysis of ticket‐splitting is a huge job. For the sake of tractability, Burden and Kimball make some small simplifications (disregarding minor party candidates), some bigger simplifications (disregarding abstention from the presidential contest), some substantial simplifications (examining only two contests at a time), and a very strong and unambiguously incorrect assumption that estimation errors produced at each stage of their analysis could be safely ignored thereafter. While these choices made the problem manageable, they also greatly limit the applicability and validity of the eventual substantive conclusions about who splits tickets and why.            "
"35","Bearing these limitations in mind, do the data on congressional and presidential voting in 1988 nonetheless reveal interesting or novel information about ticket splitting? Following King's own advice, one should begin an ecological inference analysis by assessing how much information is deterministically available in the aggregate data (King 1997, 277–91). Although the authors do not report any diagnostics on the data, and despite the inherent indeterminacy previously discussed, it is a useful first step of analysis, to which we now turn our attention.            "
"36","Consider Figure 6, which displays a tomography plot from the second stage of Burden and Kimball's analysis of the House data set. Recall that for this stage of their analysis, βb and βw represent proportion of the Dukakis vote and proportion of the Bush vote (respectively) cast for the Democratic House candidate. This plot resembles the one in Figure 2 in that both are very uninformative. (In fact, the lines in Figure 2 are a random draw of the lines in Figure 6.) The only difference, then, is that the Burden‐Kimball tomography plot has more seemingly unrelated lines than the uninformative tomography plot in Figure 2. Again, the bounds are too wide to imply any sort of substantive conclusion. The bounds on βb are [0.28, 0.91]. The bounds on βw are [0.24, 0.75]. Even in this initial stage of assessing the information inherent in the data, it would appear that this split‐ticket voting data set does not contain much information about the parameters of interest: the bounds are not much narrower than [0, 1], and no general area of intersection is evident. Hence, any inferences made from these data are not likely to be very reliable (King 1997, 185). If the standard errors indicate otherwise, they are likely incorrectly computed.21 For the Burden and Kimball data then, there is no reason to expect that EI estimates will be reliable. Even if the truncated bivariate normal distribution is a good approximation of the underlying data‐generating process, the high variance that characterizes the parameters is likely to render their analysis substantively uninteresting.            "
"37"," Tomography Plot for Burden‐Kimball House Vote‐Splitting Data                         "
"38","Note that we have not yet begun to estimate the parameters of interest. At this initial stage, we are merely assessing how much information is available for the EI estimation procedure. Our initial analyses do not portend success in making correct individual‐level inferences based on these aggregate data: the bounds are not informative and no mode is apparent. In some very special situations, when aggregation bias is absent, the method of bounds is truly uninformative yet we are still able to make correct inferences to the individual‐level data. So we turn now to the problem of aggregation bias."
"39","Assessing the degree to which aggregation bias exists is a daunting task, one that is replete with uncertainty. There are, however, some methods that shed some insight into this problem. One method is to examine the aggregation bias diagnostic plot suggested by King (1997, 238). This plot for the data that Burden and Kimball use in their second stage analysis is shown in Figure 7. Aggregation bias exists if there is a relationship between X and βb or βw, where, again, X is the proportion of voters who voted for Dukakis. Since βb and βw are unknown, we are able to plot only the bounds for these two parameters.22 We can see from the figure that the vast majority of the bounds cover the entire permissible range from 0 to 1. In addition, the first plot suggests that the aggregation bias for βb may be severe, since βb and X appear to be strongly correlated. In other words, there is some evidence that rates of straight‐ticket Democratic voting increase with the proportion of the presidential vote won by Dukakis.            "
"40"," Aggregation Bias Diagnostic                         "
"41","A second method for testing whether aggregation bias exists is simply to run the OLS model. If no aggregation bias exists, the assumptions of OLS are met, and so OLS will yield consistent and unbiased estimates. The OLS model for the Burden and Kimball data yields               "
"42","Since OLS produces correct estimates if no aggregation bias exists in the data set, one can conclude from equation (3) that there is a high probability of aggregation bias in the data set.23 Given that EI is not robust to violations of the aggregation bias assumption, and we now have a prior that aggregation bias exists in the data set, the EI estimates are immediately suspect. It is possible that EI will provide reasonable estimates despite the presence of aggregation bias. However, this result would be the exception, not the rule, since EI is a biased and inconsistent estimator in the presence of aggregation bias (Cho 1998; King 1997). The exception might occur when the bounds are informative. Nonetheless, it is clear from Figure 6 that the bounds are far from informative in this 1988 election data set—the vast majority of the bounds span the entire range of possibilities.            "
"43","One method for mitigating the effects of aggregation bias is to include covariates in the model (King 1997, 288). If these covariates control aggregation bias by accounting for the correlation between the parameters and the regressors, then the model will produce the correct estimates. Burden and Kimball included one covariate in their second‐stage model specification, a dummy variable that indicates whether or not a district is located in the South. They included this variable in the belief that individuals who live in the South are unlike individuals who do not live in the South when it comes to decisions about ticket splitting. They offered no indirect evidence (e.g., survey data) to support this contention. Regardless of whether their intuitions about the South are correct or not, including this covariate does not affect the estimates. Their justification for its inclusion was “to account for possible aggregation bias and to improve the estimates” (1998, 536). However, since the two models produce indistinguishable estimates, there is no reason to believe that a South dummy has any desirable effect in mitigating the aggregation bias. If the specification with no covariates is ill‐advised, so too is the specification with only the variable “South.”            "
"44","Burden and Kimball were correct that there is a need to alleviate the aggregation bias in their data set and that incorporating the correct covariates would achieve this end. The problem they encountered is that EI does not provide a test for whether one specification is better than another specification. EI users thus find themselves in a truly problematic situation: they cannot determine which specification is correct, but different specifications can produce very different and irreconcilable results. Indeed, in this way, making ecological inferences is no different than more traditional estimation where we have long known that model specification has important consequences for inference. The ecological inference context takes the challenges inherent in any statistical estimation and compounds it with the problems posed by aggregation."
"45","Consider Table 5, which shows the results from different model specifications (the covariates are from the set Burden and Kimball use in their (third‐stage) OLS analysis of their EI‐estimated split‐ticket voting levels). Most of these covariates are associated with some prior theory or result about ticket splitting. Since these are district and candidate attributes, not aggregates of individual voter traits, they are entering the model at the “right” level. (In the next section, we discuss the most important independent variable in Burden and Kimball's analysis, which is, by contrast, partly an aggregate of individual traits and, thus, subject to distortion by aggregation.) But which ones belong in the model? Burden and Kimball incorporated all of them except the South dummy as independent variables in their OLS stage rather than the EI stages not for any theoretical reason, but because EI treats covariates as incidental, and produces no coefficient estimates for them. Of course, the EI model lacking covariates and the OLS follow‐up are mutually inconsistent. And “putting off” adding covariates until the OLS stage does not ameliorate the serious problem of aggregation bias in the aggregate analysis. Is there any reason to believe that any of these covariates alleviate the aggregation bias?            "
"46","To begin with, in these different specifications, the estimated percentages of ticket splitters vary widely. The values for the standard errors are large for some specifications and extremely small for other specifications. This erratic performance is illustrated in Figure 8. Each rectangle is centered at the point estimate and extends one standard error in each direction. The rectangles would overlap if the models were consistent, but they do not. Even after accounting for the standard error, few of the point estimates are in agreement. Substantively, this is a problem because the alternative specifications imply different types of voting behavior. In addition, the computer program, citing various errors, was not able to compute estimates for certain other specifications. To settle on the best available model of the split‐ticket vote, one must somehow choose from among these different specifications. Finding a proper specification is always a major step, but in the aggregate‐data context, there are enormous barriers (see Achen and Shively 1995, ch. 4, for an extensive discussion; see also Erbring 1990, 264–65, and Haitovsky 1973).            "
"47"," The Effect of Different Covariates                         "
"48","The advice that King offers is that one should include covariates that can “be justified with specific reference to prior substantive knowledge about a problem” (King 1997, 173). He provides no empirical test for choosing covariates, but only this admonition to exercise one's belief about what may be true. This would be unproblematic if different researchers always reached common substantive conclusions after imposing their own beliefs on the model specification. As this congruence virtually never occurs, however, it is obvious that a formal method is needed to determine which covariates are likely to belong in a properly specified model. After all, “including the wrong variables does not help with aggregation bias” (King 1997, 173).            "
"49","Although the problem of identifying proper covariates with formal tests is not solved, and may not even admit a “solution” in the sense of a universally optimal test, there are some starts on this problem. For instance, Tam (1997) notes that the statistical literature on changepoints and parameter constancy addresses an analogous problem and so is very promising as a source for guidance on how to pick covariates in the aggregate data context. The reason to introduce covariates, after all, is because the parameters of interest are not constant throughout the data set. Hence, a useful empirical test should discriminate between covariates that do divide the sample into subgroups in which parameters are nearly constant and those that do not (Cho 2001). In terms of the TBVN distribution that the EI model incorporates, adding covariates into the model would condition the parameters and allow much more flexibility. We may be interested in testing, for instance, whether people who split their tickets are distinguishable by education level from those who vote straight tickets. If so, we should not necessarily try to fit a TBVN distribution with a single mode and set of variance parameters.            "
"50","In a general changepoint problem, a random process generates independent observations indexed by some nonrandom factor, often, but not exclusively, time. One may wish to test whether a change occurred in the random process by searching over partitions that divide the data into subsets appearing to have different distribution functions. Again, the subsets can be sorted chronologically, or can be generated from an ordering of some other measured property. The literature is large and diverse: some tests assume that the number of changepoints is unknown, while others assume a fixed number of changepoints; some fix the variance of the distribution, while others estimate the variance as a parameter; some assume that the different distributions take similar forms, while others allow more flexibility in this regard. Tests vary in nature as well: some are Bayesian (e.g., Carlin, Gelfand, and Smith 1992; Schulze 1982; Smith 1975), some are parametric (e.g., Andrews, Lee, and Ploberger 1996; Ritov 1990), some are nonparametric (e.g., Carlstein 1986; Wolfe and Schechtman 1984), and some are related to time series analysis (e.g.; Brown, Durbin, and Evans 1975). In short, there are diverse means by which one can draw inferences about changepoints and constancy, or lack thereof, of parameters. For present purposes, what is important is that the general object in this literature is to find a means for partitioning data sets into subsets within which there is some degree of parameter constancy. Since this is precisely the goal for the researcher choosing covariates in an ecological inference problem, the application of changepoint tests to aggregate data problems seems extremely promising.            "
"51"," Cho (2001) introduces one formal covariate‐selection test adapted from time‐series analogs. There is not likely to be one covariate‐selection test that is optimal for all aggregate data problems, but employing an empirical test is clearly preferable to imposing subjective beliefs. It is important that aggregate data analysts have some standard by which to judge whether one specification is superior to another. Further development of well‐specified statistical tests for covariate selection should be the priority for aggregate data research.            "
"52","Burden and Kimball were in need of just such a test, since their data exhibited aggregation bias. Lacking any means by which to compare covariates that might alleviate the problem, they settled on one covariate chosen on qualitative grounds. Unfortunately, this covariate did not perform the necessary function of removing aggregation bias, and their analysis suffered accordingly."
"53","Burden and Kimball contend that their research makes two distinct contributions to the study of ticket splitting. First, as pioneers in applying King's EI methods, they purport to provide the first accurate estimates of the extent of ticket splitting. Second, their analysis of splitting (as estimated by EI) reveals that it is primarily an unintentional rather than intentional activity. Americans simultaneously support different parties at a given moment not because they prefer to see power balanced or shared, but because strategic choices by candidates and parties induce splitting. We have already demonstrated that the first of these innovations is more apparent than real—EI certainly does not produce new levels of accuracy in estimating ticket‐splitting behavior. Their data were neither informative nor immune to aggregation bias. We now discuss the second main reason to doubt the generality and veracity of their conclusions, namely that their test was not informed by serious consideration of microlevel theories."
"54","The term “intentional” could be ambiguous in this context, but the authors clarify that their interest lies in assigning primary responsibility for ticket‐splitting to either candidates or voters (Burden and Kimball 1998, 533). If levels of tickets‐splitting seem to respond to candidate traits such as incumbency, spending differentials, or candidate experience, they propose, it is not the case that the masses deliberately divide their support between parties. Thus, they conclude, the candidates, not the voters, move first. It is, of course, already very well known that contemporary American elections feature a substantial incumbency advantage. To verify that some ticket splitting seems to originate in incumbents' skills at drawing cross‐party support, however, is not to rule out that voters are quite consciously spreading support across parties or ideologies. Burden and Kimball did not test whether incumbents are helped or hindered in drawing nonparty‐based support by the expected fates of their parties' presidential candidates. In that respect, they do not give “intentional” ticket‐splitting much chance to surface.         "
"55","The critical result for their claims about balancing and intent, ultimately, is an insignificant coefficient on the variable they label “ideological distance.” They operationalize this variable as the mean distance on a seven‐point ideology scale between Bush (Rp) and the Democratic Senate candidate (Ds), as assigned by a state's Senate Election Study (SES) respondents. There are some problems with this construction. Aggregation to state means is noisy: in most states, standard deviations of Rp and Ds span about a quarter of the entire interval. More importantly, the idea that greater spread between these two candidates might yield more ticket splitting relies on some strong, unstated assumptions about intraparty homogeneity, voter distributions, and the origin of vote splitting.         "
"56"," Figure 9A illustrates the logic whereby spatial party differentials might lead to vote splitting. If both Democrats (e.g., Ds and Dp, where s and p denote “senate” and “presidential” candidates) are located at D, both Republicans at R, and if expected policy outcomes for unified government are, thus, D and R, but for divided government are some weighted average of D and R, say M, then standard proximity theory identifies cutpoints defining zones in which voters should prefer to vote straight tickets (DD or RR) or split tickets (DR or RD). Then, as |Ds−Rp| grows, the central region containing split‐ticket voters grows. Burden and Kimball's “ideological distance” variable is thus constructed on three assumptions: first, that voters react to expected policy outcomes, not candidates per se; second, that the two Democrats and two Republicans in question are ideologically very similar, if not identical; and, third, that a substantial portion of the electorate resides in the center of the ideological spectrum, so that enlargement of the middle region does result in more split‐ticket voting occurring.         "
"57"," Alternative Spatial Theories of Split‐Ticket Voting                      "
"58","Note, then, that the logic of their test fails if voters do not perceive the two Republicans and two Democrats to be ideological twins or if the district's electorate is bipolar. On the first point, consider Figure 9B, and suppose that Connecticut has a symmetrical (e.g., uniform) voter distribution. As Rp moves right, |Ds−Rp| increases. Under the same assumptions about voting as just applied to Figure 9A, though, the amount of split ticket voting should decrease with this increase in |Ds−Rp|, since the ticket splitters are now those in the outer regions, given this particular arrangement of candidates. This is the exact opposite effect from that shown by Figure 9A and assumed to apply everywhere by Burden and Kimball. And, although they point out in their footnote 11 (1998, 539) that the SES set uniquely provides the necessary data to test a balancing thesis since it includes placements of respondents and all four candidates on a common scale, only half of this information is actually incorporated in their construction of “ideological distance.” Either large ideological variation between different party nominees or a noncentrally distributed electorate thwarts interpretation of their regression results.         "
"59","There is, moreover, a plausible variety of intentional vote splitting not captured by their logic. Voters who select candidates only according to ideological proximity, without making projections about policy outcomes that will result from the various permutations of candidate victories, can intentionally split tickets if they perceive there to be large differences in the positions of candidates from the same party. Figure 9B shows two actual SES respondents (marked with asterisks) whose split‐ticket votes exactly match the predictions of simple candidate‐proximity theory. Note that |Ds−Rp| is identical in the two cases, even though one is a vote of RpDs and the other a vote of DpRs. For both of these respondents, in fact, reported votes are consistent with either expected‐policy voting or candidate‐proximity voting. This observational equivalence also undercuts strong claims about voter intentions.         "
"60","More generally, spatial theories of voting are many and varied, and even if one posits that voters choose according to policy outcomes, the proper econometric specification to test for “intent” will depend critically on the underlying formal model. Merrill and Grofman's recent work (1999) unifying directional and proximity models is one excellent blueprint in this regard, since they carefully construct a hybrid model in which both rivals are nested, and allow data to adjudicate between them, or to select a mixture."
"61","Even more pertinent to the issue at hand are two recent articles about how American voters do or do not link their votes in search of moderate policy. In his analysis of individual‐level voting data from the NES, Mebane reaches a conclusion very different from Burden and Kimball, that “policy‐related balancing has often been an important determinant of election outcomes.” (2000, 51) Mebane and Sekhon (2002) extend the logic of that article to midterm voting and find further evidence for not only moderation, but coordination. In both cases, a virtue in these articles is that coordinating and noncoordinating models are estimated in tandem, so that competing models can be compared formally. In each case, coordination and moderating are formally and explicitly defined and distinguished from related phenomena such as economic retrospective voting and incumbency advantage. Mebane notes that the model does not achieve great success at identifying ticket‐splitters (2000, note 26, 51), and this work is not necessarily the last word on the topic. But the care with which the terms of the theoretical model are operationalized is exemplary, and the conclusions are appropriately qualified, particularly concerning underlying assumptions about the effects of institutional context, and how these relate to unrealistic assumptions about voter‐level mechanisms. Burden and Kimball's analysis, by contrast, is not nearly flexible or general enough to support their strong conclusion that voters do not consciously choose to split tickets."
"62","Our hope is that our discussion of ecological inference and its inherent uncertainty highlights the numerous reasons why a researcher must exercise great caution when analyzing aggregate data. With EI, in particular, the purported advances are coupled with much greater computational complexity and a large number of new assumptions. Since EI generally does not outperform OLS (Cho 1998), it is difficult to justify the additional overhead. EI does supply district‐level estimates, but these estimates do not possess desirable statistical properties (Herron and Shotts 2003, 2004). There are instances when one needs to make ecological inferences, and so one will choose to use an ecological inference model such as EI. In these cases, the researcher must be fully aware of the numerous pitfalls that may ensue.         "
"63","For example, Burden and Kimball claim as their principal achievement to have developed the first‐ever accurate district‐level estimates of vote‐splitting. However, their faith in the multi‐stage aggregate data analysis is misplaced, and there are a multitude of reasons to doubt the accuracy of their findings. Furthermore, their search for “intention” in vote‐splitting is more accurately a test for a particular kind of balancing behavior, under key assumptions about ideological homogeneity within parties and distributions of district electorates. That analysis is not general enough to support strong conclusions about voting behavior. It is unreasonable to declare new‐found knowledge when the novel findings depend critically on very strong and unverifiable assumptions about the underlying individual‐level data. Split‐ticket voting behavior remains a fascinating topic, and it also remains a topic plagued by severe data‐analysis barriers."
"64","Caution can never be thrown to the wind when an analysis proceeds through multiple stages of analysis, especially when multiple stages of ecological inference are involved. Ultimately, there is no escaping indeterminacy in cross‐level inference. The problem is ill‐posed and so not amenable to unique “solutions” as such. Here we have emphasized that those who must proceed with ecological inference just the same ought to know their data well; be aware that even ostensibly informative data can be misleading; be on guard against aggregation bias, and endeavor to model it when it occurs; and be as explicit as possible about the logic connecting the micro‐ and macrolevels. But a final caveat is that one can still go astray even having exercised care in all of these manners. Hence, except in highly unusual circumstances, aggregate data analysis intended to yield insight into micro behavior always calls for cautious and guarded interpretation."
