"","x"
"1","Boolean keyword search of textual documents is a generic task used in numerous methods and application areas. Sometimes researchers seek one or a small number of the most relevant documents, a use case we call fact finding and for which Google, Bing, and other search engines were designed. For example, to find the capital of Montana, a weather forecast, or the latest news about the president, the user only wants one site (or a small number of sites) returned. In the second collecting use case, which we focus on, researchers do not try to find the needle in the haystack, at least at first; instead, they seek all documents that describe a particular literature, topic, person, sentiment, event, or concept.         "
"2","Collecting is typically performed by attempting to think of all keywords that represent a specific concept, and selecting documents that mention one or more of these keywords. Yet, this keywords selection process is known to be a “near‐impossible task” for a human being (Hayes and Weinstein 1990), which we demonstrate can greatly bias inferences. Although no researchers should be selecting keywords for this purpose on their own, many applications require keywords. For example, applications of sophisticated methods of automated text analysis, designed to get around simplistic keyword matching and counting methods, are often preceded by selecting keywords to narrow all available documents to a manageable set for further analysis. Similarly, search engines are optimized for fact finding, but regularly used for collecting, even though they are suboptimal for this alternative purpose. Indeed, as we discuss in the third section (“The Unreliability of Human Keyword Selection”), human brains have well‐studied inhibitory processes that, although adaptive for other reasons, explicitly prevent us from recalling many keywords when needed for the task of collecting.1"
"3","The problem of keyword discovery is easier when structured data are available to supplement the raw text, such as search query logs (e.g., Google's AdWords Keyword Tool, or Overture's Keyword Selection Tool), databases of meta‐tags, or web logs (Chen, Xue, and Yu 2008), and a large literature of methods of “keyword expansion or suggestion” has arisen to exploit such information. In this article, we develop methods for the wide array of problems for which raw text is the sole, or most important, source of information. To avoid requiring a human user having to think of all relevant keywords, we introduce methods of computer‐assisted keyword discovery. Our key motivating principle is that although humans perform very poorly in the task of recalling large numbers of words from memory, they excel at recognizing whether any given word is an appropriate representation of a given concept.         "
"4","We begin by describing some of the application areas to which our methodology may provide some assistance. We then conduct an experiment that illustrates the remarkable unreliability of human users in selecting appropriate keywords. Next, we define the statistical problem we seek to solve, along with our notation. We then present our algorithm, several ways of evaluating it, and an illustration of how it works in practice. Lastly, we discuss related prior literature and conclude. The appendices give details on algorithm robustness and how to build queries for much larger data sets. Replication information is available at King, Lam and Roberts (2016).         "
"5","Algorithms that meet the requirements of the statistical problem as framed in the fourth section suggest many new areas of application. We list some here, all of which the algorithm we introduce below may help advance. Some of these areas overlap to a degree, but we present them separately to highlight the different areas from which the use of this algorithm may arise."
"6","Political scientists, lobby groups, newspapers, interested citizens, and others often follow social media discussions on a chosen topic but risk losing the thread of the conversation, and the bulk of the discussion, when changes occur in how others refer to the topic. Some of these wording changes are playful or creative flourishes; others represent political moves to influence the debate or frame the issues. For example, what was once called “gay marriage” is now frequently referred to by supporters as “marriage equality.” Progressive groups try to change the discussion of abortion policy from “pro‐choice” and “pro‐life,” where the division is approximately balanced, to “reproductive rights,” where they have a large majority. Conservatives try to influence the debate by relabeling “late‐term abortion” as “partial‐birth abortion,” which is much less popular. As these examples show, selecting an incomplete set of keywords can result in severe selection bias because of their correlation with the opinions of interest."
"7","Internet censorship exists in almost all countries to some degree. Governments and social media firms that operate within their jurisdictions use techniques, such as keyword‐based blocking, content filtering, and search filtering, to monitor and selectively prune certain types of online content (Yang 2009). Even in developed countries, commercial firms routinely “moderate” product review forums, and governments require the removal of “illegal” material such as child pornography. In response to these information controls, netizens continually try to evade censorship with alternative phrasings. For example, immediately after the Chinese government arrested artist‐dissident Ai Weiwei, many social media websites began censoring the Chinese word for Ai Weiwei (King, Pan, and Roberts 2013); soon after, netizens responded by referring to the same person as “AWW” and the Chinese word for “love,” which in Chinese sounds like the “ai” in “Ai Weiwei.” Other creative censorship avoidance techniques involve using homographs and homophones.            "
"8","Most methods of automated text analysis assume the existence of a set of documents in a well‐defined corpus in order to begin their analysis. They then spend most of their effort on applying sophisticated statistical, machine learning, linguistic, or data‐analytic methods to this given corpus. In practice, this corpus is defined in one of a variety of ways, but keyword searching is a common approach (e.g., Eshbaugh‐Soha 2010; Gentzkow and Shapiro 2010; Ho and Quinn 2008; Hopkins and King 2010; King, Pan, and Roberts 2013; Puglisi and Snyder 2011). In this common situation, our algorithm should help improve the inputs to, and thus the results from, any one of these sophisticated approaches. The same issue applies for simple analysis methods, such as keyword counting.            "
"9","Because statistical classifiers are typically far from perfect (Hand 2006), ordinary users who find individual documents misclassified may question the veracity of the whole approach. Moreover, since most classifiers optimize a global function of the data set, even sophisticated users may find of value hybrid approaches for adding human effort and knowledge to improve classification at the level of smaller numbers of documents. In this situation, keyword‐based classifiers are sometimes more useful because the reasons for mistakes, even if there are more of them, are readily understandable and easily fixable (by adding or removing keywords from the selection list) for a human user (Letham et al. 2015). Keyword classifiers are also much faster than statistical classifiers and can be improved to any higher level of accuracy, with sufficient effort, by continual refinement of the Boolean query.            "
"10","Academics recruiting study participants often bid for ad space next to searches for chosen keywords (Antoun et al. 2015), just as firms do in advertising campaigns. This is common with Google Adwords, Bing Ads, Facebook, and so on. These systems, and other existing approaches, suggest new keywords to those spending advertising dollars by mining information from structured data such as web searches, weblogs from specific websites, or other ad purchases. Our approach can supplement these existing approaches by mining keywords relevant to the population of interest from raw unstructured text found in research documents, literature reviews, or information in private companies such as customer call logs, product reviews, websites, or a diverse array of other sources. Whereas keywords (or more general Boolean searches) for advertising on search engines can be mined from search engine query logs, or website logs, keywords that identify rarely visited pages, or for advertising on social media sites, can only be mined from the unstructured text.            "
"11","Modern search engines work best when prior searches and the resulting structured metadata on user behavior (e.g., clicking on one of the websites offered or not) are available to continuously improve search results. However, in some areas, such metadata are inadequate or unavailable, and keywords must be discovered from the text alone. These include (1) traditional search with unique or unusual search terms (the “long tail”); (2) searching on social media, where most searches are for posts that just appeared or are just about to appear, and so have few previous visits; and (3) enterprise search for (confidential or proprietary) documents that have rarely if ever been searched for before. In these situations, it may be useful to switch from the present fully automated searching to computer‐assisted searching using our technology."
"12","Consider social search. During the Boston Marathon bombings, many followed the conversation on Twitter by searching for #BostonBombings, but at some point the social media Boston authors expressed community spirit by switching to #BostonStrong and out‐of‐towners used #PrayForBoston. Since guessing these new keywords is nearly impossible, those who did not notice the switch lost the thread of the conversation.            "
"13","Human beings, unaided by computers, seem to have no problem coming up with some keywords to enter into search engines (even if not the optimal ones). Everyone is accustomed to doing Google searches, after all. However, as we demonstrate in this section, for the more complicated task of choosing a set of keywords for the task of collection, even expert human users perform extremely poorly and are highly unreliable at this task. That is, two human users familiar with the subject area, given the same task, usually select keyword lists that overlap very little, and the list from each is a very small subset of those they would each recognize as useful after the fact. The unreliability is exacerbated by the fact that users may not even be aware of many of the keywords that could be used to select a set of documents. And attempting to find keywords by reading large numbers of documents is likely to be logistically infeasible in a reasonable amount of time.         "
"14","Here, we first demonstrate this surprising result with a simple experiment. Second, because this result is counterintuitive ex ante, we briefly summarize the well‐developed psychological literature that can be used to explain results like this. And finally, we show the severe statistical bias (or extra ex ante variance) that can result from selecting documents with inadequate keyword lists."
"15","For our experiment, we asked 43 relatively sophisticated individuals (mostly undergraduate political science majors at a highly selective college) to recall keywords with this prompt:               "
"16","We have 10,000 twitter posts, each containing the word “healthcare,” from the time period surrounding the Supreme Court decision on Obamacare. Please list any keywords which come to mind that will select posts in this set related to Obamacare and will not select posts unrelated to Obamacare."
"17","We also gave our subjects access to a sample of the posts and asked them not to consult other sources. We repeated the experiment with an example about the Boston Marathon bombings."
"18","The median number of words selected by our respondents was 8 for the Obamacare example and 7 for the experiment about the Boston Marathon bombings. In Figure 1, we summarize our results with word clouds of the specific keywords selected. Keywords selected by one respondent and not by anyone else are colored red (or gray if reading black and white). The position of any one word within the cloud is arbitrary.            "
"19","The Unreliability of Human Keyword Selection"
"20","Note: Word clouds of keywords were selected by human users; those selected by one and only one respondent are in red (or gray if printed in black and white). The position of each word within the cloud is arbitrary.                        "
"21","The results clearly demonstrate the remarkably high level of unreliability of our human keyword selectors. In the Obamacare example, 149 unique words were recalled by at least one of our 43 respondents. Yet, for 66% of those words, every single one of the remaining 42 respondents, when given the chance, failed to recall the same word (Figure 1, red or gray words in the left panel). In the Boston Marathon bombing example, the percentage of words recalled by a single respondent was 59% (right panel). The level of unreliability was so high that no two users recalled the same entire keyword list.            "
"22","This extreme level of unreliability is not due to our research subjects' being unaware of some of the words. Indeed, after the fact, it is easy to see from Figure 1 that almost all the words recalled are recognizably related to Obamacare or the Boston bombings, respectively. In other words, although humans perform extremely poorly at recall, they are excellent at remembering.            "
"23","The counterintuitive result from our experiment is related to, and can be explained by, psychological research on “inhibitory processes” (and in particular, “part‐list cuing”). The well‐supported finding, from many experiments, is that revealing one word to the research subject facilitates remembering others, but the cue provided by revealing more than a few words strongly inhibits recall of the rest of the set, even though you would recognize them if revealed (Bauml 2008; Roediger and Neely 1982).            "
"24","Why our brains would be constructed to stop us from remembering needed information deserves at least some speculation. One way to think about this is imagining memory as a network diagram with concepts represented as nodes, and connections between concepts represented as edges. Without inhibitory processes, activating any one concept by recall would activate all concepts connected to it, and all those connected to those, and so on (e.g., orange activates apple, apple activates banana, banana activates slip, slip activates…). Millions of concepts would come flowing into your comparatively tiny, short‐term working memory and, unable to handle it all, you would likely be overwhelmed and perhaps unable to think at all. So either working memory would need to be much bigger, which does not seem to be on offer, or inhibitory processes are necessary.2"
"25","As is well known, the choice of a data selection rule, such as that defined by the choice of keywords, is only guaranteed to avoid bias if it is independent of the variables used to analyze the chosen document set. Obviously, this is a strong assumption, unlikely to hold in many appliations, especially when using unreliable (i.e., human‐only) methods of keyword selection. In other words, different keyword lists generate different document sets, which, in turn, can lead to dramatically different inferences, substantive conclusions, and biases."
"26","We now demonstrate these biases in an analysis of the data from our Boston Marathon bombings experiment. We study the well‐known tendency for communities suffering a tragedy to turn public discourse from the obvious negative events into positive expressions based on solidarity, community spirit, and individual heroics. To do this, we use a simple, but still very common, analysis measure (Nielsen 2011). The idea is to code each word in a social media post as having negative (−1), neutral (0), or positive (+1) sentiment (based on a fixed dictionary designed for Twitter) and to sum all the words in a post to give the final sentiment for that tweet. We use this method to compute the average sentiment of all tweets retrieved by each of the 43 keyword lists from our 43 subjects. The point estimates (dots) along with 95% confidence intervals (horizontal lines) for each appear in Figure 2, sorted from negative to positive sentiment.            "
"27","Average Sentiment of 43 Document Sets"
"28","Note: Each document set was selected by a different keyword list, with point estimates (as dots) and 95% confidence intervals (horizontal lines) shown.                        "
"29","The results vividly demonstrate the substantial effect the choice of a keyword list has on the sentiment of the document sets chosen by different research subjects given the identical prompt. Choosing some of the lists (on the bottom left) would lead a researcher to the conclusion that social media discourse was extremely negative during the month following the Boston Marathon bombing. If, instead, one were to choose other keyword sets (which appear in the middle of the graph), a researcher could report “evidence” that sentiment was only slightly negative. Alternatively, a researcher who used one of the keyword lists from the top right would be led to the conclusion that sentiment was relatively positive (by selecting documents that reflected expressions of community spirit). As is evident, almost any substantive conclusion can be drawn from these data by changing choice of the keyword list. This example clearly demonstrates the value of paying far more attention to how keyword lists are selected than has been the case in the literature.            "
"30","We define the reference set, R, to be a set of textual documents, all of which are examples of a single chosen concept of interest (e.g., topic, sentiment, idea, person, organization, event). This set is defined narrowly so that the probability of documents being included that do not represent this concept is negligible. The reference set need not be a random or representative sample of all documents about the concept of interest (if such a process could even be defined), and may even reflect a subset of emphases or aspects of the concept (as was common for individual humans in the previous section).            "
"31","Also define the search set, S, as a set of documents selected because it likely has additional documents of interest, as well as many others not of interest. The search set does not overlap the reference set, . Our goal is to identify a target set, T, which is the subset of the search set () containing documents with new examples of the concept defining documents in the reference set. Ultimately, we are interested in , but, since we have R, the statistical task is to find T in S.            "
"32","In practice, the reference set may be defined by choosing individual documents by hand, selecting an existing corpus, or using all available documents that contain text matching a specific Boolean query,  (defined as a string containing user‐defined keywords and Boolean operators, AND, OR, NOT, such that , for any document d under consideration). The search set can be defined as all websites on the Internet (after removing documents in R), all available documents, a different selected existing corpus, or documents that match a Boolean query,  (such that ). The elements of a Boolean query are “keywords.”3"
"33","The statistical task of finding T is “unsupervised” in that the concept defining the reference and target sets may be broadened by the human user on the fly as part of the process of discovery (rather than, as in “supervised” analyses, T being a fixed quantity to be estimated). We thus seek to identify the target set T by first finding , the set of all keywords in T ranked by likely relationships with the concept. We then use human input in specific ways to craft query , intended to retrieve T from S. Depending on the application, users may also be interested in the set of all keywords in the reference set , the target and reference sets together , a query that returns both the reference and target sets together , or all of the above.            "
"34","Our algorithm is human‐led and computer‐assisted rather than fully automated; it is related to semi‐supervised learning (Zhu and Goldberg 2009). The more common fully automated approaches to document retrieval (e.g., spam filters) use statistical or machine learning classifiers that are viewed as a black box to the user. By restricting ourselves to a simple Boolean search, defined by a set of interpretable keywords, we empower users to control, understand, and continually improve the retrieval process.            "
"35","Another reason for the choice of a human‐powered approach is that the concept that the documents in the reference set share, and for which we seek a target set, is not a well‐defined mathematical entity. Human language and conceptual definitions are rarely so unambiguous. For example, any two nonidentical documents could be regarded as the same (they are both documents), completely unrelated (since whatever difference they have may be crucial), or anything in between. Only additional information about the context (available to the person but not available solely from the data set) can informatively resolve this indeterminacy. To take a simple example, suppose one element of  is the keyword “sandy.” Should the target set include documents related to a hurricane that devastated New Jersey, a congresswoman from Florida, a congressman from Michigan, a cookie made with chopped pecans, a type of beach, a hair color, a five‐letter word, or something else? To make matters worse, it could easily be the case that documents in the reference set represent two of seven of these examples, but two others in the search set are of interest to the human user. Of course, a user can always define the reference set more precisely to avoid this problem, but the nature of language means that some ambiguity will always remain. Thus, we use human input, with information from the text presented to the human user in a manner that is easily and quickly understood, to break this indeterminacy and grow the reference set in the desired direction.            "
"36","The algorithm first partitions S into two groups by classifying whether a document belongs in set T or its complement, . It mines S for all keywords  and then ranks keywords by how well they discriminate between T and . This results in two lists of keywords  and  ranked in order of how well they discriminate each set from the other. The keyword lists themselves are often of interest to users who would like keyword recommendations for various uses. For document retrieval, the user would iterate through the two lists to produce a query  that, when combined with the reference query  to form , best retrieves his or her desired document set of interest.         "
"37","Table 1 gives a brief overview of the specific steps in our proposed algorithm.         "
"38","The simplest application of our algorithm has R and S defined at the outset, but alternatives are often easier in practice. For example, one may begin with a large document set and without any immediately obvious distinction between the two sets. This situation is common with large, continuously streaming, or even ill‐defined data, such as being based on the entire Internet, all social media posts, or all documents narrowed by a set of very broad keywords. In this situation, we can define S and R adaptively, as part of the algorithm (e.g., D'Orazio et al. 2014).            "
"39","Consider the following alternative adaptive strategy. The user begins by defining R narrowly based on one simple keyword search, as a subset of the existing corpus. We then add an intermediate step to the algorithm, which involves mining and displaying a list of keywords found in R, , ranked by a simple statistic such as document frequency or term frequency–inverse document frequency. The user then examines elements of  (aside from those used to define the set) and chooses some keywords to define , which in turn generates a definition for S, so that we can run the rest of the algorithm. The user can then continue to add keywords from  into the final desired query . In this workflow, S can be neither predefined nor retrieved ex ante. This step also mitigates the issue of how to define a search set in large data sets that do not fit into memory all at once or may not even be able to be retrieved all at once. It also leverages additional information from R in the form of keywords likely to identify additional aspects of the concept and keywords the user may not have thought of for defining both R and S.            "
"40","To partition S into T and , we first we define a “training” set by sampling from S and R. We can repeat this step with different random subsettings to increase the diversity of keyword candidates that are surfaced. (Exemplars can substitute for random sampling as well.) Since R is typically much smaller than S and our test set for our classifiers is all of S, we often use the entire R set and a sample of S as our training set.            "
"41","Next, we fit classifiers to the training set, using each document's actual membership in R or S as the outcome variable. As predictors, we use any element of the text of the documents, as well as any available metadata. Any set of statistical, machine learning, or data‐analytic classifiers can be used, but we recommend using as large and diverse a set of methods as is convenient and computationally feasible (e.g., Bishop 1995; Hastie, Tibshirani, and Friedman 2009; Kulkarni, Lugosi, and Venkatesh 1998; Schapire and Freund 2012).            "
"42","After fitting the classifiers, we use the estimated parameters to generate predicted probabilities of R membership for all documents in S. Of course, all the search set documents in fact fall within S, but our interest is in learning from the mistakes these classifiers make.            "
"43","Although we do not need to transform the probabilities into discrete classification decisions for subsequent steps in the algorithm, we provide intuition into these mistakes by doing this now. Table 2 portrays the results for one example classifier, with the originally defined truth in rows and potential classifier decisions in columns. We will typically be interested in documents from the search set, (mis)classified into the reference set, . The idea is to exploit these mistakes since documents in this set will reveal similarities to the reference set, and so they likely contain new keywords we can harvest to better represent the concept of interest.4"
"44","Once we have predicted probabilities of R membership for each document in S from the classifiers, we need to turn these into a single T membership “score” for the purpose of grouping documents. For a single classifier, the predicted probability of R membership from S is the predicted probability of T membership. In most situations, we recommend the use of multiple classifiers, so that we can extract their different “opinions” about in which set individual documents belong. The different classifiers will typically pick up on different aspects of the concept and thus highlight different keywords for the user to choose from. To ensure that this diversity of opinion is reflected in our keyword lists, we aggregate the probabilities across classifiers for a single document by taking the maximum probability across the classifiers as the membership score (i.e., rather than the usual approach of using the average or plurality vote). We then use this score to group documents into T and . Our simple aggregation rule thus boils down to placing all documents with at least one classifier “vote.”            "
"45","After partitioning S into our estimated target set T and nontarget set , we must find and rank keywords that best discriminate T and . We do this in three steps: (a) mine all keywords from S (perhaps limiting our list to those that meet thresholds such as a minimum document frequency of five documents), (b) sort them into those that predict each of the two sets, and (c) rank them by degree of discriminatory power.            "
"46","Step (a) is accomplished by merely identifying all unique keywords in S. This is a simple step for our computer algorithm, but it is important in practice since a human who thinks of a word not in any documents in S will be useless, no matter how compelling the word seems to be.            "
"47","For Step (b), we use the proportion of documents in which each keyword appears at least once. For example, if a keyword appears in 5 out of 10 T documents and 15 out of 50  documents, we put that keyword into the T list since it appears in 50% of T documents and 30% of  documents, despite the fact that it appears in 10 more  documents on an absolute scale. Keywords that appear in both sets with equal document proportions can be placed in either list or both lists.            "
"48","In Step (c), we rank the keywords within lists, according to how well they discriminate the two sets. Although different scoring metrics could be used to accomplish this task, we find that a metric based on the following likelihood approach is quite effective (see Letham et al. 2013). For document  at any point in using the algorithm, let  equal 1 if  and 0 if . For each keyword k in either list, denote  and  as the number of documents in T that do and do not match k, respectively, and  and  as the number of documents in set  that do and do not match k, respectively. Also define the marginal totals so that  and  denote the total number of documents in S that do and do not contain k, respectively, and  and  denote the number of documents in T and , respectively.            "
"49","This then leads to a convenient likelihood function for the model we use to distinguish T from :               "
"50","We then calculate the value of the likelihood function for each keyword in each list and rank them all from highest to lowest likelihood."
"51","Our final step, prior to iterating, involves using human input to choose items from the two keyword lists and to build queries  and . Following the third section, we optimize so humans do what they are good at and computerize what they are not. We present all the keywords, so the humans do not need to recall anything, along with computerized rankings to organize best guesses about what may be of interest to them. Then the humans can use their detailed contextual knowledge, unavailable to our algorithm, to find different eddies of conversation and meanings of concepts of interest not previously recalled. This process of evaluating a list of words is of course considerably faster and much more reliable than asking humans to pull keywords out of thin air or thinner memories.            "
"52","The algorithm is unsupervised so that human users can easily refine, improve, or totally redefine the concept of interest, as the keyword lists inspire them to think of new perspectives on the same material. Users may also discover new directions that cause them to begin again with a completely new reference set, or to add to the existing reference set or reference query .            "
"53","At this point, the user can iterate with the algorithm in various ways to continue to adjust the partition of S into T and  and to refine or redefine the concepts of interest. One way to iterate can be to simply update the reference query with the new selected words and rerun the algorithm. Another is for the user to designate specific keywords or documents of interest or not of interest, which gives the algorithm more information to update the definitions of T and .            "
"54","For our evaluations, we require a ground truth and a data set with documents properly coded to the concept of interest. Of course, the version of keyword selection we are studying is an unsupervised task, and so the concept initially chosen in real applications is not necessarily well defined, may differ from user to user or application to application, and can be refined or changed altogether while using the algorithm; indeed, the ability of the user to make these changes is an important strength of the algorithm in practice."
"55","Thus, to make ourselves vulnerable to being proven wrong, we evaluate distinct parts of the algorithm in specifically designed experiments. For example, we consider a limited case with a specific and fixed concept of interest. To do this, we leverage the usage of Twitter hashtags as an explicit way users code their own concepts. The 4/15/2013 Boston Marathon bombings example used earlier was defined this way, with the hashtag #bostonbombings. We then construct a data set composed of three different sets of tweets. As the reference set, we use 5,909 English‐language tweets that contain the hashtag #bostonbombings but not the word boston posted April 15–18, 2013. The target set T we hope the algorithm will identify contains 4,291 tweets during the same time period that contains both #bostonbombings and boston. We created the  portion of the search set with the 9,892 tweets that were posted April 12–13, 2013, before the bombings, that contain the word boston but not #bostonbombings. The especially useful feature of these data is that the bombings were a surprise event that no one on social media was aware of ahead of time, which makes the demarcation between T and  much clearer than it would normally be.         "
"56","The task of identifying the target set is, of course, straightforward with the keywords #bostonbombings and boston, and so solely for this experiment we remove them from the text of each of the tweets before our analysis. We also do not use metadata indicating the date or time of the tweet. This is therefore an artificial example, but one constructed to make it possible to evaluate. The goal is for human users selecting keywords with our algorithm to be more accurate, more reliable, faster, and more creative than working on their own without it. Although this is the relevant goal for a single human user, it is a trivially easy standard for our algorithm to meet. To see this, consider a limited special case of our algorithm with keyword lists ordered randomly. Since we showed above that humans are usually incapable of recalling more than a small fraction of relevant keywords, but are very good at recognizing important keywords put before them, even randomly ordered keyword lists would still provide a great deal of help.         "
"57","We thus seek to evaluate only the quantitative features of our algorithm here, and so we run the algorithm once without iteration, and also without any human input or interaction. To simplify the analysis, and to make replication of our results easier with fewer computational resources, we degrade our approach further by using only two fast classifiers (Naive Bayes and Logit). The estimated target set is designated as any document that receives at least one classifier vote, with probability above 0.5. We also preprocess the documents in standard ways, by stemming, and removing punctuation, stop words, numbers, and words with fewer than three characters."
"58","We evaluate this analysis in three ways, beginning in this section with the qualitative summary in Table 3. This table lists the top 25 (stemmed) keywords from the target T and nontarget  keyword lists produced by a single run of the algorithm, without human input. We can evaluate the algorithm informally by merely looking at the words and seeing what readers recognize. It appears that most of the target keywords are closely related to the bombing incident (e.g., #prayforboston, thought[s], prayer, fbi, arrest, bomb, inure, attack, victim, terrorist). A few words are clearly related but may be too imprecise to be useful as keywords to select documents (e.g., cnn, sad). Most nontarget keywords do a good job of finding events related to Boston that are unrelated to the bombings, largely related to sports teams (e.g., celtic, game, miami, heat, red sox, bruin, win, fan). They also include a few words that were apparently misclassified and so should be in the target set (e.g., tsarnaev). The word bostonmarathon in the target set and marathon in the nontarget set do not clearly discriminate posts related or unrelated to the bombings on their own to necessarily be useful—although interestingly, the algorithm discovered a pattern difficult for humans: that social media posts happened to use the former word to describe the bombings and the latter to describe the sporting event.5"
"59","Second, we more formally evaluate the likelihood model used in our algorithm to group and rank keywords. Ideally, the target set list should have keywords that perform well on both recall and precision at the top, and the nontarget set list should have keywords that perform poorly on both recall and precision.6 Figure 3 reports the cumulative recall and precision for the first 100 keywords in each list (introduced one at a time from left to right in both graphs). The cumulative recall (left graph) and precision (right graph) are running estimates, as we add more and more terms into an “OR” Boolean query.            "
"60","The key result in Figure 3 is that the target set line (in teal) is usually well above the nontarget set line (in red) for both recall and precision. In other words, our algorithm is doing a good job separating the two lists, which provides quantitative confirmation of the qualitative impression from the words in Table 3.            "
"61","By definition, cumulative recall increases as we add more keywords. The fact that recall is not consistently zero for the nontarget set list speaks to both the need for human input as well as the nature of human language, where the same keywords can often be used in social media posts with the opposite meanings—describing concepts of interest and not of interest. The general downward trend of the cumulative precision for the target set list shows that the general ordering of the keywords is also valid, with more precise words near the top of the list."
"62","For our final evaluation, we compare this single noniterative run of our algorithm (with no human in the loop) with a purely human approach. We do this in two ways. First, we compare the top 145 words from our target set keyword list with the 145 unique keywords that the 43 undergraduates in our Boston Bombings experiment came up with in the experiment described in the third section. For this evaluation, we are therefore comparing the effort of 43 minds versus one single run of the computer algorithm without any human input. This is not a real comparison, of course, since in practice, researchers are unlikely to be able to hire 43 research assistants and would be able to use some human input to improve the algorithm, but it gives a useful baseline comparison."
"63","Panels (a) and (b) in Figure 4 give density estimates for the overall precision and recall of the 145 words chosen by humans compared to the top 145 words from the target set list from our algorithm. The results show that recall of the algorithm is approximately the same as the collective work of 43 humans. Put differently, both the one‐step algorithm and the humans come up with keywords of about the same quality. Of course, we constrained the algorithm to the same number of words as the 43 humans when, of course, our algorithm would produce many more than the 145 words shown in the graph.            "
"64","Comparing Recall and Precision for the Algorithm versus 43 Human Users"
"65","Note: Panels (a) and (b) display the distribution of recall and precision for the 145 words from the humans and the top 145 words from the algorithm. Panels (c) and (d) display cumulative recall and precision for each human (dotted line) versus the first 50 target set keywords of the algorithm (solid line). Human keywords are in the order that humans thought of them.                        "
"66","To get a sense of the quality of the individual words in this comparison, we see from Panel (b) that the precision of the algorithm's words is generally much higher than the precision of words from the humans. When restricted to 145 words, the algorithm produces the same level of recall as the effort of 43 different humans combined, but the words chosen by the algorithm contain much less noise and are therefore of substantially higher quality than human‐only approaches."
"67","Finally, we consider a more realistic comparison of a (still limited) one‐step special case version of our algorithm without human input to one human research assistant at a time. Individual humans choose only about 7–8 words, with no one of our 43 individuals choosing more than 20. Panels (c) and (d) of Figure 4 give cumulative recall and precision for our algorithm out to 50 words (although it could of course keep going) compared to each of our 43 human users. Individual undergraduate cumulative recall appears as separate black lines in Panel (c). The algorithm's cumulative recall is better than most of the human users until about 12 words are recalled, at which point the algorithm's performance soars well beyond any one of the human users. After 20 words, the human users obviously have nothing to offer. The algorithm's precision (Panel d) is also better than most of the human users in the entire range of human‐recalled words, but then continues out to 50 words in the graph without losing much precision in the process.            "
"68","Although our algorithm is clearly better than individual human users, using the algorithm with human input as designed has the potential to be much better than either alone."
"69","In what became known as the “Wang Lijun incident” in China, police chief of Chongqing Wang Lijun was abruptly demoted from his job on February 2, 2012. Rumors began circulating that Wang had fallen out of favor with his boss, party chief of Chongqing and popular political leader Bo Xilai. On February 6, 2012, Wang Lijun went to the U.S. Consulate in Chengdu, possibly to seek asylum, but after the consulate became surrounded by police, Wang agreed to leave the consulate and was detained by the Chinese government. During this time, rumors about how the incident, perceived as treason by many in China, would affect the political prospects of Bo Xilai spread virally across social media, culminating in Bo's March 15 dismissal from his post. It was later revealed that Wang had fled to the consulate because he had confronted Bo that Bo and his wife, Gu Kailai, were connected to the murder of British businessman Neil Heywood, who had died in November 2011 in Chongqing. In the dramatic trials of Wang, Gu, and Bo that followed, all were convicted with lengthy prison sentences."
"70","The Wang Lijun incident and Bo Xilai scandal were some of the most dramatic and important political events to occur in China in decades. Bo Xilai, son of famous revolutionary Bo Yibo, had gained widespread popular support in Chongqing for his crackdown on crime and promotion of Maoist culture. He was also an ambitious politician who was hoping to be promoted to higher leadership roles within the Party. Because of the scale and drama involved in the scandal, the Bo Xilai scandal was of tremendous public interest and widely discussed, but at the same time highly censored."
"71","Social media posts that used the names “Bo Xilai,” “Gu Kailai,” and “Wang Lijun” were censored across much of the social media landscape by automated filters programmed in many social media websites. At the same time, social media users, who know about these filters, tried to write posts using creative rephrasings and neologisms so their posts would slip past the filters but still be understandable to general readers. Amid this linguistic arms race between government‐controlled computers and the Chinese people, researchers trying to understand this scandal have to scramble to keep up with these novel words and rephrasings. Missing even one may cause them to lose the thread of the conversation, bias their inferences, or make finding posts of interest difficult or impossible. We show how our algorithm can be used by researchers to find these words and the posts of interest."
"72","We began with words widely known to be used to evade censorship for the reference set and those that were more commonly used to describe the scandal in the search set. Examples of a few of the words we discovered appear in the first column of Table 4. For example, the reference set was composed of microblogs that contained the word bxl (in English), the first letter of each syllable in Bo's name, during the first half of 2012, and the search set was the broader term to describe the scandal “Chongqing incident” (). The target set picked up a variety of words related to the event, including words that netizens were using to evade censorship. For example,  , a homophone for Wang Lijun, appeared within the top 100 of the list. Bu xing le (, which means “not OK,” but has the same initials as Bo Xilai) appeared within the keyword list associated with the reference set BXL. Upon reading texts with these words, we verified that both of these words were being used to evade censorship.         "
"73","Based on the new words we found to evade censorship, we further revised the reference set and reran the algorithm to search for other keywords. For example, we used the homophone for Wang Lijun,  , as the reference set and again “Chongqing incident”   as the search set. We discovered yet another nickname for Wang Lijun, “matron”  . Using Bo's full name   to define the reference set and the abbreviation for Gu Kailai's name, “gxl,” as the search set, we also found the abbreviation for Neil Heywood's name in the keyword target set, “hwd.”         "
"74","Of course, not every word on the list was being used to evade censorship, since to be effective these words need to be rare. For example, many of the words were closely indicative of the scandal but not neologisms. However, a human user knowledgeable about the region can easily pick out the words that are being used to evade the censors from this longer list. Seeing the English abbreviation “hwd” out of a list of mostly Chinese characters automatically alerts the reader or researcher that it is being used as shorthand for another word, and knowing the context (or perusing the documents) would enable one to ascertain whether it is being used to substitute for a censored word. Similar patterns emerge in the purely Chinese words as well. The power here comes from the combination of the algorithm doing the “recalling” and the human doing the recognition of what is relevant."
"75","Our algorithm is related to the information retrieval literature and “query expansion” methods, including algorithms that add or reweight keywords within search queries to retrieve a more representative set of documents (for a review, see Carpineto and Romano (2012), Rocchio (1971), Xu and Croft (1996)). Our approach differs in two important ways. First, most query expansion methods retrieve new keywords by stemming the original keyword, looking for synonyms or co‐occurrences, or finding related terms within the corpus defined by the original keyword (Bai et al. 2005; Schütze and Pedersen 1997). In contrast, our approach finds related keywords in external corpora that do not include the original keyword. For example, thesauri will not reveal novel hashtags or many of the terms in log tail search or those used to evade censors.         "
"76","While some query expansion methods use large external corpora, such as Wikipedia, to enhance keyword retrieval (Weerkamp, Balog, and de Rijke 2012), our method allows the user to define the external corpus without any structured data aside from the sets R and S. We thus rely on the user's expertise to define the search and reference sets from which new, related keywords will be generated.         "
"77","Second, current query expansion methods often try to limit “topic drift” or are concerned with identifying keywords that are too general (Mitra, Singhal, and Buckley 1998). As a result, most of those methods implicitly focus on maximizing the precision of the documents retrieved (making sure the documents retrieved are all relevant), whereas we focus on both precision and recall (making sure to retrieve as many of the relevant documents as possible). Our method intentionally suggests both general and specific keywords and includes topic drift, not as a problem to be fixed but, at times, as the subject of the study. We instead rely on the user interaction phase of our model to refine the keyword suggestions and avoid topic drift outside the user's interest.         "
"78","Finally, most query expansion methods rely on probabilistic models of the lexical properties of text (e.g. Carpineto and Romano 2004; Voorhees 1994). Our approach uses ensembles of document classifiers to first group documents that may be of interest to the user. (A related approach is search results clustering [SRC], except with user‐specified corpora of documents; see Carpineto et al. 2009 for a review.) It then retrieves keywords that are likely to appear in this document group, but unlikely to appear in the rest of the search data set. Despite the differences between our approach and the current query expansion methods, our approach is actually a more general framework that can incorporate many of the existing methods, as we describe in a later section.         "
"79","The human‐led, computer‐assisted, iterative algorithm we propose here learns from the mistakes made by automated classifiers, as well as the decisions of users in interacting with the system. In applications, it regularly produces lists of keywords that are intuitive, as well as those that would have been unlikely to have been thought of by a user working in isolation. Compared to a team of 43 human users, our algorithm has the same recall but far better precision; the algorithm also dominates individual human users on many dimensions. The algorithm discovers keywords, and associated document sets, by mining unstructured text, defined by the user, without requiring structured data. The resulting statistical framework and methods open up a range of applications for further analyses. In addition to the examples in English and Chinese, this algorithm has been useful in detecting Arabic dialects (Smith 2016), and we see no reason why it would not work on all human languages, but this would of course need to be studied further.         "
