"","x"
"1","Although our methodology works for any unstructured text, we use blogs as our running example. Blogs (or “web logs”) are periodic web postings usually listed in reverse chronological order.2 For present purposes, we define our inferential target as expressed sentiment about each candidate in the 2008 American presidential election. Measuring the national conversation in this way is not the only way to define the population of interest, but it seems to be of considerable public interest and may also be of interest to political scientists studying activists (Verba, Schlozman, and Brady 1995), the media (Drezner and Farrell 2004), public opinion (Gamson 1992), social networks (Adamic and Glance 2005; Huckfeldt and Sprague 1995), or elite influence (Grindle 2005; Hindman, Tsioutsiouliklis, and Johnson 2003; Zaller 1992). We attempted to collect all English‐language blog posts from highly political people who blog about politics all the time, as well as others who normally blog about gardening or their love lives, but choose to join the national conversation about the presidency for one or more posts. Bloggers' opinions get counted when they post and not otherwise, just as in earlier centuries when public opinion was synonymous with visible public expressions rather than attitudes and nonattitudes expressed in survey responses (Ginsberg 1986).3"
"2","Our specific goal is to compute the proportion of blogs each day or week in each of seven categories, including extremely negative (−2), negative (−1), neutral (0), positive (1), extremely positive (2), no opinion (NA), and not a blog (NB).4 Although the first five categories are logically ordered, the set of all seven categories is not (which rules out innovative approaches like Wordscores, which presently requires a single dimension; Laver, Benoit, and Garry 2003). Bloggers write to express opinions and so category 0 is not common, although it and NA occur commonly if the blogger is writing primarily about something other than our subject of study. Category NB ensures that the category list is exhaustive. This coding scheme represents a difficult test case because of the mixed data types, because “sentiment categorization is more difficult than topic classification” (Pang, Lee, and Vaithyanathan 2002, 83), and because the language used ranges from the Queen's English to “my crunchy gf thinks dubya hid the wmd's, :)!!”5"
"3","We now preview the type of empirical results we seek. To do this, we apply the nonparametric method described below to blogosphere opinions about John Kerry before, during, and after the botched joke in the 2006 election cycle, which was said to have caused him to not enter the 2008 contest (“You know, education—if you make the most of it … you can do well. If you don't, you get stuck in Iraq”). Figure 1 gives a time‐series plot of the proportion of blog posts in each of the opinion categories over time. The sharp increase in the extremely negative (−2) category occurred immediately following Kerry's joke. Note also the concomitant drop in other categories occurred primarily from the −1 category, but even the proportion in the positive categories dropped to some degree. Although the media portrayed this joke as his motivation for not entering the race, this figure suggests that his high negatives before and after this event may have been even more relevant.         "
"4","                 Blogosphere Responses to Kerry's Botched Joke                      "
"5"," Notes: Each line gives a time series of estimates of the proportion of all English‐language blog posts in categories ranging from −2 (extremely negative, colored red) to 2 (extremely positive, colored blue). The spike in the −2 category immediately followed Kerry's joke. Results were estimated with our nonparametric method.                     "
"6","These results come from an analysis of word patterns in 10,000 blog posts, of which only 442 from five days in early November were actually read and hand coded by the researchers. In other words, the method outlined in this article recovers a highly plausible pattern for several months using word patterns contained in a small, nonrandom subset of just a few days when anti‐Kerry sentiment was at its peak. This was one incident in the run‐up to the 2008 campaign, but it gives a sense of the widespread applicability of the methods. Although we do not offer these in this article, one could easily imagine many similar analyses of political or social events where scale or resource constraints make it impossible to continuously read and manually categorize texts. We offer more formal validation of our methods below."
"7","We now explain how to represent unstructured text as structured variables amenable to statistical analysis, first by coding variables and then via statistical notation."
"8","To analyze text statistically, we represent natural language as numerical variables following standard procedures (Joachims 1998; Kolari, Finin, and Joshi 2006; Manning and Schütze 1999; Pang, Lee, and Vaithyanathan 2002). For example, for our key variable, we summarize a document (a blog post) with its category. Other variables are computed from the text in three additional steps, each of which works without human input, and all of which are designed to reduce the complexity of text.            "
"9","First, we drop non‐English‐language blogs (Cavnar and Trenkle 1994), as well as spam blogs (with a technology we do not share publicly; for another, see Kolari, Finin, and Joshi 2006). For the purposes of this article, we focus on blog posts about President George W. Bush (which we define as those that use the terms “Bush,”“George W.,”“Dubya,” or “King George”) and similarly for each of the 2008 presidential candidates. We develop specific filters for each person of interest, enabling us to exclude others with similar names, such as to avoid confusing Bill and Hillary Clinton. For our present methodological purposes, we focus on 4,303 blog posts about President Bush collected February 1–5, 2006, and 6,468 posts about Senator Hillary Clinton collected August 26–30, 2006. Our method works without filtering (and in foreign languages), but filters help focus the limited time of human coders on the categories of interest.            "
"10","Second, we preprocess the text within each document by converting to lowercase, removing all punctuation, and stemming by, for example, reducing “consist,”“consisted,”“consistency,”“consistent,”“consistently,”“consisting,” and “consists” to their stem, which is “consist.” Preprocessing text strips out information, in addition to reducing complexity, but experience in this literature is that the trade‐off is well worth it (Porter 1980; Quinn et al. 2009).            "
"11","Finally, we summarize the preprocessed text as dichotomous variables, one type for the presence or absence of each word stem (or “unigram”), a second type for each word pair (or “bigram”), a third type for each word triplet (or “trigram”), and so on to all “n‐grams.” This definition is not limited to dictionary words. In our application, we measure only the presence or absence of stems rather than counts (the second time the word “awful” appears in a blog post does not provide as much information as the first). Even so, the number of variables remaining is enormous. For example, our sample of 10,771 blog posts about President Bush and Senator Clinton includes 201,676 unique unigrams, 2,392,027 unique bigrams, and 5,761,979 unique trigrams. The usual choice to simplify further is to consider only dichotomous stemmed unigram indicator variables (the presence or absence of each of a list of word stems), which we have found to work well. We also delete stemmed unigrams appearing in fewer than 1% or greater than 99% of all documents, which results in 3,672 variables. These procedures effectively group the infinite range of possible blog posts to “only”23,672 distinct types. This makes the problem feasible but still represents a huge number (larger than the number of elementary particles in the universe).            "
"12","Researchers interested in similar problems in computer science commonly find that “bag of words” simplifications like this are highly effective (e.g., Pang, Lee, and Vaithyanathan 2002; Sebastiani 2002), and our analysis reinforces that finding. This seems counterintuitive at first, since it is easy to write text whose meaning is lost when word order is discarded (e.g., “I hate Clinton. I love Obama”). But empirically, most text sources make the same point in enough different ways that representing the needed information abstractly is usually sufficient. As an analogy, when channel surfing for something to watch on television, pausing for only a few hundred milliseconds on a channel is typically sufficient; similarly, the negative content of a vitriolic post about President Bush is usually easy to spot after only a sentence or two. When the bag of words approach is not a sufficient representation, many procedures are available: we can code where each word stem appears in a document, tag each word with its part of speech, or include selective bigrams, such as by replacing “white house” with “white_house” (Das and Chen 2001). We can also use counts of variables or code variables to represent meta‐data, such as the URL, title, blogroll, or whether the post links to known liberal or conservative sites (Thomas, Pang, and Lee 2006). Many other similar tricks suggested in the computer science literature may be useful for some problems (Pang and Lee 2008), and all can be included in the methodology described below, but we have not found them necessary for the many applications we have tried to date.            "
"13","Our procedures require two sets of text documents. The first is a small labeled set, for which each document i (i = 1, …, n) is labeled with one of the given categories, usually by reading and hand coding (we discuss how large n needs to be in the sixth section, and what to do if hand coders are not sufficiently reliable in the appendix). We denote the Document category variable as Di, which in general takes on the value Di = j, for possible categories j = 1, …, J.6 (In our running example, Di takes on the potential values {−2, −1, 1, 0, 1, 2, NA, NB}.) We denote the second, larger population set of documents as the inferential target, and in which each document ℓ (for ℓ= 1, …, L) has an unobserved classification Dℓ. Sometimes the labeled set is a sample from the population and so the two overlap; more often it is a nonrandom sample from a different source than the population, such as from earlier in time.            "
"14","All other information is computed directly from the documents. To define these variables for the labeled set denote Sik as equal to 1 if word Stem k (k = 1, …, K) is used at least once in document i (for i = 1, …, n) and 0 otherwise (and similarly for the population set, substituting index i with index ℓ). This makes our abstract summary of the text of document i the set of these variables, {Si1, …, SiK}, which we summarize as the K × 1 vector of word stem variables Si. We refer to Si as a word stem profile since it provides a summary of all the word stems (or other information) used in a document.            "
"15","The quantity of interest in most of the supervised learning literature is the set of individual classifications for all documents in the population, {D1, …, DL} . In contrast, the quantity of interest for most content analyses in social science is the aggregate proportion of all (or a subset of all) of these population documents that fall into each category: P(D) ={P(D = 1), …, P(D = J)}′ where P(D) is a J × 1 vector, each element of which is a proportion computed by direct tabulation:               "
"16","This section discusses problems with two common methods that arise when they are used to estimate social aggregates rather than individual classifications."
"17","A simple way of estimating P(D) is direct sampling: identify a well‐defined population of interest, draw a random sample from the population, hand code all the documents in the sample, and count the documents in each category. This method requires basic sampling theory, no abstract numerical summaries of any text, and no classifications of individual documents in the unlabeled population.            "
"18","The second approach to estimating P(D), the aggregation of individual document classifications, is standard in the supervised learning literature. The idea is to first use the labeled sample to estimate a functional relationship between document category D and word features S. Typically, D serves as a multicategory dependent variable and is predicted with a set of explanatory variables {Si1, …, SiK}, using some statistical, machine learning, or rule‐based method (such as multinomial logit, regression, discriminant analysis, radial basis functions, CART, random forests, neural networks, support vector machines, maximum entropy, or others). Then the coefficients of the model are estimated, and both the coefficients and the data‐generating process are assumed the same in the labeled sample as in the population. The coefficients are then used with the features measured in the population, Sℓ, to predict the classification for each population document Dℓ. Social scientists then aggregate the individual classifications via equation (1) to estimate their quantity of interest, P(D).            "
"19","Unfortunately, as Hand (2006) points out, the standard supervised learning approach to individual document classification will fail in two circumstances, both of which appear common in practice. (And even if classification succeeds with high or optimal accuracy, the next subsection shows that estimating population proportions can still be biased.)            "
"20","First, when the labeled set is not a random sample from the population, both methods fail. Yet “in many, perhaps most real classification problems the data points in the [labeled] design set are not, in fact, randomly drawn from the same distribution as the data points to which the classifier will be applied. … It goes without saying that statements about classifier accuracy based on a false assumption about the identity of the [labeled] design set distribution and the distribution of future points may well be inaccurate” (Hand 2006, 2). Deviations from randomness may occur due to “population drift,” which occurs when the labeled set is collected at one point and meant to apply to a population collected over time (as with blogs), or for other reasons. The burdens of hand coding become especially apparent when considering the typical analysis within subgroups and the need for a separate random sample within each.            "
"21","Second, the data‐generation process assumed by the standard supervised learning approach predicts D with S, modeling P(D ∣S), but the world works in reverse. For our running example, bloggers do not start writing and only afterwards discover their affect toward the president: they start with a view, which we abstract as a document category, and then set it out in words. That is, the right data‐generation process is the inverse of what is being modeled, where we should be predicting S with D, and inferring P(S∣ D). The consequence of using P(D ∣S) instead (and without Bayes Theorem, which is not very helpful in this case) is the requirement of two assumptions needed to generalize from the labeled sample to the population. The first is that S “spans the space of all predictors” of D (Hand 2006, 9), which means that once one controls for the measured variables, there exists no other variable that could improve predictive power. In problems involving human language, this assumption is not met, since S is intentionally an abstraction and so by definition does not represent all existing information in the predictors. The other assumption is that the class of models chosen for P(D ∣S) includes the “true” model. This is a more familiar assumption to social scientists, but it is no easier to meet. In this case, finding even the best model or a good model, much less the “true one,” is a difficult and time‐consuming task given the huge number of potential explanatory variables coded from text and potential models to run. As Hand writes, “Of course, it would be a brave person who could confidently assert that these two conditions held” (2006, 9).            "
"22","Here we show that even optimal individual document classification that meets all the assumptions of the last section can lead to biased estimates of the document category proportions. The criterion for success in the classification literature, the percent correctly classified in a test set, is obviously appropriate for individual‐level classification, but it can be seriously misleading when characterizing document populations. For example, of the 23 models estimated by Pang, Lee, and Vaithyanathan (2002), the percent correctly predicted ranges from 77% to 83%. This is an excellent classification performance for sentiment analysis, but suppose that all the misclassifications were in a particular direction for one or more categories. In that situation, the statistical bias (the average difference between the true and estimated proportion of documents in a category) in using this method to estimate the aggregate quantities of interest could be as high as 17 to 23 percentage points. This does not matter for the authors, since their goal was classification, but it could matter for researchers interested in category proportions.            "
"23","Unfortunately, except at the extremes, there exists no necessary connection between low misclassification rates and low bias: it is easy to construct examples of learning methods that achieve a high percent of individual documents correctly predicted and large biases for estimating the aggregate document proportions, or other methods that have a low percent correctly predicted but nevertheless produce relatively unbiased estimates of the aggregate quantities. For example, flipping a coin is a bad predictor of which party will win a presidential election, but it does happen to provide an unbiased estimate of the percentage of Democratic presidential victories since 1912. Since the goal of this literature is individual classification, it does not often report the bias in estimating the aggregates. As such, the bulk of the otherwise impressive supervised learning classification literature offers little indication of whether the methods proposed would work well for those with different goals."
"24","We now introduce a method optimized for estimating document category proportions. To simplify the exposition, we first show how to correct aggregations of any existing classification method and after offer our stand‐alone procedure, not requiring (or producing) a method of individual document classification."
"25","Intuition Consider multinomial logit or any other method which can generate individual classifications. Fit this model to the labeled set, use it to classify each of the unlabeled documents in the population of interest, and aggregate the classifications to obtain a raw, uncorrected estimate of the proportion of documents in each category. Next, estimate misclassification probabilities by first dividing the labeled set of documents into a training set and a test set (ignoring the unlabeled population set). Then apply the same classification method to the training set alone and make predictions for the test set,  (ignoring the test set's labels). Then use the test set's labels to calculate the specific misclassification probabilities between each pair of actual classifications given each true value, . These misclassification probabilities do not tell us which documents are misclassified, but they can be used to correct the raw estimate of the document category proportions.               "
"26","For example, suppose we learn, in predicting test set proportions from the training set, that 17% of the documents our method classified as D = 1 really should have been classified as D = 3. For any one individual classification in the population, this fact is of no help. But for document category proportions, it is easy to use: subtract 17% from the raw estimate of the category 1 proportion in the population, P(D = 1), and add it to category 3, P(D = 3). Even if the raw estimate was badly biased, which can occur despite optimal individual document classification, the resulting corrected estimate would be unbiased so long as the population misclassification errors were estimated well enough from the labeled set (a condition we discuss below). Even if the percent correctly predicted is low, this corrected method can give unbiased estimates of the category frequencies.               "
"27","Formalization for Two Categories For the special case where D is dichotomous, the misclassification correction above is well known in epidemiology—an area of science directly analogous to the social sciences, where much data are at the individual level, but the quantities of interest are often at the population level. To see this, consider a dichotomous D, with values 1 or 2, a raw estimate of the proportion of documents in category 1 from some method of classification, , and the true proportion (corrected for misclassification), P(D = 1).7 Then define two forms of correct classification as “sensitivity,” (sometimes known as “recall”), and “specificity,” or . For example, sensitivity is the proportion of documents predicted to be in category 1 among those actually in category 1.               "
"28","Then we note that the proportion of documents estimated to be in category 1 must come from only one of two sources: documents actually in category 1 that were correctly classified and documents actually in category 2 but misclassified into category 1. We represent this accounting identity, known as the Law of Total Probability, as                   "
"29","Since equation (2) is one equation with only one unknown [since P(D = 1) = 1 − P(D = 2)], it is easy to solve. As Levy and Kass (1970) first showed, the solution is                  "
"30","Generalization to Any Number of Categories The applications in epidemiology for which these expressions were developed are completely different than our problems, but the methods developed there are directly relevant. This connection enables us to use for our application the generalizations developed by King and Lu (2008).8 Thus, we first generalize equation (2) to include any number of categories by substituting j for 1, and summing over all categories instead of just 2:                  "
"31","Interpretation The section entitled “Optimizing for a Different Goal” shows that a method meeting all the assumptions required for optimal classification performance can still give biased estimates of the document category proportions. We therefore offer here statistically consistent estimates of document category proportions, without having to improve individual classification accuracy and with no assumptions beyond those already made by the individual document classifier. In particular, classifiers require that the labeled set be a random sample from the population. Our method only requires a special case of the random selection assumption: that the misclassification probabilities (sensitivity and specificity with 2 categories or  for all j and j′ in equation (4)) estimated with data from the labeled set also hold in the unlabeled population set. This assumption may be wrong, but if it is, then the assumptions necessary for the original classifier to work are also wrong and will not necessarily even give accurate individual classifications. More importantly, our approach will also work with a biased classifier.               "
"32","We now offer an approach that requires no parametric statistical modeling, individual document classification, or random sampling from the target population. It also correctly treats S as a consequence rather than cause of D.            "
"33","The Approach This method requires only one additional step beyond that in the previous section: instead of using S and D to estimate , and then separately correcting via equation (4), we avoid having to compute  by using S in place of  in that same equation. That is, any observable implication of D can be used in place of  in equation (4); because  is a function of S—since the words chosen are by definition a function of the document category—it is simplest to use it directly. Thus, we have                  "
"34","Estimation Elements of P(S) can be estimated by direct tabulation from the target population, without parametric assumptions: we merely compute the proportion of documents observed with each pattern of word profiles. Since D is not observed in the population, we cannot estimate P(S∣ D) directly. Instead, we make the crucial assumption that its value in the labeled, hand‐coded sample, Ph(S∣ D), is the same as that in the population,                  "
"35","In principle, we could estimate P(D) in equation (6) assuming only the veracity of equation (7) and the accuracy of our estimates of P(S) and P(S∣ D), by solving equation (6) via standard regression algebra. That is, if we think of P(D) as the unknown “regression coefficients”β,  P(S∣ D) as the “explanatory variables” matrix X, and P(S) as the “dependent variable” Y, then equation (6) becomes Y = Xβ (with no error term). This happens to be a linear expression but not because of any assumption imposed on the problem that could be wrong. The result is that we can solve for P(D) via the usual regression calculation: β= (X′ X)−1X′ y (or via standard constrained least squares to ensure that elements of P(D) are each in [0,1] and collectively sum to 1). A key point is that this calculation does not require classifying individual documents into categories and then aggregating; it estimates the aggregate proportions directly.               "
"36","This simple approach poses two difficulties in our application. First, K is typically very large and so 2K is far larger than any standard computer could handle. Second is a sparseness problem since the number of observations available for estimating P(S) and P(S∣ D) is much smaller than the number of potential word profiles (n < <2K). To avoid both of these issues, we adapt results from King and Lu (2008) and randomly choose subsets of between approximately 5 and 25 words. The optimal number of words to use per subset is application‐specific, but can be determined empirically through cross‐validation within the labeled set. Although the estimator remains approximately unbiased regardless of subset size, in practice we find that setting the number of words per subset too high can lead to inefficiency. The reason is that as the number of words per subset increases, the number of unique subsets increases, reducing the number of common subsets that appear in both the labeled and unlabeled data sets. In addition, in the applications below, the words included in each subset are chosen randomly with equal probabilities, although in some applications, performance may improve by weighting words unequally.               "
"37","Once we determine the optimal number of subsets through cross‐validation, we solve for P(D) in each, and average the results across the subsets. Because S is treated as a consequence of D, using subsets of S introduces no new assumptions. This simple subsetting procedure turns out to be equivalent to a version of the standard approach of smoothing sparse matrices via kernel densities, although, unlike the typical use of this procedure, its application here reduces bias. (Standard errors and confidence intervals are computed via standard bootstrapping procedures.)               "
"38","Interpretation A key advantage of estimating P(D) without the intermediate step of computing the individual classifications is that the required assumptions are much less restrictive. They can still be wrong, and as a result our estimates can be biased, but the dramatic reduction in their restrictiveness means that under the new approach we have a fighting chance to get something close to the right answer in many applications where valid inferences were not previously likely.               "
"39","Unlike direct sampling or standard supervised learning approaches, our strategy allows the distribution of documents across word‐stem profiles, P(S), and the distribution of documents across the categories, P(D), to each be completely different in the labeled set and population set of documents. So for example, if a word or pattern of words becomes more popular between the time the labeled set was hand coded and the population documents were collected, no biases would emerge. Similarly, if documents in certain categories are more prevalent in the population than labeled set, no biases would result. In our running example, no bias would be induced if the labeled set includes a majority of conservative Republicans who defend everything President Bush does and the target population has a supermajority of liberal Democrats who want nothing more than to end the Bush presidency. In contrast, changes in either P(D) or P(S) between the labeled and population sets would be sufficient to doom existing classification‐based approaches. For example, so long as “idiot” remains an insult, our method can make appropriate use of that information, even if the word becomes less common (a change in P(S)) or if there are fewer people who think politicians deserve it (a change in P(D)).               "
"40","The key theoretical assumption is equation (7)—that the documents in the hand‐coded set contain sufficient good examples of the language used for each document category in the population. To be more specific, among all documents in a given category, the prevalence of particular word profiles in the labeled set should be the same in expectation as in the population set. For example, the language bloggers use to describe an “extremely negative” view of Hillary Clinton in the labeled set must be at least a subset of the way she is described in the target population. They do not need to write literally the same blog posts, but rather need to have the same probabilities of using similar word profiles so that Ph(S∣ D =−2) = P(S∣ D =−2). This assumption can be violated due to population drift or for other reasons, but we can always hand code some additional cases in the population set to verify that it holds sufficiently well. And as discussed above, the proportion of examples of each document category and of each word profile can differ between the two document sets.               "
"41","The methodology is also considerably easier to use in practice. Applying the standard supervised learning approach is difficult, even if we meet its assumptions. Even if we forget about choosing the “true” model, merely finding a “good” specification with thousands of explanatory variables to choose from can be extraordinarily time consuming. One needs to fit numerous statistical models, consider many specifications within each model type, run cross‐validation tests, and check various fit statistics. Social scientists have a lot of experience with specification searches, but all the explanatory variables mean that even one run would take considerable tuning and many runs would need to be conducted."
"42","The problem is further complicated by the fact that social scientists are accustomed to choosing their statistical specifications on the basis of prior theoretical expectations and results from past research, whereas the overwhelming experience in the information extraction literature is that radically empirical approaches work best for a given amount of effort. For example, we might think we could carefully choose words or phrases to characterize particular document categories (e.g., “awful,”“irresponsible,”“impeach,” etc., to describe negative views about President Bush), and indeed this approach will often work to some degree. Yet, a raw empirical search for the best specification, ignoring these theoretically chosen words, will typically turn up predictive patterns we would not have thought of ex ante. Indeed, methods based on highly detailed parsing of the grammar and sentence structure in each document can also work exceptionally well (e.g., King and Lowe 2003), but the strong impression from the literature is that the extensive, tedious work that goes into adapting these approaches for each application is more productively put into collecting more hand‐coded examples and then using an automatic specification search routine.               "
"43","We begin here with a simple simulated example, proceed to real examples of different types of documents, and study how many documents one needs to hand code. We also compare our approach to existing methods and discuss what can go wrong. Readers can replicate and modify any of these analyses using the replication files made available with this article."
"44","We begin with a simulated data set of 10 words and thus  possible word‐stem profiles. We set the elements of Ph(D) to be the same across the seven categories, and then set the population document category frequencies, P(D), to very different values. We then draw a value  from Ph(D), insert the simulation into , which we set to that from the population, and then draw the simulated matrix  from this density. We repeat the procedure 1,000 times to produce the labeled data set, and analogously for the population.            "
"45","The left two panels of Figure 2 summarize the sharp differences between the hand‐coded and population distributions in these data. The left graph plots Ph(D) horizontally by P(D) vertically, where the seven circles represent the category proportions. If the proportions were equal, they would all fall on the 45° line. If one used the labeled, hand‐coded sample in this case via direct sampling to estimate the document category frequencies in the population, the result would not even be positively correlated with the truth.            "
"46","                 Accurate Estimates Despite Differences Between Labeled and Population Sets                         "
"47"," Notes: For both P(D) on the left and P(S) in the center, the distributions differ considerably. The direct sampling estimator, Ph(D), is therefore highly biased. Yet, the right panel shows that our nonparametric estimator remains unbiased.                        "
"48","The differences between the two distributions of word frequency profiles appear in the middle graph (where for clarity the axes, but not labels, are on the log scale). Each circle in this graph represents the proportion of documents with a specific word profile. Again, if the two distributions were the same, all the circles would appear on the diagonal line, but again many of the circles fall off the line, indicating differences between the two samples."
"49","Despite the considerable differences between the labeled data set and the population, and the bias in the direct sampling estimator, our approach still produces accurate estimates. The right panel of the figure presents these results. The actual P(D) is on the horizontal axis and the estimated version is on the vertical axis, with each of the seven circles representing one of the document frequency categories. Estimates that are accurate fall on the 45° line. In fact, the points are all huddled close to this equality line, with even the maximum distance from the line for any point being quite small.            "
"50","We now offer several out‐of‐sample tests of our nonparametric approach in different types of real data. Our first test includes the 4,303 blog posts that mention George W. Bush. (For levels of intercoder reliability for this task, see the appendix.) These posts include 47,726 unique words and 3,165 unique word stems. We randomly divide the data set in half between the training set and test set and, to make the task more difficult, then randomly delete half (422) of the posts coded −2 in the test set. Our test set therefore intentionally selects on (what would be considered, in standard supervised learning approaches) the dependent variable. The results from our nonparametric estimator appear in Figure 3 as one open circle for each of the seven categories, with 95% confidence intervals appearing as a vertical line. Clearly the points are close to the 45^ line, indicating approximately unbiased estimates, and all are within the 95% confidence intervals.            "
"51","                 Out‐of‐Sample Validation                         "
"52"," Notes: The plot gives the estimated document category frequencies (vertically) by the actual frequencies (horizontally). Our nonparametric approach is represented with black open circles, with 95% confidence intervals as vertical lines. Aggregated optimized SVM analyses also appear for radial basis (black dots), linear (green triangles), polynomial (blue diamonds), and sigmoid kernels (red squares). Estimates closer to the 45° line are more accurate.                        "
"53","Also plotted on the same graph are the document category proportions aggregated up from individual classifications given by four separately optimized support vector machine (SVM) classifiers, the most widely used (and arguably the best) of the existing methods. These include SVMs using a radial basis function (black dots), linear kernel (green triangles), polynomial kernel (blue diamonds), and sigmoid kernel (red squares) (Brank et al. 2002; Hastie, Tibshirani, and Friedman 2001; Hsu, Chang, and Lin 2003; Joachims 1998). As can be seen in the graph, these results vary wildly and none do as well as our approach. They are plotted without confidence intervals since SVM is not a statistical method and has no probabilistic foundation. An additional difficulty of using individual classifiers is the highly time‐intensive tuning required. Whereas the results from our approach represent only a single run, we followed the advice of the SVM literature and chose the final four SVMs to present in Figure 3 by optimizing over a total of 19,090 separate SVM runs, including cross‐validation tests on 10 separate subsets of the labeled set. One run of our nonparametric estimator took 60 seconds of computer time, or a total of five hours for 300 bootstrapped runs. The SVM runs took approximately 8.7 days (running 24 hours/day) on a powerful server and much more in human time.            "
"54","We give an alternative view of these results in Table 1. The first three numerical columns report individual classification performance whereas the last gives the mean absolute error in the document category proportions. The last column confirms the overall impression from Figure 3 that the nonparametric method has much lower error in estimating the document category proportions. Leaving aside the sigmoid kernel, which did not work well in these data, the SVM results have the familiar patterns for individual classifiers: the models fit best to the in‐sample data, followed next by in‐sample cross‐validation, and lastly by the true out‐of‐sample predictions. The key result in this analysis is that, even among the SVM analyses, the best individual classifier (the linear kernel) is different from the best choice for minimizing the mean absolute error in the document category proportions (the polynomial kernel). Of course, nothing is wrong with SVM when applied to the individual classification goal for which it was designed.            "
"55","We now give three brief examples applying our method to different sources of unstructured text. The first is from a corpus of congressional speeches used in the computer science literature to evaluate supervised learning methods (Thomas, Pang, and Lee 2006). Researchers selected 3,838 speeches given in the House of Representatives between January 4th and May 12th, 2005, during “contentious” debates, defined as those where more than 20% of the speeches were in opposition. To simulate how a resource‐conscious researcher might proceed, we used the 1,887 speeches appearing on even‐numbered pages of the congressional record as a training set, and then estimated the distribution of supportive speeches in the test set of 1,951 speeches on odd‐numbered pages. The results using the nonparametric estimator appear in the top‐left graph in Figure 4 and are again highly accurate.            "
"56","                 Additional Out‐of‐Sample Validation                         "
"57"," Notes: The left graph displays the accuracy of the nonparametric method in recovering the distribution of supporting versus opposing speeches in Congress. The center graph shows the same for a categorization of newspaper editorials on immigration, and the right graph shows the distribution across categories of emails sent by enron employees. As before, 95% confidence intervals are represented by vertical lines, and estimates closer to the 45° line are more accurate.                        "
"58","Another example comes from a data set of 462 immigration editorials that we compiled using Factiva. The editorials appeared in major newspapers between April 1st and July 15th, 2007, and were coded into four nonordered categories indicating editorials supporting the Senate's immigration bill, those opposing it, and two categories that capture letters to the editor and other miscellaneous articles. Here, the training set includes the 283 editorials prior to June 12th, while the test set includes the 179 editorials on or after that date. Deviations from the 45° line are due to slight violations of the assumption in equation (7). This is quite a hard test, since some categories have as few as 40 examples. The small discrepancy can also be fixed easily if this were a real application by adding to the hand‐coded set a small number of documents collected over time.            "
"59","Our final example comes from 1,726 emails sent by Enron employees and classified into five nonordered categories: company business, personal communications, logistic arrangements, employment arrangements, and document editing.10 To make the task more difficult, we first created a skewed test set of 600 emails that was more uniformly distributed than the training set, with no category accounting for less than 12% or more than 39% of the observations. We then used the remaining 1,126 emails as a mutually exclusive training set where the comparable bounds were 4% and 50%. The results are quite accurate, especially given the paucity of information in many (short) emails, and are displayed in the right panel of Figure 4.            "
"60","Any remaining bias in our estimator is primarily a function of the assumption in equation (7). In contrast, efficiency, as well as confidence intervals and standard errors, are primarily a function of how many documents are hand coded and so are entirely under the control of the investigator. But how many is enough? Hand coding is expensive and time consuming and so we would want to limit its use as much as possible, subject to acceptable uncertainty intervals.            "
"61","To study this question, we set aside bias by randomly sampling the labeled set directly from the population and plotting in Figure 5 the root mean square error (RMSE) averaged across the categories vertically by the number of hand‐coded documents horizontally for our estimator (solid line) and the direct sampling estimator (dashed line). RMSE is lower for the direct estimator, of course, since this sample was drawn directly from the population and little computation is required, although the difference between the two is only about two‐tenths of a percentage point.            "
"62","                 Average Root Mean Square Error by Number of Hand‐Coded Documents                         "
"63","For our estimator, the RMSE drops quickly as the number of hand‐coded documents increases. Even the highest RMSE, with only 100 documents in the labeled set, is only slightly higher than 3 percentage points, which would be acceptable for some applications. (For example, most national surveys have a margin of error of at least 4 percentage points, even when assuming random sampling and excluding all other sources of error.) At about 500 documents, the advantage of more hand coding begins to suffer diminishing returns. In part this is because there is little more error to eliminate as our estimator then has an average RMSE of only about 1.5 percentage points."
"64","The conclusion here is clear: coding more than about 500 documents to estimate a specific quantity of interest is probably not necessary, unless one is interested in much more narrow confidence intervals than is common or in specific categories that happen to be rare. For some applications, as few as 100 documents may even be sufficient."
"65","We now discuss five problems that can arise with our methods. If they do arise, and steps are not taken to avoid or ameliorate them, they can cause our estimator to be biased or inefficient. We also discuss what to do to ameliorate these problems."
"66","First, and most importantly, our procedure cannot work without reliable information. This requires that the original documents contain the information needed, the hand codings are reliable enough to extract the information from the documents, and the quantitative summary of the document (in S) is a sufficiently accurate representation and sufficient to estimate the quantities of interest. Each of these steps requires careful study. Documents that do not contain the information needed cannot be used to estimate quantities of interest. If humans cannot code these documents into well‐defined categories with some reasonable level of reliability, then automated procedures are unlikely to succeed at the same task. And many choices are available in producing abstract numerical summaries of written text documents. Although we have found that stemmed unigrams are a sufficient representation to achieve approximately unbiased inferences in our examples, researchers may have to use some of the other tricks discussed in the section entitled “coding variables” for different applications.            "
"67","Second, a key issue is the assumption in equation (7) that P(S∣ D) is the same in the labeled and population document sets. We thus have much less restrictive assumptions than prior methods, but we still assume a particular type of connection between the two document sets. If we are studying documents over a long time period, where the language used to characterize certain categories is likely to change, it would not be advisable to select the labeled test set only from the start of the period. Checking whether this assumption holds is not difficult and merely requires hand coding some additional documents closer to the quantity presently being estimated and using them as a validation test set. If the data are collected over time, one can either hand code several data sets from different time periods or gradually add hand‐coded documents collected over time. In our running example, we are attempting to track opinions over a single presidential campaign. As such, only one hand‐coded data set at the start may be sufficient, but we have tested this assumption, and will continue to do so by periodically hand coding small numbers of blogs.11"
"68","Third, each category of D should be defined so as to be mutually exclusive, exhaustive, and relatively homogeneous. To confront cases where the categories are not mutually exclusive, one can define an additional “both” category. Categories that require many examples to define may be too broad for effective estimation as may occur for residual or catch‐all categories. Consider the “NB” category in our data as one example. There are innumerable types of web sites that are not blogs, each with very different language; yet this category was essential since our blog search algorithm was not perfect. In fact, we do find slightly more bias in estimating category NB than the others in our categorization, but not so much as to cause a problem for our applications. Given our experiences, the identification of an effective set of categories in D is an important issue and should involve careful iteration between improving concepts, validation in hand coding tests, and searching for new possibilities in example documents. Intercoder reliability is a crucial metric as well. If human coders cannot agree on a classification, automated approaches are not likely to return sensible results either.            "
"69","Fourth, our approach requires the choice of the number of word stems to use in each randomly chosen subset. While choosing the number of random subsets is easy (the more the better, and so like any simulation method the number should be chosen based on available computer time and the precision needed), the number of word stems to use in each random subset must be chosen more carefully. Choosing too few or too many will leave P(S) and P(S∣ D) too sparse or too short and may result in attenuation bias due to measurement error in P(S∣ D), which serve as the “explanatory variables” in the estimation equation. To make this choice in practice, we use standard automated cross‐validation techniques, such as by randomly dividing the labeled set into training and test sets and then checking what works in those data. In practice, the number of word stems to choose to avoid sparseness bias mainly seems to be a function of the number of unique word stems in the documents. Fixing any problem that may arise via these types of cross‐validation tests is not difficult. Given the other recommendations discussed above—stability in P(S∣ D), coding categories that are homogeneous and clearly defined—the choice of the optimal number of subsets can account for many of the performance problems we observe in practice. In some applications, researchers may find it helpful to weight the word stems unevenly, so that words likely to have more information (such as based on their “mutual information”) appear more frequently in the subsets, although we have not found this necessary.            "
"70","Finally, we require a reasonable number of documents in each category of D to be hand coded. Although we studied the efficiency of our procedure as a function of the number of hand‐coded documents above, these results would change if by chance some categories had very few hand‐coded documents and we cared about small differences in the proportions in these population categories. This makes sense, of course, since the method requires examples from which to generalize. Discovering too few examples for one or more categories can be dealt with in several ways. Most commonly, one can alter the definition of the categories or can change the coding rules.            "
"71","However, even if examples of some categories are rare, they may be sufficiently well represented in the much larger population set to be of interest to social scientists. To deal with situations like this, we would need to find more examples from these relatively rare categories. Doing so by merely increasing the size of the hand‐coded data set would be wasteful given that we would wind up with many more coded documents in the more prevalent categories. Still, it may be possible to use available meta‐data to find the needed documents with higher probability. In our blogs data, we could find blog posts of certain types via links from other already hand‐coded posts or from popular directories of certain types of blogs. Fortunately, the labeled set is assumed to be generated conditional on the categories, and so no bias is induced if we add extra examples via this “case‐control” approach (cf. King and Zeng 2002).            "
"72","Throughout all these potential problems, the best approach seems to be the radically empirical procedure suggested in the supervised learning literature. If the procedure you choose works, it works; if it doesn't, it doesn't. And so one should verify that the procedures work by subdividing the labeled set into training and (truly out of sample) test sets and then directly testing hypotheses about the success of the procedure. Ideally, this should then be repeated with different types of labeled test sets. The more we make ourselves vulnerable to being wrong, using rigorous scientific procedures, the more we learn. Fortunately, the tools we make available here would seem to make it possible to learn enough to produce a reliable procedure in many applications."
"73","Relatedly, standard errors and confidence intervals take a very different role in this type of research than the typical observational social science work. For most methods, the only way to shrink confidence intervals is to collect more data. For the method introduced here, all a researcher needs to do is to hand code additional documents (selected randomly or randomly conditional on D) and rerun the algorithm. As long as no data are discarded along the way, continuing to hand code until one's confidence intervals are small enough induces no bias, since our methodology (like direct sampling) is invariant to sampling plans (Thompson 2002, 286ff). A reasonably general approach is to hand code roughly 200 documents and run the algorithm. If uncertainty is more than desired, then hand code 100 more randomly selected documents, add them to the first set, reestimate, and continue until the uncertainty is small enough.            "
"74","Given the many possible applications of this method, it is difficult to provide general guidelines about how time‐intensive the entire process is likely to be. However, our experience is that identifying clear categories that humans are consistently able to differentiate takes far longer than the automated analyses we propose. Once users have clearly defined categories hand coded for a few hundred documents, they can often estimate the document category proportions for far larger corpora a few hours later."
"75","Existing supervised methods of analyzing textual data come primarily from the tremendously productive computer science literature. This literature has been focused on optimizing the goals of computer science, which for the most part involve maximizing the percent of documents correctly classified into a given set of categories. We do not offer a way to improve on the computer scientists' goals. Instead of seeking to classify any individual document, most social science literature that has hand‐(or computer‐) coded text is primarily interested in broad characterizations about the whole set of documents, such as unbiased estimates of the proportion of documents in given categories. Unfortunately, since they are optimized for a different purpose, computer science methods often produce biased estimates of these category proportions."
"76","By developing methods for analyzing textual data that optimize social science goals directly, we are able to considerably outperform standard computer science methods developed for a different purpose. In addition, our approach requires no modeling assumptions, no modeling choices, and no complicated statistical approaches, and lets the social scientist pose the theoretical question to be answered. It also requires far less work than projects based entirely on hand coding, and much less work than most computer science methods of individual classification; it is both fast and can be used in real time. Individual‐level classification is not a result of this method, and so it is not useful for all tasks, but numerous quantities of interest, from separate subdivisions of the population or different populations, can be estimated. As with all supervised learning methods, our approach does require careful efforts to properly define categories and to hand code a small sample of texts."
"77","Although we have included only a few applications in this article, the methods offered here would seem applicable to many possible analyses that may not have been feasible previously. With the explosion of numerous types and huge quantities of text available to researchers on the web and elsewhere, we hope social scientists will begin to use these methods, and develop others, to harvest this new information and to improve our knowledge of the political, social, cultural, and economic worlds."
